#!/usr/bin/env python3
"""
vclm-KoCoder-7B ë²¤ì¹˜ë§ˆí¬ í‰ê°€
- HumanEval: ì½”ë“œ ìƒì„± ëŠ¥ë ¥ (Python)
- GSM8K: ìˆ˜í•™ ì¶”ë¡  ëŠ¥ë ¥
- MMLU: ì¼ë°˜ ì§€ì‹
- KoBEST: í•œêµ­ì–´ ì´í•´
"""

import os
import sys
import json
import subprocess
from datetime import datetime
from pathlib import Path

# ì„¤ì •
MODEL_PATH = "/home/work/tesseract/vclm/vclm-korean-7b-coder-merged"
MODEL_NAME = "vclm-KoCoder-7B"
OUTPUT_DIR = "/home/work/tesseract/vclm/benchmark_results/kocoder"
TIMESTAMP = datetime.now().strftime("%Y%m%d_%H%M%S")

# ë²¤ì¹˜ë§ˆí¬ ì„¤ì •
BENCHMARKS = {
    "humaneval": {
        "name": "HumanEval",
        "description": "Python ì½”ë“œ ìƒì„± (pass@1, pass@10)",
        "command": "{lm_eval_cmd} --model hf --model_args pretrained={model_path},trust_remote_code=True --tasks humaneval --device cuda --batch_size 4 --output_path {output_dir}/humaneval_{timestamp}.json",
        "priority": "HIGH"  # ì½”ë“œ ëŠ¥ë ¥ í•µì‹¬ ì§€í‘œ
    },
    "mbpp": {
        "name": "MBPP",
        "description": "Python ì½”ë“œ ìƒì„± (ê¸°ë³¸ í”„ë¡œê·¸ë˜ë°)",
        "command": "{lm_eval_cmd} --model hf --model_args pretrained={model_path},trust_remote_code=True --tasks mbpp --device cuda --batch_size 4 --output_path {output_dir}/mbpp_{timestamp}.json",
        "priority": "HIGH"
    },
    "gsm8k": {
        "name": "GSM8K",
        "description": "ìˆ˜í•™ ë¬¸ì œ í’€ì´ (ì´ˆë“± ìˆ˜ì¤€)",
        "command": "{lm_eval_cmd} --model hf --model_args pretrained={model_path},trust_remote_code=True --tasks gsm8k --device cuda --batch_size 8 --num_fewshot 5 --output_path {output_dir}/gsm8k_{timestamp}.json",
        "priority": "MEDIUM"  # ê¸°ì¡´ ëŠ¥ë ¥ ìœ ì§€ í™•ì¸
    },
    "mmlu": {
        "name": "MMLU",
        "description": "ì¼ë°˜ ì§€ì‹ (57ê°œ ê³¼ëª©)",
        "command": "{lm_eval_cmd} --model hf --model_args pretrained={model_path},trust_remote_code=True --tasks mmlu --device cuda --batch_size 8 --num_fewshot 5 --output_path {output_dir}/mmlu_{timestamp}.json",
        "priority": "MEDIUM"
    },
    "kobest": {
        "name": "KoBEST",
        "description": "í•œêµ­ì–´ ì´í•´ (BoolQ, COPA, HellaSwag, SentiNeg, WiC)",
        "command": "{lm_eval_cmd} --model hf --model_args pretrained={model_path},trust_remote_code=True --tasks kobest --device cuda --batch_size 8 --output_path {output_dir}/kobest_{timestamp}.json",
        "priority": "MEDIUM"  # í•œêµ­ì–´ ëŠ¥ë ¥ ìœ ì§€ í™•ì¸
    },
    "arc_challenge": {
        "name": "ARC Challenge",
        "description": "ê³¼í•™ ë¬¸ì œ (ì¤‘ê³ ë“± ìˆ˜ì¤€)",
        "command": "{lm_eval_cmd} --model hf --model_args pretrained={model_path},trust_remote_code=True --tasks arc_challenge --device cuda --batch_size 8 --num_fewshot 25 --output_path {output_dir}/arc_challenge_{timestamp}.json",
        "priority": "LOW"  # ì°¸ê³ ìš©
    }
}

def print_header():
    print("=" * 80)
    print(f"ğŸ”¬ {MODEL_NAME} ë²¤ì¹˜ë§ˆí¬ í‰ê°€")
    print("=" * 80)
    print(f"ğŸ“… ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“‚ ëª¨ë¸ ê²½ë¡œ: {MODEL_PATH}")
    print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {OUTPUT_DIR}")
    print("=" * 80)
    print()

def check_dependencies():
    """lm-evaluation-harness ì„¤ì¹˜ í™•ì¸"""
    print("[1/6] ì˜ì¡´ì„± í™•ì¸...")
    
    # lm_eval ë˜ëŠ” lm-eval ëª…ë ¹ì–´ í™•ì¸
    for cmd in ["/home/work/.local/bin/lm_eval", "lm_eval", "lm-eval"]:
        try:
            result = subprocess.run(
                [cmd, "--help"],
                capture_output=True,
                text=True,
                timeout=5
            )
            if result.returncode == 0:
                print(f"âœ… lm-evaluation-harness ì„¤ì¹˜ë¨: {cmd}")
                return cmd
        except (FileNotFoundError, subprocess.TimeoutExpired):
            continue
    
    # python -m lm_eval ì‹œë„
    try:
        result = subprocess.run(
            ["python", "-m", "lm_eval", "--help"],
            capture_output=True,
            text=True,
            timeout=5
        )
        if result.returncode == 0:
            print("âœ… lm-evaluation-harness ì„¤ì¹˜ë¨: python -m lm_eval")
            return "python -m lm_eval"
    except:
        pass
    
    print("âŒ lm-evaluation-harnessê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("ì„¤ì¹˜ ëª…ë ¹ì–´:")
    print("  pip install lm-eval")
    return None

def check_model():
    """ëª¨ë¸ íŒŒì¼ ì¡´ì¬ í™•ì¸"""
    print("\n[2/6] ëª¨ë¸ í™•ì¸...")
    model_path = Path(MODEL_PATH)
    
    required_files = [
        "config.json",
        "modeling_soka.py",
        "tokenizer.json"
    ]
    
    missing = []
    for file in required_files:
        if not (model_path / file).exists():
            missing.append(file)
    
    if missing:
        print(f"âŒ ëˆ„ë½ëœ íŒŒì¼: {', '.join(missing)}")
        return False
    
    print(f"âœ… ëª¨ë¸ íŒŒì¼ í™•ì¸ ì™„ë£Œ: {MODEL_PATH}")
    return True

def create_output_dir():
    """ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±"""
    print("\n[3/6] ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±...")
    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)
    print(f"âœ… {OUTPUT_DIR}")

def select_benchmarks():
    """ì‹¤í–‰í•  ë²¤ì¹˜ë§ˆí¬ ì„ íƒ"""
    print("\n[4/6] ë²¤ì¹˜ë§ˆí¬ ì„ íƒ...")
    print("\nì‚¬ìš© ê°€ëŠ¥í•œ ë²¤ì¹˜ë§ˆí¬:")
    for i, (key, config) in enumerate(BENCHMARKS.items(), 1):
        print(f"  {i}. [{config['priority']:>6}] {config['name']:<20} - {config['description']}")
    
    print("\nì˜µì…˜:")
    print("  all    - ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ (ì•½ 2-3ì‹œê°„ ì†Œìš”)")
    print("  high   - HIGH ìš°ì„ ìˆœìœ„ë§Œ (ì½”ë“œ í…ŒìŠ¤íŠ¸, ì•½ 30ë¶„)")
    print("  custom - ê°œë³„ ì„ íƒ")
    print()
    
    choice = input("ì„ íƒ (all/high/custom/ë²ˆí˜¸): ").strip().lower()
    
    if choice == "all":
        return list(BENCHMARKS.keys())
    elif choice == "high":
        return [k for k, v in BENCHMARKS.items() if v["priority"] == "HIGH"]
    elif choice == "custom":
        selected = []
        for key in BENCHMARKS.keys():
            ans = input(f"  {BENCHMARKS[key]['name']} ì‹¤í–‰? (y/n): ").strip().lower()
            if ans == 'y':
                selected.append(key)
        return selected
    else:
        # ë²ˆí˜¸ë¡œ ì„ íƒ
        try:
            idx = int(choice) - 1
            keys = list(BENCHMARKS.keys())
            if 0 <= idx < len(keys):
                return [keys[idx]]
        except:
            pass
        print("âŒ ì˜ëª»ëœ ì…ë ¥. HIGH ìš°ì„ ìˆœìœ„ë§Œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
        return [k for k, v in BENCHMARKS.items() if v["priority"] == "HIGH"]

def run_benchmark(benchmark_key, lm_eval_cmd):
    """ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    config = BENCHMARKS[benchmark_key]
    
    print("\n" + "=" * 80)
    print(f"ğŸš€ {config['name']} ì‹¤í–‰ ì¤‘...")
    print(f"   {config['description']}")
    print("=" * 80)
    
    cmd = config["command"].format(
        lm_eval_cmd=lm_eval_cmd,
        model_path=MODEL_PATH,
        output_dir=OUTPUT_DIR,
        timestamp=TIMESTAMP
    )
    
    print(f"ğŸ“ ëª…ë ¹ì–´: {cmd}\n")
    
    start_time = datetime.now()
    
    try:
        result = subprocess.run(
            cmd,
            shell=True,
            capture_output=False,  # ì‹¤ì‹œê°„ ì¶œë ¥
            text=True
        )
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        if result.returncode == 0:
            print(f"\nâœ… {config['name']} ì™„ë£Œ (ì†Œìš” ì‹œê°„: {duration:.1f}ì´ˆ)")
            return True, duration
        else:
            print(f"\nâŒ {config['name']} ì‹¤íŒ¨ (exit code: {result.returncode})")
            return False, duration
            
    except Exception as e:
        print(f"\nâŒ {config['name']} ì˜¤ë¥˜: {e}")
        return False, 0

def generate_report(results):
    """ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
    print("\n" + "=" * 80)
    print("ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½")
    print("=" * 80)
    
    report = {
        "model": MODEL_NAME,
        "model_path": MODEL_PATH,
        "timestamp": TIMESTAMP,
        "date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "benchmarks": results
    }
    
    total_time = sum(r["duration"] for r in results.values())
    success_count = sum(1 for r in results.values() if r["success"])
    total_count = len(results)
    
    print(f"\nì´ ì‹¤í–‰: {total_count}ê°œ")
    print(f"ì„±ê³µ: {success_count}ê°œ")
    print(f"ì‹¤íŒ¨: {total_count - success_count}ê°œ")
    print(f"ì´ ì†Œìš” ì‹œê°„: {total_time:.1f}ì´ˆ ({total_time/60:.1f}ë¶„)")
    
    # ìƒì„¸ ê²°ê³¼
    print("\nìƒì„¸ ê²°ê³¼:")
    for key, result in results.items():
        status = "âœ…" if result["success"] else "âŒ"
        name = BENCHMARKS[key]["name"]
        duration = result["duration"]
        print(f"  {status} {name:<20} ({duration:.1f}ì´ˆ)")
    
    # JSON ì €ì¥
    report_path = Path(OUTPUT_DIR) / f"benchmark_summary_{TIMESTAMP}.json"
    with open(report_path, "w", encoding="utf-8") as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")
    
    # ê²°ê³¼ íŒŒì¼ ìœ„ì¹˜
    print(f"\nğŸ“ ê²°ê³¼ íŒŒì¼:")
    print(f"   {OUTPUT_DIR}/")
    for key in results.keys():
        result_file = f"{key}_{TIMESTAMP}.json"
        print(f"   - {result_file}")
    
    print("\n" + "=" * 80)
    print("ğŸ‰ ë²¤ì¹˜ë§ˆí¬ í‰ê°€ ì™„ë£Œ!")
    print("=" * 80)

def main():
    print_header()
    
    # 1. ì˜ì¡´ì„± í™•ì¸
    lm_eval_cmd = check_dependencies()
    if not lm_eval_cmd:
        print("\nâŒ lm-evaluation-harnessë¥¼ ë¨¼ì € ì„¤ì¹˜í•˜ì„¸ìš”:")
        print("   pip install lm-eval")
        sys.exit(1)
    
    # 2. ëª¨ë¸ í™•ì¸
    if not check_model():
        print(f"\nâŒ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {MODEL_PATH}")
        sys.exit(1)
    
    # 3. ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    create_output_dir()
    
    # 4. ë²¤ì¹˜ë§ˆí¬ ì„ íƒ
    selected = select_benchmarks()
    
    if not selected:
        print("âŒ ì„ íƒëœ ë²¤ì¹˜ë§ˆí¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(1)
    
    print(f"\nâœ… ì„ íƒëœ ë²¤ì¹˜ë§ˆí¬: {', '.join([BENCHMARKS[k]['name'] for k in selected])}")
    print(f"â±ï¸  ì˜ˆìƒ ì†Œìš” ì‹œê°„: ", end="")
    if len(selected) >= 5:
        print("2-3ì‹œê°„")
    elif len(selected) >= 3:
        print("1-2ì‹œê°„")
    else:
        print("30ë¶„-1ì‹œê°„")
    
    input("\nê³„ì†í•˜ë ¤ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")
    
    # 5. ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    print("\n[5/6] ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì¤‘...")
    results = {}
    
    for i, benchmark_key in enumerate(selected, 1):
        print(f"\nì§„í–‰ë¥ : {i}/{len(selected)}")
        success, duration = run_benchmark(benchmark_key, lm_eval_cmd)
        results[benchmark_key] = {
            "success": success,
            "duration": duration,
            "name": BENCHMARKS[benchmark_key]["name"]
        }
    
    # 6. ë¦¬í¬íŠ¸ ìƒì„±
    print("\n[6/6] ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
    generate_report(results)

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(0)
    except Exception as e:
        print(f"\n\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

