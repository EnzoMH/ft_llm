#!/usr/bin/env python3
"""
4ê°€ì§€ ëª¨ë¸ ì„¤ì • ë¹„êµ í…ŒìŠ¤íŠ¸
1. EXAONE-4.0 (RAG ì—†ìŒ)
2. EXAONE-4.0 + RAG
3. Qwen2.5-7B (RAG ì—†ìŒ)
4. Qwen2.5-7B + RAG
"""

import torch
import json
import random
import faiss
import numpy as np
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM
from sentence_transformers import SentenceTransformer
import time


def load_topics(topics_file: str) -> list:
    """í† í”½ ë¡œë“œ"""
    with open(topics_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data.get('topics', [])


def convert_topic_to_question(topic: str) -> str:
    """í† í”½ì„ ìì—°ìŠ¤ëŸ¬ìš´ ì§ˆë¬¸ìœ¼ë¡œ ë³€í™˜"""
    if topic.endswith('?') or 'ì–´ë–»ê²Œ' in topic or 'ë°©ë²•' in topic:
        return topic
    
    question_patterns = [
        f"{topic}ì— ëŒ€í•´ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        f"{topic}ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ êµ¬í˜„í•˜ëŠ” ë°©ë²•ì€?",
        f"{topic} ë„ì… ì‹œ ê³ ë ¤ì‚¬í•­ê³¼ ì‹¤ë¬´ íŒì„ ì•Œë ¤ì£¼ì„¸ìš”.",
        f"{topic}ì˜ ì¥ë‹¨ì ê³¼ ì‹¤ì œ ì ìš© ì‚¬ë¡€ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”."
    ]
    
    if len(topic) < 10:
        return question_patterns[0]
    elif 'ìµœì í™”' in topic or 'ê´€ë¦¬' in topic:
        return question_patterns[1]
    elif 'ë„ì…' in topic or 'êµ¬ì¶•' in topic:
        return question_patterns[2]
    else:
        return question_patterns[3]


def evaluate_answer_quality(answer: str, topic: str) -> float:
    """ë‹µë³€ í’ˆì§ˆ í‰ê°€ (0-10ì )"""
    score = 5.0
    
    if len(answer) < 100:
        score -= 2
    elif 200 <= len(answer) <= 800:
        score += 1
    elif len(answer) > 1000:
        score -= 0.5
    
    if any(word in answer for word in ['ì²«ì§¸', 'ë‘˜ì§¸', 'ì…‹ì§¸', 'ë¨¼ì €', 'ë‹¤ìŒìœ¼ë¡œ', 'ë§ˆì§€ë§‰ìœ¼ë¡œ']):
        score += 1
    
    if '\n' in answer or '  ' in answer:
        score += 0.5
    
    technical_terms = ['ì‹œìŠ¤í…œ', 'í”„ë¡œì„¸ìŠ¤', 'ìµœì í™”', 'íš¨ìœ¨', 'ê´€ë¦¬', 'ìë™í™”', 
                       'WMS', 'ì¬ê³ ', 'ë¬¼ë¥˜', 'ë°ì´í„°', 'ì‹¤ì‹œê°„', 'í†µí•©']
    term_count = sum(1 for term in technical_terms if term in answer)
    score += min(term_count * 0.2, 2)
    
    topic_words = topic.split()
    relevance = sum(1 for word in topic_words if word in answer and len(word) > 1)
    score += min(relevance * 0.3, 1.5)
    
    if 'ëª¨ë¥´ê² ' in answer or 'ì˜ ëª¨ë¦„' in answer:
        score -= 2
    if len(answer) < 50:
        score -= 3
    if answer.count('.') < 2:
        score -= 1
    
    return max(0, min(10, score))


class RAGSystem:
    """FAISS ê¸°ë°˜ RAG ì‹œìŠ¤í…œ"""
    
    def __init__(self, faiss_path: str = "/home/work/tesseract/faiss_storage"):
        print(f"  RAG ì‹œìŠ¤í…œ ë¡œë”© ì¤‘... ({faiss_path})")
        
        faiss_dir = Path(faiss_path)
        
        # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
        index_file = faiss_dir / "warehouse_automation_knowledge.index"
        self.index = faiss.read_index(str(index_file))
        
        # ë¬¸ì„œ ë¡œë“œ
        with open(faiss_dir / "documents.json", 'r', encoding='utf-8') as f:
            self.documents = json.load(f)
        
        with open(faiss_dir / "metadata.json", 'r', encoding='utf-8') as f:
            self.metadata = json.load(f)
        
        # ì„ë² ë”© ëª¨ë¸
        self.embedding_model = SentenceTransformer(
            "jhgan/ko-sroberta-multitask",
            device='cuda' if torch.cuda.is_available() else 'cpu'
        )
        
        print(f"  âœ“ RAG ë¡œë“œ ì™„ë£Œ: {len(self.documents)}ê°œ ë¬¸ì„œ")
    
    def retrieve_context(self, question: str, k: int = 3) -> list:
        """ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰"""
        # ì„ë² ë”© ìƒì„±
        query_embedding = self.embedding_model.encode(
            question,
            convert_to_numpy=True
        ).reshape(1, -1).astype('float32')
        
        # FAISS ê²€ìƒ‰
        distances, indices = self.index.search(query_embedding, k)
        
        contexts = []
        for idx, distance in zip(indices[0], distances[0]):
            if idx < len(self.documents):
                contexts.append({
                    'content': self.documents[idx][:500],  # ê¸¸ì´ ì œí•œ
                    'distance': float(distance)
                })
        
        return contexts
    
    def create_rag_prompt(self, question: str, contexts: list) -> str:
        """RAG í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        context_text = "\n\n".join([
            f"ì°¸ê³ ì •ë³´ {i+1}:\n{ctx['content']}"
            for i, ctx in enumerate(contexts)
        ])
        
        prompt = f"""ë‹¤ìŒ ì°¸ê³ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

{context_text}

ì§ˆë¬¸: {question}

ìœ„ ì •ë³´ë¥¼ í™œìš©í•˜ë˜, "ì°¸ê³ ìë£Œ" ê°™ì€ í‘œí˜„ì€ ì‚¬ìš©í•˜ì§€ ë§ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì„¤ëª…í•˜ì„¸ìš”.
ë‹µë³€:"""
        
        return prompt


def test_single_model(
    model_name: str,
    topics: list,
    use_rag: bool = False,
    rag_system: RAGSystem = None,
    num_samples: int = 5,
    seed: int = 42
):
    """ë‹¨ì¼ ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    
    config_name = f"{model_name.split('/')[-1]} {'+ RAG' if use_rag else '(RAG ì—†ìŒ)'}"
    
    print(f"\n{'='*80}")
    print(f"í…ŒìŠ¤íŠ¸: {config_name}")
    print(f"{'='*80}\n")
    
    # ëª¨ë¸ ë¡œë”©
    print("ëª¨ë¸ ë¡œë”© ì¤‘...")
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    
    print(f"âœ“ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    print(f"âœ“ ë©”ëª¨ë¦¬: {model.get_memory_footprint() / 1024**3:.2f} GB\n")
    
    # ìƒ˜í”Œë§ (ë™ì¼í•œ ì§ˆë¬¸)
    random.seed(seed)
    sampled_topics = random.sample(topics, min(num_samples, len(topics)))
    
    generation_config = {
        "max_new_tokens": 800,
        "temperature": 0.7,
        "top_p": 0.9,
        "do_sample": True,
        "pad_token_id": tokenizer.pad_token_id,
        "eos_token_id": tokenizer.eos_token_id
    }
    
    results = []
    total_time = 0
    
    for i, topic in enumerate(sampled_topics, 1):
        question = convert_topic_to_question(topic)
        
        print(f"ì§ˆë¬¸ {i}/{num_samples}: {topic}")
        
        # RAG ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ í”„ë¡¬í”„íŠ¸ ìƒì„±
        if use_rag and rag_system:
            contexts = rag_system.retrieve_context(question)
            rag_prompt = rag_system.create_rag_prompt(question, contexts)
            
            # ë””ë²„ê¹…: í”„ë¡¬í”„íŠ¸ ì¶œë ¥
            if i == 1:  # ì²« ë²ˆì§¸ ì§ˆë¬¸ë§Œ
                print(f"\n{'='*80}")
                print("RAG í”„ë¡¬í”„íŠ¸ í™•ì¸:")
                print(f"{'='*80}")
                print(rag_prompt[:500] + "..." if len(rag_prompt) > 500 else rag_prompt)
                print(f"{'='*80}\n")
            
            # Chat templateì— RAG í”„ë¡¬í”„íŠ¸ ë„£ê¸° ì‹œë„
            messages = [
                {
                    "role": "system",
                    "content": "ë‹¹ì‹ ì€ 10ë…„ ê²½ë ¥ì˜ WMS ë° ë¬¼ë¥˜ ìë™í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
                },
                {"role": "user", "content": rag_prompt}
            ]
            
            try:
                input_ids = tokenizer.apply_chat_template(
                    messages,
                    tokenize=True,
                    add_generation_prompt=True,
                    return_tensors="pt"
                ).to(model.device)
                input_text = None
            except:
                # Chat template ì‹¤íŒ¨ ì‹œ ì§ì ‘ í…ìŠ¤íŠ¸ ì‚¬ìš©
                input_text = rag_prompt
        else:
            # Chat template ì‚¬ìš©
            messages = [
                {
                    "role": "system",
                    "content": "ë‹¹ì‹ ì€ 10ë…„ ê²½ë ¥ì˜ WMS ë° ë¬¼ë¥˜ ìë™í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
                },
                {"role": "user", "content": question}
            ]
            
            try:
                input_ids = tokenizer.apply_chat_template(
                    messages,
                    tokenize=True,
                    add_generation_prompt=True,
                    return_tensors="pt"
                ).to(model.device)
                input_text = None
                
                # ë””ë²„ê¹…: ì²« ë²ˆì§¸ ì§ˆë¬¸ì˜ í”„ë¡¬í”„íŠ¸ ì¶œë ¥
                if i == 1:
                    decoded_prompt = tokenizer.decode(input_ids[0])
                    print(f"\n{'='*80}")
                    print("Chat Template í”„ë¡¬í”„íŠ¸ í™•ì¸:")
                    print(f"{'='*80}")
                    print(decoded_prompt[:500] + "..." if len(decoded_prompt) > 500 else decoded_prompt)
                    print(f"{'='*80}\n")
            except:
                input_text = f"ì§ˆë¬¸: {question}\n\në‹µë³€:"
        
        # í† í°í™”
        if input_text:
            input_ids = tokenizer(input_text, return_tensors="pt").input_ids.to(model.device)
        
        # ìƒì„±
        start_time = time.time()
        
        with torch.no_grad():
            outputs = model.generate(input_ids, **generation_config)
        
        elapsed = time.time() - start_time
        total_time += elapsed
        
        # ë””ì½”ë”©
        answer = tokenizer.decode(
            outputs[0][input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        quality_score = evaluate_answer_quality(answer, topic)
        
        print(f"  âœ“ ì™„ë£Œ - ê¸¸ì´: {len(answer)}ì, ì‹œê°„: {elapsed:.1f}s, ì ìˆ˜: {quality_score:.1f}/10\n")
        
        results.append({
            'topic': topic,
            'question': question,
            'answer': answer,
            'length': len(answer),
            'time': elapsed,
            'quality_score': quality_score,
            'use_rag': use_rag
        })
    
    # í†µê³„
    avg_length = sum(r['length'] for r in results) / len(results)
    avg_time = total_time / len(results)
    avg_quality = sum(r['quality_score'] for r in results) / len(results)
    
    print(f"{'='*80}")
    print(f"[{config_name}] í†µê³„")
    print(f"{'='*80}")
    print(f"í‰ê·  ë‹µë³€ ê¸¸ì´: {avg_length:.0f}ì")
    print(f"í‰ê·  ìƒì„± ì‹œê°„: {avg_time:.2f}ì´ˆ")
    print(f"í‰ê·  í’ˆì§ˆ ì ìˆ˜: {avg_quality:.2f}/10")
    print(f"{'='*80}\n")
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del model
    del tokenizer
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    return {
        'config_name': config_name,
        'model_name': model_name,
        'use_rag': use_rag,
        'avg_length': avg_length,
        'avg_time': avg_time,
        'avg_quality': avg_quality,
        'results': results
    }


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='4ê°€ì§€ ëª¨ë¸ ì„¤ì • ë¹„êµ')
    parser.add_argument('--topics-file', type=str,
                        default='expanded_data/topics_200_mixed.json')
    parser.add_argument('--num-samples', type=int, default=5,
                        help='ê° ì„¤ì •ë‹¹ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ìˆ˜')
    parser.add_argument('--seed', type=int, default=42)
    parser.add_argument('--output', type=str, default='model_comparison.json')
    parser.add_argument('--skip-qwen', action='store_true',
                        help='Qwen í…ŒìŠ¤íŠ¸ ìŠ¤í‚µ (ì‹œê°„ ì ˆì•½)')
    
    args = parser.parse_args()
    
    # í† í”½ ë¡œë“œ
    topics_path = Path(args.topics_file)
    if not topics_path.exists():
        topics_path = Path(__file__).parent / args.topics_file
    
    topics = load_topics(str(topics_path))
    print(f"\nâœ“ {len(topics)}ê°œ í† í”½ ë¡œë“œë¨\n")
    
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ì¬ì‚¬ìš©)
    print("="*80)
    print("RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
    print("="*80)
    rag_system = RAGSystem()
    
    all_results = []
    
    # 1. EXAONE-4.0 (RAG ì—†ìŒ)
    print(f"\n{'#'*80}")
    print("1/4: EXAONE-4.0 (RAG ì—†ìŒ)")
    print(f"{'#'*80}")
    result1 = test_single_model(
        model_name="LGAI-EXAONE/EXAONE-4.0-1.2B",
        topics=topics,
        use_rag=False,
        num_samples=args.num_samples,
        seed=args.seed
    )
    all_results.append(result1)
    
    # 2. EXAONE-4.0 + RAG
    print(f"\n{'#'*80}")
    print("2/4: EXAONE-4.0 + RAG")
    print(f"{'#'*80}")
    result2 = test_single_model(
        model_name="LGAI-EXAONE/EXAONE-4.0-1.2B",
        topics=topics,
        use_rag=True,
        rag_system=rag_system,
        num_samples=args.num_samples,
        seed=args.seed
    )
    all_results.append(result2)
    
    if not args.skip_qwen:
        # 3. Qwen2.5-7B (RAG ì—†ìŒ)
        print(f"\n{'#'*80}")
        print("3/4: Qwen2.5-7B (RAG ì—†ìŒ)")
        print(f"{'#'*80}")
        result3 = test_single_model(
            model_name="Qwen/Qwen2.5-7B-Instruct",
            topics=topics,
            use_rag=False,
            num_samples=args.num_samples,
            seed=args.seed
        )
        all_results.append(result3)
        
        # 4. Qwen2.5-7B + RAG
        print(f"\n{'#'*80}")
        print("4/4: Qwen2.5-7B + RAG")
        print(f"{'#'*80}")
        result4 = test_single_model(
            model_name="Qwen/Qwen2.5-7B-Instruct",
            topics=topics,
            use_rag=True,
            rag_system=rag_system,
            num_samples=args.num_samples,
            seed=args.seed
        )
        all_results.append(result4)
    
    # ìµœì¢… ë¹„êµ ë¦¬í¬íŠ¸
    print(f"\n\n{'#'*80}")
    print("ìµœì¢… ë¹„êµ ê²°ê³¼")
    print(f"{'#'*80}\n")
    
    print(f"{'ì„¤ì •':<30} {'í‰ê·  ê¸¸ì´':<12} {'í‰ê·  ì‹œê°„':<12} {'í‰ê·  í’ˆì§ˆ':<12}")
    print("="*80)
    
    for result in all_results:
        print(f"{result['config_name']:<30} "
              f"{result['avg_length']:<12.0f} "
              f"{result['avg_time']:<12.2f} "
              f"{result['avg_quality']:<12.2f}")
    
    print("="*80)
    
    # ìŠ¹ì ê²°ì •
    best_quality = max(all_results, key=lambda x: x['avg_quality'])
    best_speed = min(all_results, key=lambda x: x['avg_time'])
    
    print(f"\nğŸ† ìµœê³  í’ˆì§ˆ: {best_quality['config_name']} ({best_quality['avg_quality']:.2f}/10)")
    print(f"âš¡ ìµœê³  ì†ë„: {best_speed['config_name']} ({best_speed['avg_time']:.2f}ì´ˆ)")
    
    # ê²°ê³¼ ì €ì¥
    with open(args.output, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ“ ê²°ê³¼ ì €ì¥: {args.output}\n")


if __name__ == "__main__":
    main()

