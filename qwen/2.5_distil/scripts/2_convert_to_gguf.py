#!/usr/bin/env python3
"""
Step 2: Merged ëª¨ë¸ì„ GGUFë¡œ ë³€í™˜
- Llama.cpp, Ollama í˜¸í™˜ í¬ë§·
- ì—¬ëŸ¬ ì–‘ìí™” ë ˆë²¨ (Q4_K_M, Q5_K_M, Q8_0, F16)
"""

import os
import subprocess
from pathlib import Path

# ì„¤ì •
MERGED_MODEL_DIR = "/home/work/.setting/qwen/2.5_distil/outputs/merged"
OUTPUT_DIR = "/home/work/.setting/qwen/2.5_distil/outputs/gguf"
LLAMA_CPP_DIR = "/home/work/llama.cpp"  # llama.cpp ê²½ë¡œ

# Step 1: HF â†’ F16 GGUF ë³€í™˜ (convert_hf_to_gguf.py)
# Step 2: F16 â†’ ì–‘ìí™” ë ˆë²¨ (llama-quantize)
QUANTIZATION_LEVELS = [
    ("Q4_K_M", "4-bit ì¤‘ê°„ í’ˆì§ˆ (ê¶Œì¥, ë¹ ë¦„)"),
    ("Q5_K_M", "5-bit ì¤‘ê°„ í’ˆì§ˆ (ê· í˜•)"),
    ("Q8_0", "8-bit ê³ í’ˆì§ˆ"),
    ("F16", "16-bit ì›ë³¸ (ë³€í™˜ë§Œ)")
]

print("=" * 80)
print("GGUF ë³€í™˜ ì‹œì‘")
print("=" * 80)

# 1. llama.cpp í™•ì¸
if not Path(LLAMA_CPP_DIR).exists():
    print(f"\nâŒ llama.cppë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {LLAMA_CPP_DIR}")
    print(f"\nğŸ“¥ llama.cpp í´ë¡  ì¤‘...")
    subprocess.run([
        "git", "clone", "https://github.com/ggerganov/llama.cpp",
        LLAMA_CPP_DIR
    ], check=True)
    print(f"  âœ… llama.cpp í´ë¡  ì™„ë£Œ")
    
    print(f"\nğŸ”§ llama.cpp ë¹Œë“œ ì¤‘ (CMake)...")
    # CMake ë¹Œë“œ (ìµœì‹  llama.cpp)
    build_dir = Path(LLAMA_CPP_DIR) / "build"
    build_dir.mkdir(exist_ok=True)
    
    # CMake ì„¤ì •
    subprocess.run([
        "cmake", "-B", str(build_dir), "-S", LLAMA_CPP_DIR,
        "-DCMAKE_BUILD_TYPE=Release",
        "-DGGML_CUDA=ON"  # GPU ì§€ì›
    ], check=True)
    
    # ë¹Œë“œ
    subprocess.run([
        "cmake", "--build", str(build_dir), "--config", "Release", "-j"
    ], check=True)
    
    print(f"  âœ… llama.cpp ë¹Œë“œ ì™„ë£Œ")
else:
    print(f"âœ… llama.cpp í™•ì¸ë¨: {LLAMA_CPP_DIR}")
    
    # ë¹Œë“œê°€ ì•ˆ ë˜ì–´ ìˆìœ¼ë©´ ë¹Œë“œ
    build_dir = Path(LLAMA_CPP_DIR) / "build"
    if not build_dir.exists():
        print(f"\nğŸ”§ llama.cpp ë¹Œë“œ ì¤‘ (CMake)...")
        build_dir.mkdir(exist_ok=True)
        
        subprocess.run([
            "cmake", "-B", str(build_dir), "-S", LLAMA_CPP_DIR,
            "-DCMAKE_BUILD_TYPE=Release",
            "-DGGML_CUDA=ON"
        ], check=True)
        
        subprocess.run([
            "cmake", "--build", str(build_dir), "--config", "Release", "-j"
        ], check=True)
        
        print(f"  âœ… llama.cpp ë¹Œë“œ ì™„ë£Œ")

# 2. ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸ í™•ì¸ (ì—¬ëŸ¬ ê°€ëŠ¥í•œ ì´ë¦„ ì‹œë„)
possible_scripts = [
    Path(LLAMA_CPP_DIR) / "convert_hf_to_gguf.py",
    Path(LLAMA_CPP_DIR) / "convert-hf-to-gguf.py",
    Path(LLAMA_CPP_DIR) / "convert.py"
]

convert_script = None
for script in possible_scripts:
    if script.exists():
        convert_script = script
        break

if not convert_script:
    print(f"âŒ ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    print(f"  ì‹œë„í•œ ê²½ë¡œ:")
    for script in possible_scripts:
        print(f"    - {script}")
    exit(1)

print(f"âœ… ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸ í™•ì¸ë¨: {convert_script.name}")

# 3. Merged ëª¨ë¸ í™•ì¸
if not Path(MERGED_MODEL_DIR).exists():
    print(f"\nâŒ Merged ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {MERGED_MODEL_DIR}")
    print(f"  ğŸ’¡ ë¨¼ì € 1_merge_lora.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”")
    exit(1)

print(f"âœ… Merged ëª¨ë¸ í™•ì¸ë¨: {MERGED_MODEL_DIR}")

# 4. ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(OUTPUT_DIR, exist_ok=True)

# 5. Step 1: HF â†’ F16 GGUF ë³€í™˜
f16_file = Path(OUTPUT_DIR) / "qwen25-3b-korean-F16.gguf"

print(f"\n[ Step 1 ] HF â†’ F16 GGUF ë³€í™˜")
print(f"  â„¹ï¸  ì¶œë ¥: {f16_file.name}")

if f16_file.exists():
    print(f"  âš ï¸  F16 íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤, ìŠ¤í‚µ")
    size_mb = f16_file.stat().st_size / (1024 * 1024)
    print(f"  â„¹ï¸  í¬ê¸°: {size_mb:.2f} MB")
else:
    print(f"  ğŸ”„ ë³€í™˜ ì¤‘...")
    cmd = [
        "python", str(convert_script),
        MERGED_MODEL_DIR,
        "--outtype", "f16",
        "--outfile", str(f16_file)
    ]
    
    try:
        subprocess.run(cmd, check=True)
        print(f"  âœ… F16 GGUF ë³€í™˜ ì™„ë£Œ")
        size_mb = f16_file.stat().st_size / (1024 * 1024)
        print(f"  â„¹ï¸  í¬ê¸°: {size_mb:.2f} MB")
    except subprocess.CalledProcessError as e:
        print(f"  âŒ ë³€í™˜ ì‹¤íŒ¨: {e}")
        exit(1)

# 6. Step 2: F16 â†’ ì–‘ìí™” ë ˆë²¨ (llama-quantize)
quantize_bin = Path(LLAMA_CPP_DIR) / "build" / "bin" / "llama-quantize"
if not quantize_bin.exists():
    # ëŒ€ì•ˆ ê²½ë¡œ
    quantize_bin = Path(LLAMA_CPP_DIR) / "llama-quantize"

if not quantize_bin.exists():
    print(f"\nâŒ llama-quantizeë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    print(f"  ì‹œë„í•œ ê²½ë¡œ:")
    print(f"    - {Path(LLAMA_CPP_DIR) / 'build' / 'bin' / 'llama-quantize'}")
    print(f"    - {Path(LLAMA_CPP_DIR) / 'llama-quantize'}")
    exit(1)

print(f"\n[ Step 2 ] F16 â†’ ì–‘ìí™” ë ˆë²¨")
print(f"  â„¹ï¸  ë„êµ¬: {quantize_bin.name}")

# F16 ì œì™¸í•˜ê³  ì–‘ìí™” ì§„í–‰
for i, (quant_type, description) in enumerate([q for q in QUANTIZATION_LEVELS if q[0] != "F16"], 1):
    quant_type, description = quant_type, description
    output_file = Path(OUTPUT_DIR) / f"qwen25-3b-korean-{quant_type}.gguf"
    
    print(f"\n[ {i}/3 ] {quant_type}: {description}")
    print(f"  â„¹ï¸  ì¶œë ¥: {output_file.name}")
    
    if output_file.exists():
        print(f"  âš ï¸  íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤, ìŠ¤í‚µ")
        size_mb = output_file.stat().st_size / (1024 * 1024)
        print(f"  â„¹ï¸  í¬ê¸°: {size_mb:.2f} MB")
        continue
    
    # llama-quantize ì‹¤í–‰
    cmd = [
        str(quantize_bin),
        str(f16_file),
        str(output_file),
        quant_type
    ]
    
    print(f"  ğŸ”„ ì–‘ìí™” ì¤‘...")
    try:
        subprocess.run(cmd, check=True, capture_output=True)
        print(f"  âœ… ì–‘ìí™” ì™„ë£Œ")
        
        # íŒŒì¼ í¬ê¸° ì¶œë ¥
        size_mb = output_file.stat().st_size / (1024 * 1024)
        print(f"  â„¹ï¸  í¬ê¸°: {size_mb:.2f} MB")
        
    except subprocess.CalledProcessError as e:
        print(f"  âŒ ì–‘ìí™” ì‹¤íŒ¨: {e}")
        if e.stderr:
            print(f"  stderr: {e.stderr.decode()}")
        continue

# 6. ê²°ê³¼ ìš”ì•½
print(f"\n{'=' * 80}")
print(f"ğŸ‰ GGUF ë³€í™˜ ì™„ë£Œ!")
print(f"{'=' * 80}")

print(f"\nğŸ“‚ ìƒì„±ëœ íŒŒì¼:")
gguf_files = sorted(Path(OUTPUT_DIR).glob("*.gguf"))
total_size = 0
for gguf_file in gguf_files:
    size_mb = gguf_file.stat().st_size / (1024 * 1024)
    total_size += size_mb
    print(f"  - {gguf_file.name:40s} ({size_mb:>8.2f} MB)")

print(f"\nğŸ“Š ì´ í¬ê¸°: {total_size:.2f} MB ({total_size / 1024:.2f} GB)")

print(f"\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„: HuggingFace Hub ì—…ë¡œë“œ")
print(f"  python 3_upload_to_hub.py")

# 7. í…ŒìŠ¤íŠ¸ ëª…ë ¹ì–´ ì¶œë ¥
print(f"\nğŸ§ª í…ŒìŠ¤íŠ¸ ëª…ë ¹ì–´ (Llama.cpp):")
if gguf_files:
    test_file = gguf_files[0]  # ì²« ë²ˆì§¸ íŒŒì¼ (ë³´í†µ Q4_K_M)
    print(f"""
{LLAMA_CPP_DIR}/main \\
    -m {test_file} \\
    -p "<|im_start|>user\\ní•œêµ­ì˜ ìˆ˜ë„ëŠ”?<|im_end|>\\n<|im_start|>assistant\\n" \\
    -n 512 \\
    --temp 0.7 \\
    -ngl 99
""")

