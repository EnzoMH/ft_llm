#!/usr/bin/env python3
"""
Step 1: LoRA μ–΄λ‘ν„°λ¥Ό Base λ¨λΈκ³Ό Merge
- PEFT μ–΄λ‘ν„°λ¥Ό λ² μ΄μ¤ λ¨λΈκ³Ό λ³‘ν•©
- μ™„μ „ν• λ¨λΈλ΅ μ €μ¥ (λ¨λ“  ν”„λ μ„μ›ν¬ νΈν™)
"""

import os
import torch
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# μ„¤μ •
BASE_MODEL = "Qwen/Qwen2.5-3B-Instruct"
LORA_ADAPTER = "MyeongHo0621/Qwen2.5-3B-Korean-QLoRA"
LORA_CHECKPOINT = None  # None = λ£¨νΈ κ²½λ΅ μ‚¬μ©, "final" = final ν΄λ” μ‚¬μ©
OUTPUT_DIR = "/home/work/.setting/qwen/2.5_distil/outputs/merged"

print("=" * 80)
print("LoRA μ–΄λ‘ν„° Merge μ‹μ‘")
print("=" * 80)

# μ¶λ ¥ λ””λ ‰ν† λ¦¬ μƒμ„±
os.makedirs(OUTPUT_DIR, exist_ok=True)

# 1. Base λ¨λΈ λ΅λ”©
print(f"\n[ 1/4 ] Base λ¨λΈ λ΅λ”©: {BASE_MODEL}")
base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    torch_dtype=torch.bfloat16,  # λ©”λ¨λ¦¬ μ μ•½
    device_map="auto",
    trust_remote_code=True
)
print(f"  β… Base λ¨λΈ λ΅λ”© μ™„λ£")
print(f"  β„ΉοΈ  νλΌλ―Έν„° μ: {base_model.num_parameters() / 1e9:.2f}B")

# 2. LoRA μ–΄λ‘ν„° λ΅λ”©
if LORA_CHECKPOINT:
    print(f"\n[ 2/4 ] LoRA μ–΄λ‘ν„° λ΅λ”©: {LORA_ADAPTER}/{LORA_CHECKPOINT}")
    model = PeftModel.from_pretrained(
        base_model,
        LORA_ADAPTER,
        subfolder=LORA_CHECKPOINT
    )
else:
    print(f"\n[ 2/4 ] LoRA μ–΄λ‘ν„° λ΅λ”©: {LORA_ADAPTER} (λ£¨νΈ)")
    model = PeftModel.from_pretrained(
        base_model,
        LORA_ADAPTER
    )
print(f"  β… LoRA μ–΄λ‘ν„° λ΅λ”© μ™„λ£ (step 4689, μµμΆ… λ¨λΈ)")

# 3. Merge
print(f"\n[ 3/4 ] LoRA μ–΄λ‘ν„°λ¥Ό Base λ¨λΈκ³Ό Merge μ¤‘...")
print(f"  β„ΉοΈ  λ©”λ¨λ¦¬ μ‚¬μ©λ‰: {torch.cuda.memory_allocated() / 1e9:.2f}GB")
merged_model = model.merge_and_unload()
print(f"  β… Merge μ™„λ£!")

# 4. μ €μ¥
print(f"\n[ 4/4 ] Merged λ¨λΈ μ €μ¥ μ¤‘: {OUTPUT_DIR}")
merged_model.save_pretrained(
    OUTPUT_DIR,
    safe_serialization=True,  # safetensors μ‚¬μ©
    max_shard_size="2GB"
)
print(f"  β… λ¨λΈ μ €μ¥ μ™„λ£")

# 5. ν† ν¬λ‚μ΄μ € μ €μ¥
print(f"\n[ 5/4 ] ν† ν¬λ‚μ΄μ € μ €μ¥ μ¤‘...")
tokenizer = AutoTokenizer.from_pretrained(
    BASE_MODEL,
    trust_remote_code=True
)
tokenizer.save_pretrained(OUTPUT_DIR)
print(f"  β… ν† ν¬λ‚μ΄μ € μ €μ¥ μ™„λ£")

# 6. μ €μ¥λ νμΌ ν™•μΈ
print(f"\nπ“‚ μ €μ¥λ νμΌ:")
for file in sorted(Path(OUTPUT_DIR).glob("*")):
    if file.is_file():
        size_mb = file.stat().st_size / (1024 * 1024)
        print(f"  - {file.name:40s} ({size_mb:>8.2f} MB)")

print(f"\n{'=' * 80}")
print(f"π‰ Merge μ™„λ£!")
print(f"{'=' * 80}")
print(f"\nπ“ μ¶λ ¥ λ””λ ‰ν† λ¦¬: {OUTPUT_DIR}")
print(f"\nπ’΅ λ‹¤μ λ‹¨κ³„: GGUF λ³€ν™")
print(f"  python 2_convert_to_gguf.py")

