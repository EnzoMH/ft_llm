#!/usr/bin/env python3
"""
Private HuggingFace ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸
"""
import os
import json
from pathlib import Path
from dotenv import load_dotenv
from datasets import load_dataset

# .env íŒŒì¼ ë¡œë“œ
# /home/work/.setting/qwen/2.5_distil/scripts/download_data.py
# -> /home/work/.setting/.env
env_path = Path(__file__).resolve().parent.parent.parent.parent / ".env"
if env_path.exists():
    load_dotenv(env_path)
    print(f"âœ… .env íŒŒì¼ ë¡œë“œ: {env_path}")
else:
    # ì ˆëŒ€ ê²½ë¡œë¡œë„ ì‹œë„
    env_path = Path("/home/work/.setting/.env")
    if env_path.exists():
        load_dotenv(env_path)
        print(f"âœ… .env íŒŒì¼ ë¡œë“œ (ì ˆëŒ€ ê²½ë¡œ): {env_path}")

# HF_TOKEN í™•ì¸
hf_token = os.getenv("HF_TOKEN")
if not hf_token:
    print("âŒ HF_TOKENì´ .env íŒŒì¼ì— ì—†ìŠµë‹ˆë‹¤!")
    exit(1)

print(f"âœ… HF_TOKEN ë°œê²¬: {hf_token[:10]}...")

# ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
dataset_name = "MyeongHo0621/Qwen2.5-14B-Korean-Data"
output_dir = Path("/home/work/.setting/data")
output_dir.mkdir(parents=True, exist_ok=True)
output_path = output_dir / "smol_koreantalk_full.jsonl"

print(f"\nğŸ“¥ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {dataset_name}")
print(f"   íŒŒì¼: smol_koreantalk_full.jsonl")
print(f"   ì €ì¥ ê²½ë¡œ: {output_path}")

try:
    # Private ë°ì´í„°ì…‹ì—ì„œ íŠ¹ì • íŒŒì¼ ë‹¤ìš´ë¡œë“œ (í† í° ì‚¬ìš©)
    # ë°ì´í„°ì…‹ì´ ì•„ë‹Œ íŒŒì¼ì¸ ê²½ìš° hf_hub_download ì‚¬ìš©
    try:
        from huggingface_hub import hf_hub_download
        import json
        
        print("ğŸ“¥ HuggingFace Hubì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘...")
        # íŒŒì¼ ì§ì ‘ ë‹¤ìš´ë¡œë“œ
        downloaded_file = hf_hub_download(
            repo_id=dataset_name,
            filename="smol_koreantalk_full.jsonl",
            token=hf_token,
            repo_type="dataset"
        )
        
        print(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {downloaded_file}")
        print(f"ğŸ“ íŒŒì¼ ë³µì‚¬ ì¤‘...")
        
        # ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ì„ ëª©ì ì§€ë¡œ ë³µì‚¬
        import shutil
        shutil.copy2(downloaded_file, output_path)
        
        # íŒŒì¼ì—ì„œ ìƒ˜í”Œ ìˆ˜ í™•ì¸
        count = 0
        with open(output_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    count += 1
        
        print(f"\nâœ… ì €ì¥ ì™„ë£Œ!")
        print(f"   íŒŒì¼: {output_path}")
        print(f"   ìƒ˜í”Œ ìˆ˜: {count:,}ê°œ")
        
    except Exception as e:
        # íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ì‹œ ë°ì´í„°ì…‹ìœ¼ë¡œ ì‹œë„
        print(f"âš ï¸  íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨, ë°ì´í„°ì…‹ìœ¼ë¡œ ì‹œë„: {e}")
        dataset = load_dataset(
            dataset_name,
            split="train",
            token=hf_token  # Private ë°ì´í„°ì…‹ ì ‘ê·¼ì„ ìœ„í•œ í† í°
        )
    
        print(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {len(dataset):,}ê°œ ìƒ˜í”Œ")
        print(f"ğŸ“ JSONL íŒŒì¼ë¡œ ì €ì¥ ì¤‘...")
        
        # JSONLë¡œ ì €ì¥
        with open(output_path, 'w', encoding='utf-8') as f:
            for i, item in enumerate(dataset):
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
                if (i + 1) % 10000 == 0:
                    print(f"   ì§„í–‰: {i+1:,}/{len(dataset):,}")
        
        print(f"\nâœ… ì €ì¥ ì™„ë£Œ!")
        print(f"   íŒŒì¼: {output_path}")
        print(f"   ìƒ˜í”Œ ìˆ˜: {len(dataset):,}ê°œ")
    
    # íŒŒì¼ í¬ê¸° í™•ì¸
    size_mb = output_path.stat().st_size / (1024 * 1024)
    print(f"   íŒŒì¼ í¬ê¸°: {size_mb:.2f} MB")
    
except Exception as e:
    print(f"\nâŒ ì—ëŸ¬ ë°œìƒ: {e}")
    import traceback
    traceback.print_exc()
    exit(1)

