#!/usr/bin/env python3
"""
Qwen2.5-3B-Instruct í•œêµ­ì–´ ë©€í‹°í„´ ëŒ€í™” íŒŒì¸íŠœë‹
- H100 80GB ìµœì í™”
- Flash Attention 3
- smol_koreantalk_full.jsonl ë°ì´í„°ì…‹
- LoRA + 8bit ì–‘ìí™”
"""

import os
import sys
import logging
import time
import torch
import argparse

# ë©€í‹°í”„ë¡œì„¸ì‹± ìµœì  ì„¤ì •
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["OMP_NUM_THREADS"] = "1"
os.environ["MKL_NUM_THREADS"] = "1"
os.environ["NUMEXPR_NUM_THREADS"] = "1"

# CUDA ë©”ëª¨ë¦¬ ìµœì í™”
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

# CPU ì½”ì–´ ìˆ˜ ì œí•œ (RAM ì••ë°• ë°©ì§€ ë° ë©€í‹°í”„ë¡œì„¸ì‹± ì•ˆì •ì„± í–¥ìƒ)
# ì‹œìŠ¤í…œ: 24 ì½”ì–´, 63GB RAM â†’ 8 ì½”ì–´ë¡œ ì œí•œí•˜ì—¬ ì•ˆì •ì„± í™•ë³´
import multiprocessing
_original_mp_cpu_count = multiprocessing.cpu_count
_original_os_cpu_count = os.cpu_count
LIMIT_CPU_CORES = 8  # RAM 51GB ì‚¬ìš© ê°€ëŠ¥, ì•ˆì •ì ì¸ í† í¬ë‚˜ì´ì§•ì„ ìœ„í•´ 8ê°œë¡œ ì œí•œ
multiprocessing.cpu_count = lambda: LIMIT_CPU_CORES
os.cpu_count = lambda: LIMIT_CPU_CORES

# psutilë„ ì˜¤ë²„ë¼ì´ë“œ
try:
    import psutil
    _original_psutil_cpu_count = psutil.cpu_count
    psutil.cpu_count = lambda logical=True: LIMIT_CPU_CORES
except ImportError:
    pass

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
_current_dir = os.path.dirname(os.path.abspath(__file__))
_project_root = os.path.dirname(_current_dir)  # scriptsì˜ ìƒìœ„ ë””ë ‰í† ë¦¬
_src_dir = os.path.join(_project_root, 'src')
if _src_dir not in sys.path:
    sys.path.insert(0, _src_dir)

# ëª¨ë“ˆ ì„í¬íŠ¸
from qwen_finetuning_3b import Qwen3BFineTuner


def load_config_from_checkpoint(checkpoint_path: str):
    """Checkpointì—ì„œ ì„¤ì • ìë™ ê°ì§€"""
    import json
    from pathlib import Path
    
    adapter_config_path = Path(checkpoint_path) / "adapter_config.json"
    
    if not adapter_config_path.exists():
        logger.warning(f"âš ï¸  adapter_config.json ì—†ìŒ: {checkpoint_path}")
        return None
    
    try:
        with open(adapter_config_path, 'r') as f:
            config = json.load(f)
        
        lora_r = config.get('r', 32)
        lora_alpha = config.get('lora_alpha', 64)
        lora_dropout = config.get('lora_dropout', 0.0)
        
        logger.info(f"ğŸ“‚ Checkpoint ì„¤ì • ê°ì§€:")
        logger.info(f"   LoRA r={lora_r}, alpha={lora_alpha}, dropout={lora_dropout}")
        
        # r ê°’ìœ¼ë¡œ LoRA vs QLoRA ì¶”ì •
        if lora_r >= 64:
            logger.info(f"   â†’ QLoRA (4bit) ì„¤ì •ìœ¼ë¡œ ì¶”ì •")
            return "qlora"
        else:
            logger.info(f"   â†’ LoRA (8bit) ì„¤ì •ìœ¼ë¡œ ì¶”ì •")
            return "lora"
    
    except Exception as e:
        logger.warning(f"âš ï¸  Checkpoint ì„¤ì • ì½ê¸° ì‹¤íŒ¨: {e}")
        return None


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="Qwen2.5-3B íŒŒì¸íŠœë‹")
    parser.add_argument(
        "--resume_from_checkpoint",
        type=str,
        default=None,
        help="ì¬ê°œí•  checkpoint ê²½ë¡œ (Hub ëª¨ë¸ ID ë˜ëŠ” ë¡œì»¬ ê²½ë¡œ). ì˜ˆ: MyeongHo0621/Qwen2.5-3B-Korean ë˜ëŠ” outputs/checkpoints/checkpoint-2500"
    )
    parser.add_argument(
        "--config",
        type=str,
        choices=["lora", "qlora", "auto"],
        default="auto",
        help="ì„¤ì • ì„ íƒ: lora (8bit), qlora (4bit), auto (checkpoint ìë™ ê°ì§€)"
    )
    args = parser.parse_args()
    
    print("\n" + "="*80)
    print(" Qwen2.5-3B-Instruct í•œêµ­ì–´ ë©€í‹°í„´ ëŒ€í™” íŒŒì¸íŠœë‹")
    print(" H100 80GB ìµœì í™” | Flash Attention 2.8.3")
    print("="*80)
    
    # GPU í™•ì¸
    if not torch.cuda.is_available():
        print("\n[ ERROR ] CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        sys.exit(1)
    
    gpu_name = torch.cuda.get_device_name(0)
    gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
    
    print(f"\nGPU: {gpu_name}")
    print(f"ë©”ëª¨ë¦¬: {gpu_memory_gb:.1f}GB")
    
    # ì„¤ì •
    config = Qwen3BFineTuningConfig()
    
    # Checkpoint ì¬ê°œ ì •ë³´ ì¶œë ¥
    if args.resume_from_checkpoint:
        print(f"\n[ INFO ] Checkpointì—ì„œ ì¬ê°œ: {args.resume_from_checkpoint}")
    
    print(f"\n{'='*80}")
    print(" ì„¤ì • ìš”ì•½")
    print(f"{'='*80}")
    print(f"ëª¨ë¸: {config.base_model}")
    print(f"ë°ì´í„°: {config.korean_data_dir}")
    print(f"  íŒŒì¼: {', '.join(config.data_files)}")
    print(f"  ìµœëŒ€ ìƒ˜í”Œ: {config.max_samples if config.max_samples else 'ì „ì²´'}")
    print(f"ì¶œë ¥: {config.output_dir}")
    print(f"LoRA: r={config.lora_r}, alpha={config.lora_alpha}, dropout={config.lora_dropout}")
    print(f"Epoch: {config.num_train_epochs}")
    print(f"ë°°ì¹˜: {config.per_device_train_batch_size} Ã— {config.gradient_accumulation_steps} = {config.per_device_train_batch_size * config.gradient_accumulation_steps}")
    print(f"í•™ìŠµë¥ : {config.learning_rate}")
    print(f"Max Seq: {config.max_seq_length}")
    print(f"ì²´í¬í¬ì¸íŠ¸: {config.save_steps} stepë§ˆë‹¤")
    if config.pre_tokenize_dataset:
        print(f"ë°ì´í„°ì…‹ ì²˜ë¦¬: ë¯¸ë¦¬ í† í¬ë‚˜ì´ì§• (ë©€í‹°í”„ë¡œì„¸ì‹± ì˜¤ë¥˜ ë°©ì§€)")
    else:
        print(f"ë°ì´í„°ì…‹ ì²˜ë¦¬: {config.dataset_num_proc} í”„ë¡œì„¸ìŠ¤ (CPU ì½”ì–´ ì œí•œ: 8ê°œ)")
    print(f"Hub ì—…ë¡œë“œ: {config.hub_model_id}")
    print(f"{'='*80}\n")
    
    # íŒŒì¸íŠœë„ˆ
    finetuner = Qwen3BFineTuner(config)
    
    try:
        # 1. ëª¨ë¸ ë¡œë“œ
        finetuner.load_model()
        
        # 2. ë°ì´í„° ë¡œë“œ
        dataset = finetuner.load_data()
        num_samples = len(dataset)
        
        # 3. í›ˆë ¨
        train_start = time.perf_counter()
        model_path = finetuner.train(dataset, resume_from_checkpoint=args.resume_from_checkpoint)
        train_end = time.perf_counter()
        
        total_time = train_end - train_start
        
        # ëŒ€ëµì ì¸ ì´ í† í° ìˆ˜ ì¶”ì •
        total_tokens = (
            num_samples
            * config.max_seq_length
            * config.num_train_epochs
        )
        
        tokens_per_sec = total_tokens / total_time if total_time > 0 else 0
        
        print(f"\n{'='*80}")
        print(" ì™„ë£Œ!")
        print(f"{'='*80}")
        print(f"ëª¨ë¸: {model_path}")
        print(f"ì´ ìƒ˜í”Œ ìˆ˜: {num_samples:,}")
        print(f"ì´ í† í° ìˆ˜(ëŒ€ëµ): {total_tokens:,}")
        print(f"ì´ í•™ìŠµ ì‹œê°„: {total_time/3600:.2f} ì‹œê°„")
        print(f"í‰ê·  ì²˜ë¦¬ ì†ë„: {tokens_per_sec:,.0f} tokens/sec")
        print(f"{'='*80}\n")
        
    except Exception as e:
        logger.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        raise


if __name__ == "__main__":
    main()

