{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Qwen2.5-3B-Instruct 한국어 멀티턴 대화 파인튜닝\n",
        "\n",
        "- H100 80GB 최적화\n",
        "- Flash Attention 3\n",
        "- smol_koreantalk_full.jsonl 데이터셋\n",
        "- LoRA + 8bit 양자화\n",
        "- 데이터셋 미리 토크나이징 (멀티프로세싱 오류 방지)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# 환경 설정 및 임포트\n",
        "import os\n",
        "import sys\n",
        "import logging\n",
        "import time\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "# 멀티프로세싱 최적 설정\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
        "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
        "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
        "\n",
        "# CUDA 메모리 최적화\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# CPU 코어 수 제한 (RAM 압박 방지 및 멀티프로세싱 안정성 향상)\n",
        "# 시스템: 24 코어, 63GB RAM → 8 코어로 제한하여 안정성 확보\n",
        "import multiprocessing\n",
        "_original_mp_cpu_count = multiprocessing.cpu_count\n",
        "_original_os_cpu_count = os.cpu_count\n",
        "LIMIT_CPU_CORES = 8  # RAM 51GB 사용 가능, 안정적인 토크나이징을 위해 8개로 제한\n",
        "multiprocessing.cpu_count = lambda: LIMIT_CPU_CORES\n",
        "os.cpu_count = lambda: LIMIT_CPU_CORES\n",
        "\n",
        "# psutil도 오버라이드\n",
        "try:\n",
        "    import psutil\n",
        "    _original_psutil_cpu_count = psutil.cpu_count\n",
        "    psutil.cpu_count = lambda logical=True: LIMIT_CPU_CORES\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "# 로깅 설정\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO, \n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# 모듈 경로 추가\n",
        "# Jupyter notebook에서는 현재 작업 디렉토리 기준으로 경로 설정\n",
        "_current_dir = Path.cwd()\n",
        "# scripts 디렉토리에서 실행 중인지 확인\n",
        "if _current_dir.name == 'scripts':\n",
        "    _project_root = _current_dir.parent\n",
        "else:\n",
        "    # 상위 디렉토리에서 실행 중인 경우\n",
        "    _project_root = _current_dir\n",
        "_src_dir = _project_root / 'src'\n",
        "if str(_src_dir) not in sys.path:\n",
        "    sys.path.insert(0, str(_src_dir))\n",
        "\n",
        "# 모듈 임포트\n",
        "from qwen_finetuning_3b import Qwen3BFineTuningConfig, Qwen3BFineTuner\n",
        "\n",
        "print(\"환경 설정 완료!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 시스템 리소스 확인\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            " 시스템 정보\n",
            "================================================================================\n",
            "GPU: NVIDIA H100 80GB HBM3\n",
            "GPU 메모리: 79.2GB\n",
            "CUDA 사용 가능: True\n",
            "CUDA 버전: 12.8\n",
            "\n",
            "CPU 코어 수: 8 (제한: 8)\n",
            "총 RAM: 63.0 GB\n",
            "사용 가능 RAM: 34.6 GB\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# GPU 확인\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"\\n[ ERROR ] CUDA를 사용할 수 없습니다!\")\n",
        "    raise RuntimeError(\"CUDA를 사용할 수 없습니다!\")\n",
        "\n",
        "gpu_name = torch.cuda.get_device_name(0)\n",
        "gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\" 시스템 정보\")\n",
        "print(\"=\"*80)\n",
        "print(f\"GPU: {gpu_name}\")\n",
        "print(f\"GPU 메모리: {gpu_memory_gb:.1f}GB\")\n",
        "print(f\"CUDA 사용 가능: {torch.cuda.is_available()}\")\n",
        "print(f\"CUDA 버전: {torch.version.cuda}\")\n",
        "\n",
        "# CPU 및 RAM 정보\n",
        "try:\n",
        "    import psutil\n",
        "    mem = psutil.virtual_memory()\n",
        "    print(f\"\\nCPU 코어 수: {psutil.cpu_count()} (제한: {LIMIT_CPU_CORES})\")\n",
        "    print(f\"총 RAM: {mem.total/(1024**3):.1f} GB\")\n",
        "    print(f\"사용 가능 RAM: {mem.available/(1024**3):.1f} GB\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 설정 생성 및 확인\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            " 설정 요약\n",
            "================================================================================\n",
            "모델: Qwen/Qwen2.5-3B-Instruct\n",
            "데이터: /home/work/vss/ft_llm/data\n",
            "  파일: smol_koreantalk_full.jsonl\n",
            "  최대 샘플: 전체\n",
            "출력: /home/work/vss/ft_llm/qwen/2.5_distil/outputs/checkpoints\n",
            "LoRA: r=32, alpha=64, dropout=0.0\n",
            "Epoch: 1\n",
            "배치: 32 × 4 = 128\n",
            "학습률: 0.0002\n",
            "Max Seq: 2048\n",
            "체크포인트: 500 step마다\n",
            "데이터셋 처리: 미리 토크나이징 (멀티프로세싱 오류 방지)\n",
            "Hub 업로드: MyeongHo0621/Qwen2.5-3B-Korean\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 설정 생성\n",
        "config = Qwen3BFineTuningConfig()\n",
        "\n",
        "# Checkpoint 재개 설정 (필요시 수정)\n",
        "resume_from_checkpoint = None  # 예: \"MyeongHo0621/Qwen2.5-3B-Korean\" 또는 \"outputs/checkpoints/checkpoint-2500\"\n",
        "\n",
        "# 설정 요약 출력\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" 설정 요약\")\n",
        "print(\"=\"*80)\n",
        "print(f\"모델: {config.base_model}\")\n",
        "print(f\"데이터: {config.korean_data_dir}\")\n",
        "print(f\"  파일: {', '.join(config.data_files)}\")\n",
        "print(f\"  최대 샘플: {config.max_samples if config.max_samples else '전체'}\")\n",
        "print(f\"출력: {config.output_dir}\")\n",
        "print(f\"LoRA: r={config.lora_r}, alpha={config.lora_alpha}, dropout={config.lora_dropout}\")\n",
        "print(f\"Epoch: {config.num_train_epochs}\")\n",
        "print(f\"배치: {config.per_device_train_batch_size} × {config.gradient_accumulation_steps} = {config.per_device_train_batch_size * config.gradient_accumulation_steps}\")\n",
        "print(f\"학습률: {config.learning_rate}\")\n",
        "print(f\"Max Seq: {config.max_seq_length}\")\n",
        "print(f\"체크포인트: {config.save_steps} step마다\")\n",
        "if config.pre_tokenize_dataset:\n",
        "    print(f\"데이터셋 처리: 미리 토크나이징 (멀티프로세싱 오류 방지)\")\n",
        "else:\n",
        "    print(f\"데이터셋 처리: {config.dataset_num_proc} 프로세스 (CPU 코어 제한: 8개)\")\n",
        "print(f\"Hub 업로드: {config.hub_model_id}\")\n",
        "if resume_from_checkpoint:\n",
        "    print(f\"\\n[ INFO ] Checkpoint에서 재개: {resume_from_checkpoint}\")\n",
        "print(\"=\"*80 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 파인튜너 초기화\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "파인튜너 초기화 완료!\n"
          ]
        }
      ],
      "source": [
        "# 파인튜너 생성\n",
        "finetuner = Qwen3BFineTuner(config)\n",
        "print(\"파인튜너 초기화 완료!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 모델 로드\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-18 00:51:36,114 - INFO - ================================================================================\n",
            "2025-11-18 00:51:36,115 - INFO - Qwen2.5-3B-Instruct 모델 로딩\n",
            "2025-11-18 00:51:36,116 - INFO - ================================================================================\n",
            "2025-11-18 00:51:36,116 - INFO - 모델: Qwen/Qwen2.5-3B-Instruct\n",
            "2025-11-18 00:51:36,117 - INFO - Max Seq Length: 2048\n",
            "2025-11-18 00:51:36,117 - INFO - 8bit: True, 4bit: False\n",
            "2025-11-18 00:51:36,117 - INFO - ================================================================================\n",
            "2025-11-18 00:51:36,118 - INFO - \n",
            "================================================================================\n",
            "2025-11-18 00:51:36,119 - INFO - [모델 로드 전] 시스템 리소스 모니터링\n",
            "2025-11-18 00:51:36,119 - INFO - ================================================================================\n",
            "2025-11-18 00:51:36,334 - INFO - ================================================================================\n",
            "2025-11-18 00:51:36,334 - INFO - [모델 로드 전] CPU/RAM 상세 모니터링\n",
            "2025-11-18 00:51:36,335 - INFO -   시간: 2025-11-18 00:51:36\n",
            "2025-11-18 00:51:36,335 - INFO -   CPU 사용률: 36.3% (8 cores @ 2292 MHz)\n",
            "2025-11-18 00:51:36,335 - INFO -   RAM: 28.3GB / 63.0GB (44.9%)\n",
            "2025-11-18 00:51:36,336 - INFO -   프로세스 CPU: 0.0%\n",
            "2025-11-18 00:51:36,336 - INFO -   프로세스 RAM: 1.02GB\n",
            "2025-11-18 00:51:36,336 - INFO -   프로세스 스레드: 61\n",
            "2025-11-18 00:51:36,341 - INFO -   Top 3 프로세스:\n",
            "2025-11-18 00:51:36,342 - INFO -     1. docker-init (PID: 1): CPU 0.0% | RAM 0.0%\n",
            "2025-11-18 00:51:36,342 - INFO -     2. backend.ai: ker (PID: 7): CPU 0.0% | RAM 0.1%\n",
            "2025-11-18 00:51:36,342 - INFO -     3. ssh-agent (PID: 66): CPU 0.0% | RAM 0.0%\n",
            "2025-11-18 00:51:36,343 - INFO - ================================================================================\n",
            "2025-11-18 00:51:36,343 - INFO - ================================================================================\n",
            "2025-11-18 00:51:36,343 - INFO - [모델 로드 전] 전체 GPU 상태\n",
            "2025-11-18 00:51:36,344 - INFO -   총 GPU 수: 1\n",
            "2025-11-18 00:51:36,439 - INFO -   GPU0 (NVIDIA H100 80GB HBM3): 0.0GB/79.2GB (0.0%) | Load: 0%\n",
            "2025-11-18 00:51:36,440 - INFO - ================================================================================\n",
            "2025-11-18 00:51:36,442 - INFO - ================================================================================\n",
            "\n",
            "2025-11-18 00:51:36,442 - INFO - [ INFO ] 모델 다운로드 시작...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: WARNING `trust_remote_code` is True.\n",
            "Are you certain you want to do remote code execution?\n",
            "==((====))==  Unsloth 2025.11.3: Fast Qwen2 patching. Transformers: 4.57.1.\n",
            "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.19 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu128. CUDA: 9.0. CUDA Toolkit: 12.8. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: Qwen2 does not support SDPA - switching to fast eager.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.45s/it]\n",
            "2025-11-18 00:51:57,791 - INFO - [ INFO ] ✅ Flash Attention 3 로드 성공!\n",
            "2025-11-18 00:51:57,792 - INFO - [ COMPLETE ] 베이스 모델 로드 완료\n",
            "2025-11-18 00:51:57,792 - INFO - \n",
            "================================================================================\n",
            "2025-11-18 00:51:57,792 - INFO - [베이스 모델 로드 후] 시스템 리소스 모니터링\n",
            "2025-11-18 00:51:57,793 - INFO - ================================================================================\n",
            "2025-11-18 00:51:58,028 - INFO - ================================================================================\n",
            "2025-11-18 00:51:58,029 - INFO - [베이스 모델 로드 후] CPU/RAM 상세 모니터링\n",
            "2025-11-18 00:51:58,029 - INFO -   시간: 2025-11-18 00:51:58\n",
            "2025-11-18 00:51:58,030 - INFO -   CPU 사용률: 29.1% (8 cores @ 2292 MHz)\n",
            "2025-11-18 00:51:58,030 - INFO -   RAM: 23.6GB / 63.0GB (37.5%)\n",
            "2025-11-18 00:51:58,030 - INFO -   프로세스 CPU: 0.0%\n",
            "2025-11-18 00:51:58,031 - INFO -   프로세스 RAM: 1.23GB\n",
            "2025-11-18 00:51:58,031 - INFO -   프로세스 스레드: 102\n",
            "2025-11-18 00:51:58,035 - INFO -   Top 3 프로세스:\n",
            "2025-11-18 00:51:58,036 - INFO -     1. python (PID: 105596): CPU 153.6% | RAM 1.9%\n",
            "2025-11-18 00:51:58,036 - INFO -     2. python (PID: 99692): CPU 100.0% | RAM 22.8%\n",
            "2025-11-18 00:51:58,037 - INFO -     3. cicc (PID: 103819): CPU 100.0% | RAM 1.7%\n",
            "2025-11-18 00:51:58,037 - INFO - ================================================================================\n",
            "2025-11-18 00:51:58,037 - INFO - ================================================================================\n",
            "2025-11-18 00:51:58,038 - INFO - [베이스 모델 로드 후] 전체 GPU 상태\n",
            "2025-11-18 00:51:58,038 - INFO -   총 GPU 수: 1\n",
            "2025-11-18 00:51:58,175 - INFO -   GPU0 (NVIDIA H100 80GB HBM3): 3.2GB/79.2GB (4.0%) | Load: 0%\n",
            "2025-11-18 00:51:58,176 - INFO - ================================================================================\n",
            "2025-11-18 00:51:58,178 - INFO - ================================================================================\n",
            "\n",
            "2025-11-18 00:51:58,226 - INFO - [ INFO ] Vocab size: 151,665\n",
            "2025-11-18 00:51:58,228 - INFO - [ INFO ] LoRA 설정 적용 중...\n",
            "2025-11-18 00:51:58,229 - INFO -   LoRA r=32, alpha=64\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Making `model.base_model.model.model` require gradients\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-18 00:52:01,396 - INFO - [ COMPLETE ] LoRA 적용 완료\n",
            "2025-11-18 00:52:01,397 - INFO - \n",
            "================================================================================\n",
            "2025-11-18 00:52:01,398 - INFO - [LoRA 적용 후] 시스템 리소스 모니터링\n",
            "2025-11-18 00:52:01,398 - INFO - ================================================================================\n",
            "2025-11-18 00:52:01,613 - INFO - ================================================================================\n",
            "2025-11-18 00:52:01,614 - INFO - [LoRA 적용 후] CPU/RAM 상세 모니터링\n",
            "2025-11-18 00:52:01,614 - INFO -   시간: 2025-11-18 00:52:01\n",
            "2025-11-18 00:52:01,614 - INFO -   CPU 사용률: 29.4% (8 cores @ 2292 MHz)\n",
            "2025-11-18 00:52:01,615 - INFO -   RAM: 22.9GB / 63.0GB (36.3%)\n",
            "2025-11-18 00:52:01,615 - INFO -   프로세스 CPU: 0.0%\n",
            "2025-11-18 00:52:01,616 - INFO -   프로세스 RAM: 1.23GB\n",
            "2025-11-18 00:52:01,616 - INFO -   프로세스 스레드: 102\n",
            "2025-11-18 00:52:01,620 - INFO -   Top 3 프로세스:\n",
            "2025-11-18 00:52:01,621 - INFO -     1. python (PID: 105596): CPU 339.2% | RAM 2.0%\n",
            "2025-11-18 00:52:01,621 - INFO -     2. python (PID: 99692): CPU 100.1% | RAM 22.8%\n",
            "2025-11-18 00:52:01,622 - INFO -     3. cicc (PID: 105349): CPU 100.1% | RAM 1.5%\n",
            "2025-11-18 00:52:01,622 - INFO - ================================================================================\n",
            "2025-11-18 00:52:01,622 - INFO - ================================================================================\n",
            "2025-11-18 00:52:01,623 - INFO - [LoRA 적용 후] 전체 GPU 상태\n",
            "2025-11-18 00:52:01,623 - INFO -   총 GPU 수: 1\n",
            "2025-11-18 00:52:01,793 - INFO -   GPU0 (NVIDIA H100 80GB HBM3): 3.4GB/79.2GB (4.3%) | Load: 0%\n",
            "2025-11-18 00:52:01,794 - INFO - ================================================================================\n",
            "2025-11-18 00:52:01,796 - INFO - ================================================================================\n",
            "\n",
            "2025-11-18 00:52:01,810 - INFO - [ INFO ] 학습 가능 파라미터: 59,867,136 / 3,145,805,824 (1.90%)\n",
            "2025-11-18 00:52:01,811 - INFO - ================================================================================\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "모델: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): Qwen2ForCausalLM(\n",
            "      (model): Qwen2Model(\n",
            "        (embed_tokens): Embedding(151936, 2048, padding_idx=151654)\n",
            "        (layers): ModuleList(\n",
            "          (0-35): 36 x Qwen2DecoderLayer(\n",
            "            (self_attn): Qwen2Attention(\n",
            "              (q_proj): lora.Linear8bitLt(\n",
            "                (base_layer): Linear8bitLt(in_features=2048, out_features=2048, bias=True)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=2048, out_features=32, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=32, out_features=2048, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (k_proj): lora.Linear8bitLt(\n",
            "                (base_layer): Linear8bitLt(in_features=2048, out_features=256, bias=True)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=2048, out_features=32, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=32, out_features=256, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (v_proj): lora.Linear8bitLt(\n",
            "                (base_layer): Linear8bitLt(in_features=2048, out_features=256, bias=True)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=2048, out_features=32, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=32, out_features=256, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (o_proj): lora.Linear8bitLt(\n",
            "                (base_layer): Linear8bitLt(in_features=2048, out_features=2048, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=2048, out_features=32, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=32, out_features=2048, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "            )\n",
            "            (mlp): Qwen2MLP(\n",
            "              (gate_proj): lora.Linear8bitLt(\n",
            "                (base_layer): Linear8bitLt(in_features=2048, out_features=11008, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=2048, out_features=32, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=32, out_features=11008, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (up_proj): lora.Linear8bitLt(\n",
            "                (base_layer): Linear8bitLt(in_features=2048, out_features=11008, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=2048, out_features=32, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=32, out_features=11008, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (down_proj): lora.Linear8bitLt(\n",
            "                (base_layer): Linear8bitLt(in_features=11008, out_features=2048, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Identity()\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=11008, out_features=32, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=32, out_features=2048, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act_fn): SiLUActivation()\n",
            "            )\n",
            "            (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
            "            (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
            "          )\n",
            "        )\n",
            "        (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
            "        (rotary_emb): Qwen2RotaryEmbedding()\n",
            "      )\n",
            "      (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "토크나이저: Qwen2TokenizerFast(name_or_path='unsloth/Qwen2.5-3B-Instruct', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|vision_pad|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
            "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "}\n",
            ")\n",
            "Vocab size: 151,665\n"
          ]
        }
      ],
      "source": [
        "# 모델 로드\n",
        "finetuner.load_model()\n",
        "\n",
        "# 모델 정보 확인\n",
        "print(f\"\\n모델: {finetuner.model}\")\n",
        "print(f\"토크나이저: {finetuner.tokenizer}\")\n",
        "print(f\"Vocab size: {len(finetuner.tokenizer):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 데이터 로드\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-18 00:52:01,891 - INFO - ================================================================================\n",
            "2025-11-18 00:52:01,892 - INFO - 한국어 멀티턴 대화 데이터셋 로딩\n",
            "2025-11-18 00:52:01,892 - INFO - ================================================================================\n",
            "2025-11-18 00:52:01,893 - INFO - 데이터 디렉토리: /home/work/vss/ft_llm/data\n",
            "2025-11-18 00:52:01,893 - INFO - \n",
            "================================================================================\n",
            "2025-11-18 00:52:01,894 - INFO - [데이터 로드 전] 시스템 리소스 모니터링\n",
            "2025-11-18 00:52:01,894 - INFO - ================================================================================\n",
            "2025-11-18 00:52:02,110 - INFO - ================================================================================\n",
            "2025-11-18 00:52:02,111 - INFO - [데이터 로드 전] CPU/RAM 상세 모니터링\n",
            "2025-11-18 00:52:02,111 - INFO -   시간: 2025-11-18 00:52:02\n",
            "2025-11-18 00:52:02,112 - INFO -   CPU 사용률: 30.2% (8 cores @ 2291 MHz)\n",
            "2025-11-18 00:52:02,112 - INFO -   RAM: 23.0GB / 63.0GB (36.6%)\n",
            "2025-11-18 00:52:02,112 - INFO -   프로세스 CPU: 0.0%\n",
            "2025-11-18 00:52:02,113 - INFO -   프로세스 RAM: 1.23GB\n",
            "2025-11-18 00:52:02,113 - INFO -   프로세스 스레드: 102\n",
            "2025-11-18 00:52:02,117 - INFO -   Top 3 프로세스:\n",
            "2025-11-18 00:52:02,118 - INFO -     1. python (PID: 99692): CPU 100.6% | RAM 22.9%\n",
            "2025-11-18 00:52:02,118 - INFO -     2. cicc (PID: 104265): CPU 100.6% | RAM 2.7%\n",
            "2025-11-18 00:52:02,118 - INFO -     3. cicc (PID: 104559): CPU 100.6% | RAM 2.9%\n",
            "2025-11-18 00:52:02,119 - INFO - ================================================================================\n",
            "2025-11-18 00:52:02,119 - INFO - ================================================================================\n",
            "2025-11-18 00:52:02,119 - INFO - [데이터 로드 전] 전체 GPU 상태\n",
            "2025-11-18 00:52:02,120 - INFO -   총 GPU 수: 1\n",
            "2025-11-18 00:52:02,229 - INFO -   GPU0 (NVIDIA H100 80GB HBM3): 3.4GB/79.2GB (4.3%) | Load: 0%\n",
            "2025-11-18 00:52:02,229 - INFO - ================================================================================\n",
            "2025-11-18 00:52:02,232 - INFO - ================================================================================\n",
            "\n",
            "2025-11-18 00:52:02,232 - INFO -   로딩: smol_koreantalk_full.jsonl\n",
            "2025-11-18 00:52:25,811 - INFO -     추가됨: 460,281개\n",
            "2025-11-18 00:52:25,812 - INFO - [ COMPLETE ] 총 460,281개 데이터 로드\n",
            "2025-11-18 00:53:03,583 - INFO - [ COMPLETE ] 총 460,281개 데이터 로드\n",
            "2025-11-18 00:53:03,586 - INFO - \n",
            "================================================================================\n",
            "2025-11-18 00:53:03,587 - INFO - [데이터 로드 후] 시스템 리소스 모니터링\n",
            "2025-11-18 00:53:03,588 - INFO - ================================================================================\n",
            "2025-11-18 00:53:03,845 - INFO - ================================================================================\n",
            "2025-11-18 00:53:03,846 - INFO - [데이터 로드 후] CPU/RAM 상세 모니터링\n",
            "2025-11-18 00:53:03,846 - INFO -   시간: 2025-11-18 00:53:03\n",
            "2025-11-18 00:53:03,847 - INFO -   CPU 사용률: 28.5% (8 cores @ 2292 MHz)\n",
            "2025-11-18 00:53:03,847 - INFO -   RAM: 37.6GB / 63.0GB (59.8%)\n",
            "2025-11-18 00:53:03,847 - INFO -   프로세스 CPU: 0.0%\n",
            "2025-11-18 00:53:03,848 - INFO -   프로세스 RAM: 10.68GB\n",
            "2025-11-18 00:53:03,848 - INFO -   프로세스 스레드: 102\n",
            "2025-11-18 00:53:03,852 - INFO -   Top 3 프로세스:\n",
            "2025-11-18 00:53:03,853 - INFO -     1. python (PID: 99692): CPU 100.0% | RAM 23.6%\n",
            "2025-11-18 00:53:03,853 - INFO -     2. cicc (PID: 105028): CPU 100.0% | RAM 2.5%\n",
            "2025-11-18 00:53:03,853 - INFO -     3. cicc (PID: 105349): CPU 100.0% | RAM 2.9%\n",
            "2025-11-18 00:53:03,854 - INFO - ================================================================================\n",
            "2025-11-18 00:53:03,854 - INFO - ================================================================================\n",
            "2025-11-18 00:53:03,854 - INFO - [데이터 로드 후] 전체 GPU 상태\n",
            "2025-11-18 00:53:03,855 - INFO -   총 GPU 수: 1\n",
            "2025-11-18 00:53:03,959 - INFO -   GPU0 (NVIDIA H100 80GB HBM3): 3.4GB/79.2GB (4.3%) | Load: 0%\n",
            "2025-11-18 00:53:03,959 - INFO - ================================================================================\n",
            "2025-11-18 00:53:03,961 - INFO - ================================================================================\n",
            "\n",
            "2025-11-18 00:53:03,962 - INFO - ================================================================================\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "데이터 로드 완료!\n",
            "총 샘플 수: 460,281\n",
            "\n",
            "첫 번째 샘플 키: ['messages', 'custom_id']\n",
            "메시지 수: 6\n",
            "첫 번째 메시지: {'content': '제가 편집할 내용이 있어요. 제가 작성한 텍스트는 다음과 같아요,\\n\\n\"저와 제 친구들은 오랫동안 영화를 다시 보러 가서 우리가 기다려온 영화를 볼 날을 기다려왔어요. 어젯밤에 저는 마침내 표를 사러 갔는데, 영화관에 도착했을 때 표가 매진되었고, 그래서 저는 매우 화가 났어요. 우리 모두 화가 났어요. 그래서 오늘 우리는 대신 경기를 보러 갈지 결정했지만, 지금은 경기를 보러 가지 않을 것 같아요.\"', 'content_en': 'I need you to edit something for me. This is the text I wrote, \\n\\n\"Me and my friends have been waiting for a long time to go back to the movies and catch a movie we been waiting on. Last nite I finely went to go buy tickets and when I got to the movie theater the tickets were sold out, so I was pretty pist. We were all pist. So today we deside if we wanted to go to a game instead, but I dont think we will go to a game now.\"', 'role': 'user'}\n"
          ]
        }
      ],
      "source": [
        "# 데이터 로드\n",
        "dataset = finetuner.load_data()\n",
        "num_samples = len(dataset)\n",
        "\n",
        "print(f\"\\n데이터 로드 완료!\")\n",
        "print(f\"총 샘플 수: {num_samples:,}\")\n",
        "\n",
        "# 샘플 확인\n",
        "if len(dataset) > 0:\n",
        "    print(f\"\\n첫 번째 샘플 키: {list(dataset[0].keys())}\")\n",
        "    if 'messages' in dataset[0]:\n",
        "        print(f\"메시지 수: {len(dataset[0]['messages'])}\")\n",
        "        print(f\"첫 번째 메시지: {dataset[0]['messages'][0] if dataset[0]['messages'] else 'None'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 데이터 포맷팅 및 토크나이징 (선택적 확인)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'finetuner' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 데이터 포맷팅 (토크나이징 포함)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# 주의: 이 단계는 시간이 오래 걸릴 수 있습니다 (460k 샘플)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 필요시 이 셀을 건너뛰고 바로 학습을 시작할 수도 있습니다\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 전체 데이터셋 포맷팅\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m train_formatted \u001b[38;5;241m=\u001b[39m \u001b[43mfinetuner\u001b[49m\u001b[38;5;241m.\u001b[39mformat_dataset(dataset)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m포맷팅 완료!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m포맷팅된 샘플 수: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_formatted)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'finetuner' is not defined"
          ]
        }
      ],
      "source": [
        "# 데이터 포맷팅 (토크나이징 포함)\n",
        "# 주의: 이 단계는 시간이 오래 걸릴 수 있습니다 (460k 샘플)\n",
        "# 필요시 이 셀을 건너뛰고 바로 학습을 시작할 수도 있습니다\n",
        "\n",
        "# 작은 샘플로 먼저 테스트하려면:\n",
        "# test_dataset = dataset.select(range(min(1000, len(dataset))))\n",
        "# train_formatted = finetuner.format_dataset(test_dataset)\n",
        "\n",
        "# 전체 데이터셋 포맷팅\n",
        "train_formatted = finetuner.format_dataset(dataset)\n",
        "\n",
        "print(f\"\\n포맷팅 완료!\")\n",
        "print(f\"포맷팅된 샘플 수: {len(train_formatted):,}\")\n",
        "\n",
        "# 샘플 확인\n",
        "if len(train_formatted) > 0:\n",
        "    sample = train_formatted[0]\n",
        "    print(f\"\\n포맷팅된 샘플 키: {list(sample.keys())}\")\n",
        "    if 'input_ids' in sample:\n",
        "        \n",
        "        print(f\"input_ids 길이: {len(sample['input_ids'])}\")\n",
        "        # 디코딩하여 확인\n",
        "        decoded = finetuner.tokenizer.decode(sample['input_ids'], skip_special_tokens=False)\n",
        "        print(f\"디코딩된 텍스트 (처음 200자): {decoded[:200]}...\")\n",
        "    elif 'text' in sample:\n",
        "        print(f\"텍스트 (처음 200자): {sample['text'][:200]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 학습 실행\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 학습 실행\n",
        "# 주의: 이 단계는 매우 오래 걸립니다 (수 시간 소요)\n",
        "\n",
        "train_start = time.perf_counter()\n",
        "\n",
        "try:\n",
        "    # 데이터가 이미 포맷팅되어 있으면 format_dataset을 건너뛰기 위해\n",
        "    # train 메서드 내부에서 다시 포맷팅하지 않도록 수정이 필요할 수 있습니다.\n",
        "    # 현재는 train 메서드가 내부에서 format_dataset을 호출하므로,\n",
        "    # 위에서 포맷팅한 데이터를 재사용하려면 코드 수정이 필요합니다.\n",
        "    \n",
        "    # 간단한 방법: 원본 dataset을 전달하고 train 메서드 내부에서 포맷팅하도록 함\n",
        "    model_path = finetuner.train(dataset, resume_from_checkpoint=resume_from_checkpoint)\n",
        "    \n",
        "    train_end = time.perf_counter()\n",
        "    total_time = train_end - train_start\n",
        "    \n",
        "    # 결과 출력\n",
        "    total_tokens = (\n",
        "        num_samples\n",
        "        * config.max_seq_length\n",
        "        * config.num_train_epochs\n",
        "    )\n",
        "    \n",
        "    tokens_per_sec = total_tokens / total_time if total_time > 0 else 0\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\" 완료!\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"모델: {model_path}\")\n",
        "    print(f\"총 샘플 수: {num_samples:,}\")\n",
        "    print(f\"총 토큰 수(대략): {total_tokens:,}\")\n",
        "    print(f\"총 학습 시간: {total_time/3600:.2f} 시간\")\n",
        "    print(f\"평균 처리 속도: {tokens_per_sec:,.0f} tokens/sec\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "    \n",
        "except Exception as e:\n",
        "    logger.error(f\"오류 발생: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 결과 확인 (선택적)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 학습 완료 후 결과 확인\n",
        "# 체크포인트 디렉토리 확인\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "output_dir = Path(config.output_dir)\n",
        "if output_dir.exists():\n",
        "    checkpoints = sorted([d for d in output_dir.iterdir() if d.is_dir() and d.name.startswith(\"checkpoint-\")])\n",
        "    print(f\"\\n생성된 체크포인트:\")\n",
        "    for cp in checkpoints:\n",
        "        print(f\"  - {cp.name}\")\n",
        "    \n",
        "    final_dir = output_dir / \"final\"\n",
        "    if final_dir.exists():\n",
        "        print(f\"\\n최종 모델: {final_dir}\")\n",
        "        files = list(final_dir.glob(\"*\"))\n",
        "        print(f\"  파일 수: {len(files)}\")\n",
        "        if files:\n",
        "            print(f\"  예시 파일: {files[0].name}\")\n",
        "else:\n",
        "    print(f\"\\n출력 디렉토리가 아직 생성되지 않았습니다: {output_dir}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "fa3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
