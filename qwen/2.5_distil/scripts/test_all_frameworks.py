#!/usr/bin/env python3
"""
ëª¨ë“  í”„ë ˆì„ì›Œí¬ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
- vLLM (Merged ëª¨ë¸)
- SGLang (Merged ëª¨ë¸)
- Transformers (Merged ëª¨ë¸)
- PEFT + Transformers (LoRA ì–´ëŒ‘í„°)
- Ollama (GGUF)
- Llama.cpp (GGUF)
"""

import os
import sys
import subprocess
from pathlib import Path

# í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
TEST_PROMPTS = [
    "í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?",
    "ê¹€ì¹˜ì°Œê°œ ë ˆì‹œí”¼ë¥¼ ê°„ë‹¨íˆ ì•Œë ¤ì£¼ì„¸ìš”",
    "íŒŒì´ì¬ìœ¼ë¡œ í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ì„ êµ¬í˜„í•˜ëŠ” ë°©ë²•ì€?"
]

print("=" * 80)
print("Qwen2.5-3B-Korean-QLoRA í”„ë ˆì„ì›Œí¬ í…ŒìŠ¤íŠ¸")
print("=" * 80)

# 1. vLLM í…ŒìŠ¤íŠ¸ (Merged ëª¨ë¸)
print("\n[ 1/6 ] vLLM í…ŒìŠ¤íŠ¸ (Merged ëª¨ë¸)")
print("-" * 80)
try:
    from vllm import LLM, SamplingParams
    
    print("  â„¹ï¸  vLLM ë¡œë”© ì¤‘...")
    llm = LLM(
        model="MyeongHo0621/Qwen2.5-3B-Korean",
        quantization="bitsandbytes",
        gpu_memory_utilization=0.6
    )
    
    params = SamplingParams(temperature=0.7, max_tokens=256)
    
    print("  âœ… vLLM ë¡œë”© ì™„ë£Œ")
    
    for i, prompt in enumerate(TEST_PROMPTS, 1):
        print(f"\n  ì§ˆë¬¸ {i}: {prompt}")
        outputs = llm.generate([prompt], params)
        response = outputs[0].outputs[0].text
        print(f"  ë‹µë³€: {response[:200]}...")
    
    print("\n  âœ… vLLM í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    
except Exception as e:
    print(f"  âŒ vLLM í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

# 2. SGLang í…ŒìŠ¤íŠ¸ (Merged ëª¨ë¸)
print("\n[ 2/6 ] SGLang í…ŒìŠ¤íŠ¸ (Merged ëª¨ë¸)")
print("-" * 80)
try:
    import sglang as sgl
    
    print("  â„¹ï¸  SGLang ë¡œë”© ì¤‘...")
    runtime = sgl.Runtime(
        model_path="MyeongHo0621/Qwen2.5-3B-Korean",
        quantization="bitsandbytes"
    )
    sgl.set_default_backend(runtime)
    
    @sgl.function
    def chat(s, prompt):
        s += sgl.user(prompt)
        s += sgl.assistant(sgl.gen("response", max_tokens=256, temperature=0.7))
    
    print("  âœ… SGLang ë¡œë”© ì™„ë£Œ")
    
    for i, prompt in enumerate(TEST_PROMPTS, 1):
        print(f"\n  ì§ˆë¬¸ {i}: {prompt}")
        state = chat.run(prompt=prompt)
        response = state["response"]
        print(f"  ë‹µë³€: {response[:200]}...")
    
    print("\n  âœ… SGLang í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    
except Exception as e:
    print(f"  âŒ SGLang í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

# 3. Transformers í…ŒìŠ¤íŠ¸ (Merged ëª¨ë¸)
print("\n[ 3/6 ] Transformers í…ŒìŠ¤íŠ¸ (Merged ëª¨ë¸)")
print("-" * 80)
try:
    from transformers import AutoModelForCausalLM, AutoTokenizer
    import torch
    
    print("  â„¹ï¸  ëª¨ë¸ ë¡œë”© ì¤‘...")
    model = AutoModelForCausalLM.from_pretrained(
        "MyeongHo0621/Qwen2.5-3B-Korean",
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )
    
    tokenizer = AutoTokenizer.from_pretrained("MyeongHo0621/Qwen2.5-3B-Korean")
    
    print("  âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
    
    for i, prompt in enumerate(TEST_PROMPTS, 1):
        print(f"\n  ì§ˆë¬¸ {i}: {prompt}")
        
        messages = [
            {"role": "system", "content": "You are a helpful Korean assistant."},
            {"role": "user", "content": prompt}
        ]
        
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        inputs = tokenizer(text, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        if "assistant" in response:
            response = response.split("assistant")[-1].strip()
        
        print(f"  ë‹µë³€: {response[:200]}...")
    
    print("\n  âœ… Transformers í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    
except Exception as e:
    print(f"  âŒ Transformers í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

# 4. PEFT + Transformers í…ŒìŠ¤íŠ¸ (LoRA ì–´ëŒ‘í„°)
print("\n[ 4/6 ] PEFT + Transformers í…ŒìŠ¤íŠ¸ (LoRA ì–´ëŒ‘í„°)")
print("-" * 80)
try:
    from transformers import AutoModelForCausalLM, AutoTokenizer
    from peft import PeftModel
    import torch
    
    print("  â„¹ï¸  ëª¨ë¸ ë¡œë”© ì¤‘...")
    base_model = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen2.5-3B-Instruct",
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )
    
    model = PeftModel.from_pretrained(
        base_model,
        "MyeongHo0621/Qwen2.5-3B-Korean-QLoRA"  # ë£¨íŠ¸ = ìµœì¢… ëª¨ë¸
    )
    
    tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-3B-Instruct")
    
    print("  âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
    
    for i, prompt in enumerate(TEST_PROMPTS, 1):
        print(f"\n  ì§ˆë¬¸ {i}: {prompt}")
        
        messages = [
            {"role": "system", "content": "You are a helpful Korean assistant."},
            {"role": "user", "content": prompt}
        ]
        
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        inputs = tokenizer(text, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µë§Œ ì¶”ì¶œ
        if "assistant" in response:
            response = response.split("assistant")[-1].strip()
        
        print(f"  ë‹µë³€: {response[:200]}...")
    
    print("\n  âœ… PEFT í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    
except Exception as e:
    print(f"  âŒ PEFT í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

# 5. Ollama í…ŒìŠ¤íŠ¸ (GGUF)
print("\n[ 5/6 ] Ollama í…ŒìŠ¤íŠ¸ (GGUF)")
print("-" * 80)
try:
    # Ollama ì„¤ì¹˜ í™•ì¸
    result = subprocess.run(
        ["ollama", "list"],
        capture_output=True,
        text=True,
        check=False
    )
    
    if result.returncode != 0:
        print("  âš ï¸  Ollamaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        print("  â„¹ï¸  ì„¤ì¹˜: curl -fsSL https://ollama.com/install.sh | sh")
    else:
        # ëª¨ë¸ í™•ì¸
        if "qwen25-korean" not in result.stdout:
            print("  âš ï¸  qwen25-korean ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
            print("  â„¹ï¸  ë¨¼ì € GGUF ë³€í™˜ ë° Ollama ëª¨ë¸ ìƒì„±ì´ í•„ìš”í•©ë‹ˆë‹¤")
        else:
            print("  âœ… Ollama ëª¨ë¸ í™•ì¸ë¨")
            
            for i, prompt in enumerate(TEST_PROMPTS, 1):
                print(f"\n  ì§ˆë¬¸ {i}: {prompt}")
                result = subprocess.run(
                    ["ollama", "run", "qwen25-korean", prompt],
                    capture_output=True,
                    text=True,
                    timeout=30
                )
                response = result.stdout.strip()
                print(f"  ë‹µë³€: {response[:200]}...")
            
            print("\n  âœ… Ollama í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    
except Exception as e:
    print(f"  âŒ Ollama í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

# 6. Llama.cpp í…ŒìŠ¤íŠ¸ (GGUF)
print("\n[ 6/6 ] Llama.cpp í…ŒìŠ¤íŠ¸ (GGUF)")
print("-" * 80)
try:
    llama_cpp_main = "/home/work/llama.cpp/main"
    gguf_file = "/home/work/.setting/qwen/2.5_distil/outputs/gguf/qwen25-3b-korean-Q4_K_M.gguf"
    
    if not Path(llama_cpp_main).exists():
        print(f"  âš ï¸  Llama.cppë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {llama_cpp_main}")
        print(f"  â„¹ï¸  ë¨¼ì € Llama.cppë¥¼ ë¹Œë“œí•´ì£¼ì„¸ìš”")
    elif not Path(gguf_file).exists():
        print(f"  âš ï¸  GGUF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {gguf_file}")
        print(f"  â„¹ï¸  ë¨¼ì € GGUF ë³€í™˜ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”")
    else:
        print("  âœ… Llama.cpp ë° GGUF í™•ì¸ë¨")
        
        for i, prompt in enumerate(TEST_PROMPTS, 1):
            print(f"\n  ì§ˆë¬¸ {i}: {prompt}")
            
            full_prompt = f"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
            
            result = subprocess.run(
                [
                    llama_cpp_main,
                    "-m", gguf_file,
                    "-p", full_prompt,
                    "-n", "256",
                    "--temp", "0.7",
                    "-ngl", "99"
                ],
                capture_output=True,
                text=True,
                timeout=60
            )
            
            # ì‘ë‹µ ì¶”ì¶œ (assistant ì´í›„)
            output = result.stdout
            if "assistant" in output:
                response = output.split("assistant")[-1].strip()
            else:
                response = output
            
            print(f"  ë‹µë³€: {response[:200]}...")
        
        print("\n  âœ… Llama.cpp í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    
except Exception as e:
    print(f"  âŒ Llama.cpp í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

# ì™„ë£Œ
print("\n" + "=" * 80)
print("ğŸ‰ ì „ì²´ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
print("=" * 80)

