# Qwen2.5-14B-Instruct í•œêµ­ì–´ íŒŒì¸íŠœë‹ í”„ë¡œì íŠ¸

H100 80GB í™˜ê²½ì—ì„œ Qwen2.5-14B-Instruct ëª¨ë¸ì„ í•œêµ­ì–´ ë©€í‹°í„´ ëŒ€í™” ë°ì´í„°ë¡œ íŒŒì¸íŠœë‹í•˜ëŠ” í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
2.5_14B_Inst/
â”œâ”€â”€ scripts/              # ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ train.py         # í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ inference_test.py
â”‚   â”œâ”€â”€ merge_and_upload.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ src/
â”‚   â””â”€â”€ qwen_finetuning/ # í•µì‹¬ ëª¨ë“ˆ
â”‚       â”œâ”€â”€ config.py
â”‚       â”œâ”€â”€ trainer.py
â”‚       â”œâ”€â”€ dataset_loader.py
â”‚       â””â”€â”€ ...
â”œâ”€â”€ configs/             # ì„¤ì • íŒŒì¼
â”‚   â””â”€â”€ config.py
â”œâ”€â”€ evaluation/          # í‰ê°€ ê´€ë ¨
â”‚   â”œâ”€â”€ evaluate_korean_benchmarks.py
â”‚   â””â”€â”€ evaluation_results/
â”œâ”€â”€ docs/                # ë¬¸ì„œ
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ README_EVALUATION.md
â”‚   â””â”€â”€ ...
â”œâ”€â”€ outputs/            # ì¶œë ¥ íŒŒì¼
â”‚   â”œâ”€â”€ checkpoints/    # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸
â”‚   â””â”€â”€ logs/          # ë¡œê·¸ íŒŒì¼
â”œâ”€â”€ utils/              # ìœ í‹¸ë¦¬í‹°
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. í™˜ê²½ ì„¤ì •

```bash
# ê°€ìƒí™˜ê²½ í™œì„±í™” (fa3)
source /home/work/miniconda3/etc/profile.d/conda.sh
conda activate fa3

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# HuggingFace ë¡œê·¸ì¸
huggingface-cli login
# ë˜ëŠ” .env íŒŒì¼ì— HF_TOKEN ì„¤ì •
```

### 2. í•™ìŠµ ì‹¤í–‰

```bash
cd /home/work/vss/ft_llm/qwen/2.5_14B_Inst

# ê¸°ë³¸ í•™ìŠµ
python scripts/train.py

# ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ (ë¡œê·¸ ì €ì¥)
nohup python scripts/train.py > outputs/logs/train.log 2>&1 &
```

### 3. í•™ìŠµ ëª¨ë‹ˆí„°ë§

```bash
# ë¡œê·¸ í™•ì¸
tail -f outputs/logs/train.log

# GPU ëª¨ë‹ˆí„°ë§
watch -n 1 nvidia-smi
```

## ğŸ“Š í‰ê°€

í•œêµ­ì–´ ë²¤ì¹˜ë§ˆí¬ í‰ê°€:

```bash
# ì „ì²´ í‰ê°€
python evaluation/evaluate_korean_benchmarks.py

# íŠ¹ì • ì¹´í…Œê³ ë¦¬ë§Œ í‰ê°€
python evaluation/evaluate_korean_benchmarks.py --categories qa math

# ì¹´í…Œê³ ë¦¬ ì˜µì…˜: qa, math, code, mmlu, all
```

## ğŸ”§ ì„¤ì • ì»¤ìŠ¤í„°ë§ˆì´ì§•

`configs/config.py` ë˜ëŠ” `src/qwen_finetuning/config.py`ì—ì„œ ì„¤ì • ë³€ê²½:

```python
@dataclass
class Qwen14BFineTuningConfig:
    # ëª¨ë¸
    max_seq_length: int = 4096
    
    # LoRA
    lora_r: int = 64
    lora_alpha: int = 128
    lora_dropout: float = 0.05
    
    # í•™ìŠµ
    num_train_epochs: int = 3
    per_device_train_batch_size: int = 22  # 72GB VRAM ê¸°ì¤€
    gradient_accumulation_steps: int = 4
    learning_rate: float = 2e-4
```

## ğŸ“¦ ëª¨ë¸ ì •ë³´

- **ë² ì´ìŠ¤ ëª¨ë¸**: [Qwen/Qwen2.5-14B-Instruct](https://huggingface.co/Qwen/Qwen2.5-14B-Instruct)
- **íŒŒì¸íŠœë‹ ëª¨ë¸**: [MyeongHo0621/Qwen2.5-14B-Korean](https://huggingface.co/MyeongHo0621/Qwen2.5-14B-Korean)
- **íŒŒë¼ë¯¸í„°**: 14.7B (13.1B non-embedding)
- **Context Length**: 131,072 tokens (í•™ìŠµ ì‹œ 4,096 ì‚¬ìš©)

## ğŸ¯ ìµœì í™” ê¸°ìˆ 

1. **Flash Attention 3**: H100ì—ì„œ ìµœì í™”ëœ ì–´í…ì…˜ ì—°ì‚°
2. **LoRA**: ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ íŒŒì¸íŠœë‹ (r=64, alpha=128)
3. **8-bit ì–‘ìí™”**: bitsandbytesë¥¼ í†µí•œ ë©”ëª¨ë¦¬ ì ˆì•½
4. **Gradient Checkpointing**: Unsloth ìµœì í™” ë²„ì „
5. **BF16**: H100 ë„¤ì´í‹°ë¸Œ ì§€ì›

## ğŸ“š ë°ì´í„°ì…‹

í•œêµ­ì–´ ë©€í‹°í„´ ëŒ€í™” ë°ì´í„°ì…‹ (ì•½ 68ë§Œê°œ):
- `kowiki_qa_data.jsonl` (48,699)
- `kullm_v2_full_data.jsonl` (146,963)
- `orca_math_ko_data.jsonl` (192,807)
- `smol_koreantalk_data.jsonl` (88,752)
- `won_instruct_data.jsonl` (86,007)

**ë°ì´í„° í¬ë§·**: ChatML í˜•ì‹ì˜ `messages` í•„ë“œ

```json
{
  "messages": [
    {"role": "user", "content": "ì§ˆë¬¸"},
    {"role": "assistant", "content": "ë‹µë³€"}
  ],
  "source": "dataset_name"
}
```

## ğŸ“ˆ í‰ê°€ ë²¤ì¹˜ë§ˆí¬

- **KMMLU**: í•œêµ­ì–´ MMLU ìŠ¤íƒ€ì¼ ë²¤ì¹˜ë§ˆí¬
- **KoBEST**: í•œêµ­ì–´ ê³ ê¸‰ ì–¸ì–´ í˜„ìƒ/ì¶”ë¡ 
- **KorQuAD**: í•œêµ­ì–´ ìœ„í‚¤ ê¸°ë°˜ MRC
- **GSM8K-Ko**: í•œêµ­ì–´ ìˆ˜í•™ ì¶”ë¡ 
- **HRM8K**: í•œêµ­ ìˆ˜í•™ ì¶”ë¡  ë²¤ì¹˜ë§ˆí¬
- **HumanEval-Ko**: ì½”ë“œ ìƒì„± í‰ê°€

## ğŸ” ì£¼ìš” ìŠ¤í¬ë¦½íŠ¸

- `scripts/train.py`: í•™ìŠµ ì‹¤í–‰
- `scripts/merge_and_upload.py`: LoRA ì–´ëŒ‘í„°ì™€ ë² ì´ìŠ¤ ëª¨ë¸ ë³‘í•© í›„ HuggingFace Hub ì—…ë¡œë“œ
- `scripts/test_hub_model.py`: Hubì—ì„œ ë‹¤ìš´ë¡œë“œí•œ ëª¨ë¸ í…ŒìŠ¤íŠ¸
- `evaluation/evaluate_korean_benchmarks.py`: í•œêµ­ì–´ ë²¤ì¹˜ë§ˆí¬ í‰ê°€

## ğŸ“ ë¬¸ì„œ

- [í‰ê°€ ê°€ì´ë“œ](docs/README_EVALUATION.md)
- [í•™ìŠµ ìƒíƒœ ë¶„ì„](docs/TRAINING_STATUS_ANALYSIS.md)
- [Flash Attention 3 ì„¤ì • ê°€ì´ë“œ](docs/FLASH_ATTENTION_3_SETUP_GUIDE.md)

## ğŸ› ë¬¸ì œ í•´ê²°

### OOM (Out of Memory) ì—ëŸ¬
```python
# ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
per_device_train_batch_size: int = 2
gradient_accumulation_steps: int = 8
```

### í•™ìŠµ ì†ë„ê°€ ëŠë¦¼
```python
# Gradient accumulation ì¤„ì´ê¸°
gradient_accumulation_steps: int = 2
```

### Flash Attention ì—ëŸ¬
```python
# Flash Attention ë¹„í™œì„±í™”
attn_implementation="eager"
```

## ğŸ“„ ë¼ì´ì„ ìŠ¤

- Qwen2.5-14B-Instruct: Apache 2.0

## ğŸ”— ì°¸ê³  ìë£Œ

- [Qwen2.5 ê³µì‹ ë¬¸ì„œ](https://qwenlm.github.io/blog/qwen2.5/)
- [Unsloth ë¬¸ì„œ](https://github.com/unslothai/unsloth)
- [Flash Attention](https://github.com/Dao-AILab/flash-attention)
- [LoRA ë…¼ë¬¸](https://arxiv.org/abs/2106.09685)

