nohup: ignoring input
/home/work/tesseract/ft_llm/qwen/2.5_14B_Inst/0_qwen14b/trainer.py:13: UserWarning: WARNING: Unsloth should be imported before transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.

Please restructure your imports with 'import unsloth' at the top of your file.
  from unsloth import FastLanguageModel, is_bfloat16_supported
ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
Unsloth: Your Flash Attention 2 installation seems to be broken?
A possible explanation is you have a new CUDA version which isn't
yet compatible with FA2? Please file a ticket to Unsloth or FA2.
We shall now use Xformers instead, which does not have any performance hits!
We found this negligible impact by benchmarking on 1x A100.
========
Switching to PyTorch attention since your Xformers is broken.
========

/usr/local/lib/python3.12/dist-packages/flash_attn_2_cuda.cpython-312-x86_64-linux-gnu.so: undefined symbol: _ZN3c104cuda9SetDeviceEa
ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
/usr/local/lib/python3.12/dist-packages/astroid/interpreter/_import/util.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources

================================================================================
 Qwen2.5-14B-Instruct í•œêµ­ì–´ ë©€í‹°í„´ ëŒ€í™” íŒŒì¸íŠœë‹
 H100 80GB ìµœì í™” | Flash Attention 2
================================================================================

GPU: NVIDIA H100 80GB HBM3
ë©”ëª¨ë¦¬: 79.2GB

================================================================================
 ì„¤ì • ìš”ì•½
================================================================================
ëª¨ë¸: Qwen/Qwen2.5-14B-Instruct
ë°ì´í„°: /home/work/tesseract/korean_large_data/cleaned_jsonl
  íŒŒì¼: smol_koreantalk_data.jsonl
  ìµœëŒ€ ìƒ˜í”Œ: 200,000ê°œ
ì¶œë ¥: /home/work/tesseract/qwen/2.5_14B_Inst/output
LoRA: r=64, alpha=128, dropout=0.0
Epoch: 3
ë°°ì¹˜: 28 Ã— 2 = 56
í•™ìŠµë¥ : 0.0002
Max Seq: 4096
ì²´í¬í¬ì¸íŠ¸: 100 stepë§ˆë‹¤
================================================================================

2025-11-13 01:15:04,951 - INFO - ================================================================================
2025-11-13 01:15:04,951 - INFO - Qwen2.5-14B-Instruct ëª¨ë¸ ë¡œë”©
2025-11-13 01:15:04,951 - INFO - ================================================================================
2025-11-13 01:15:04,951 - INFO - ëª¨ë¸: Qwen/Qwen2.5-14B-Instruct
2025-11-13 01:15:04,951 - INFO - Max Seq Length: 4096
2025-11-13 01:15:04,951 - INFO - 8bit: True, 4bit: False
2025-11-13 01:15:04,952 - INFO - ================================================================================
2025-11-13 01:15:04,952 - INFO - 
================================================================================
2025-11-13 01:15:04,952 - INFO - [ëª¨ë¸ ë¡œë“œ ì „] ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§
2025-11-13 01:15:04,952 - INFO - ================================================================================
2025-11-13 01:15:05,207 - INFO - ================================================================================
2025-11-13 01:15:05,207 - INFO - [ëª¨ë¸ ë¡œë“œ ì „] CPU/RAM ìƒì„¸ ëª¨ë‹ˆí„°ë§
2025-11-13 01:15:05,207 - INFO -   ì‹œê°„: 2025-11-13 01:15:05
2025-11-13 01:15:05,207 - INFO -   CPU ì‚¬ìš©ë¥ : 0.0% (12 cores @ 2000 MHz)
2025-11-13 01:15:05,207 - INFO -   RAM: 1.8GB / 187.0GB (1.0%)
2025-11-13 01:15:05,207 - INFO -   í”„ë¡œì„¸ìŠ¤ CPU: 0.0%
2025-11-13 01:15:05,207 - INFO -   í”„ë¡œì„¸ìŠ¤ RAM: 1.14GB
2025-11-13 01:15:05,208 - INFO -   í”„ë¡œì„¸ìŠ¤ ìŠ¤ë ˆë“œ: 51
2025-11-13 01:15:05,211 - INFO -   Top 3 í”„ë¡œì„¸ìŠ¤:
2025-11-13 01:15:05,211 - INFO -     1. docker-init (PID: 1): CPU 0.0% | RAM 0.0%
2025-11-13 01:15:05,211 - INFO -     2. backend.ai: ker (PID: 7): CPU 0.0% | RAM 0.0%
2025-11-13 01:15:05,211 - INFO -     3. ssh-agent (PID: 66): CPU 0.0% | RAM 0.0%
2025-11-13 01:15:05,211 - INFO - ================================================================================
2025-11-13 01:15:05,211 - INFO - ================================================================================
2025-11-13 01:15:05,211 - INFO - [ëª¨ë¸ ë¡œë“œ ì „] ì „ì²´ GPU ìƒíƒœ
2025-11-13 01:15:05,211 - INFO -   ì´ GPU ìˆ˜: 1
2025-11-13 01:15:05,511 - INFO -   GPU0 (NVIDIA H100 80GB HBM3): 0.0GB/79.2GB (0.0%) | Load: 0%
2025-11-13 01:15:05,511 - INFO - ================================================================================
2025-11-13 01:15:05,513 - INFO - ================================================================================

2025-11-13 01:15:05,513 - INFO - [ INFO ] ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘...
2025-11-13 01:15:05,513 - INFO - [ INFO ] Attention êµ¬í˜„: flash_attention_2
Unsloth: WARNING `trust_remote_code` is True.
Are you certain you want to do remote code execution?
==((====))==  Unsloth 2025.11.2: Fast Qwen2 patching. Transformers: 4.57.1.
   \\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.19 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.8.0+cu128. CUDA: 9.0. CUDA Toolkit: 12.8. Triton: 3.4.0
\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Qwen2 does not support SDPA - switching to fast eager.
Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]Fetching 6 files:  17%|â–ˆâ–‹        | 1/6 [00:24<02:01, 24.26s/it]Fetching 6 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:24<00:00,  4.05s/it]
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|â–ˆâ–‹        | 1/6 [00:03<00:18,  3.75s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 2/6 [00:08<00:18,  4.57s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:14<00:14,  4.92s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:19<00:10,  5.09s/it]Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 5/6 [00:24<00:05,  5.13s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:28<00:00,  4.78s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:28<00:00,  4.81s/it]
2025-11-13 01:16:12,702 - INFO - [ COMPLETE ] ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ
2025-11-13 01:16:12,702 - INFO - 
================================================================================
2025-11-13 01:16:12,702 - INFO - [ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ í›„] ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§
2025-11-13 01:16:12,702 - INFO - ================================================================================
2025-11-13 01:16:12,932 - INFO - ================================================================================
2025-11-13 01:16:12,932 - INFO - [ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ í›„] CPU/RAM ìƒì„¸ ëª¨ë‹ˆí„°ë§
2025-11-13 01:16:12,932 - INFO -   ì‹œê°„: 2025-11-13 01:16:12
2025-11-13 01:16:12,932 - INFO -   CPU ì‚¬ìš©ë¥ : 0.0% (12 cores @ 2040 MHz)
2025-11-13 01:16:12,932 - INFO -   RAM: 4.2GB / 187.0GB (2.3%)
2025-11-13 01:16:12,933 - INFO -   í”„ë¡œì„¸ìŠ¤ CPU: 0.0%
2025-11-13 01:16:12,933 - INFO -   í”„ë¡œì„¸ìŠ¤ RAM: 2.78GB
2025-11-13 01:16:12,933 - INFO -   í”„ë¡œì„¸ìŠ¤ ìŠ¤ë ˆë“œ: 103
2025-11-13 01:16:12,935 - INFO -   Top 3 í”„ë¡œì„¸ìŠ¤:
2025-11-13 01:16:12,935 - INFO -     1. python (PID: 1770): CPU 366.5% | RAM 1.5%
2025-11-13 01:16:12,935 - INFO -     2. node (PID: 416): CPU 0.9% | RAM 0.2%
2025-11-13 01:16:12,936 - INFO -     3. node (PID: 339): CPU 0.3% | RAM 0.1%
2025-11-13 01:16:12,936 - INFO - ================================================================================
2025-11-13 01:16:12,936 - INFO - ================================================================================
2025-11-13 01:16:12,936 - INFO - [ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ í›„] ì „ì²´ GPU ìƒíƒœ
2025-11-13 01:16:12,936 - INFO -   ì´ GPU ìˆ˜: 1
2025-11-13 01:16:13,104 - INFO -   GPU0 (NVIDIA H100 80GB HBM3): 15.3GB/79.2GB (19.3%) | Load: 0%
2025-11-13 01:16:13,104 - INFO - ================================================================================
2025-11-13 01:16:13,105 - INFO - ================================================================================

2025-11-13 01:16:13,160 - INFO - [ INFO ] Vocab size: 151,665
2025-11-13 01:16:13,161 - INFO - [ INFO ] LoRA ì„¤ì • ì ìš© ì¤‘...
2025-11-13 01:16:13,163 - INFO -   LoRA r=64, alpha=128
Unsloth: Making `model.base_model.model.model` require gradients
2025-11-13 01:16:18,674 - INFO - [ COMPLETE ] LoRA ì ìš© ì™„ë£Œ
2025-11-13 01:16:18,674 - INFO - 
================================================================================
2025-11-13 01:16:18,674 - INFO - [LoRA ì ìš© í›„] ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§
2025-11-13 01:16:18,674 - INFO - ================================================================================
2025-11-13 01:16:18,927 - INFO - ================================================================================
2025-11-13 01:16:18,927 - INFO - [LoRA ì ìš© í›„] CPU/RAM ìƒì„¸ ëª¨ë‹ˆí„°ë§
2025-11-13 01:16:18,927 - INFO -   ì‹œê°„: 2025-11-13 01:16:18
2025-11-13 01:16:18,927 - INFO -   CPU ì‚¬ìš©ë¥ : 0.0% (12 cores @ 2000 MHz)
2025-11-13 01:16:18,927 - INFO -   RAM: 4.2GB / 187.0GB (2.3%)
2025-11-13 01:16:18,927 - INFO -   í”„ë¡œì„¸ìŠ¤ CPU: 0.0%
2025-11-13 01:16:18,927 - INFO -   í”„ë¡œì„¸ìŠ¤ RAM: 2.79GB
2025-11-13 01:16:18,927 - INFO -   í”„ë¡œì„¸ìŠ¤ ìŠ¤ë ˆë“œ: 103
2025-11-13 01:16:18,930 - INFO -   Top 3 í”„ë¡œì„¸ìŠ¤:
2025-11-13 01:16:18,930 - INFO -     1. python (PID: 1770): CPU 643.9% | RAM 1.5%
2025-11-13 01:16:18,930 - INFO -     2. node (PID: 339): CPU 0.5% | RAM 0.1%
2025-11-13 01:16:18,930 - INFO -     3. node (PID: 416): CPU 0.3% | RAM 0.2%
2025-11-13 01:16:18,930 - INFO - ================================================================================
2025-11-13 01:16:18,930 - INFO - ================================================================================
2025-11-13 01:16:18,930 - INFO - [LoRA ì ìš© í›„] ì „ì²´ GPU ìƒíƒœ
2025-11-13 01:16:18,930 - INFO -   ì´ GPU ìˆ˜: 1
2025-11-13 01:16:19,657 - INFO -   GPU0 (NVIDIA H100 80GB HBM3): 16.3GB/79.2GB (20.6%) | Load: 0%
2025-11-13 01:16:19,657 - INFO - ================================================================================
2025-11-13 01:16:19,658 - INFO - ================================================================================

2025-11-13 01:16:19,672 - INFO - [ INFO ] í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: 275,251,200 / 15,045,284,864 (1.83%)
2025-11-13 01:16:19,672 - INFO - ================================================================================
2025-11-13 01:16:19,672 - INFO - ================================================================================
2025-11-13 01:16:19,672 - INFO - í•œêµ­ì–´ ë©€í‹°í„´ ëŒ€í™” ë°ì´í„°ì…‹ ë¡œë”©
2025-11-13 01:16:19,672 - INFO - ================================================================================
2025-11-13 01:16:19,672 - INFO - ë°ì´í„° ë””ë ‰í† ë¦¬: /home/work/tesseract/korean_large_data/cleaned_jsonl
2025-11-13 01:16:19,672 - INFO - 
================================================================================
2025-11-13 01:16:19,673 - INFO - [ë°ì´í„° ë¡œë“œ ì „] ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§
2025-11-13 01:16:19,673 - INFO - ================================================================================
2025-11-13 01:16:19,906 - INFO - ================================================================================
2025-11-13 01:16:19,906 - INFO - [ë°ì´í„° ë¡œë“œ ì „] CPU/RAM ìƒì„¸ ëª¨ë‹ˆí„°ë§
2025-11-13 01:16:19,906 - INFO -   ì‹œê°„: 2025-11-13 01:16:19
2025-11-13 01:16:19,906 - INFO -   CPU ì‚¬ìš©ë¥ : 0.0% (12 cores @ 2000 MHz)
2025-11-13 01:16:19,906 - INFO -   RAM: 4.2GB / 187.0GB (2.3%)
2025-11-13 01:16:19,906 - INFO -   í”„ë¡œì„¸ìŠ¤ CPU: 0.0%
2025-11-13 01:16:19,906 - INFO -   í”„ë¡œì„¸ìŠ¤ RAM: 2.79GB
2025-11-13 01:16:19,906 - INFO -   í”„ë¡œì„¸ìŠ¤ ìŠ¤ë ˆë“œ: 102
2025-11-13 01:16:19,909 - INFO -   Top 3 í”„ë¡œì„¸ìŠ¤:
2025-11-13 01:16:19,909 - INFO -     1. python (PID: 1770): CPU 2.0% | RAM 1.5%
2025-11-13 01:16:19,909 - INFO -     2. node (PID: 339): CPU 1.0% | RAM 0.1%
2025-11-13 01:16:19,909 - INFO -     3. docker-init (PID: 1): CPU 0.0% | RAM 0.0%
2025-11-13 01:16:19,909 - INFO - ================================================================================
2025-11-13 01:16:19,910 - INFO - ================================================================================
2025-11-13 01:16:19,910 - INFO - [ë°ì´í„° ë¡œë“œ ì „] ì „ì²´ GPU ìƒíƒœ
2025-11-13 01:16:19,910 - INFO -   ì´ GPU ìˆ˜: 1
2025-11-13 01:16:21,155 - INFO -   GPU0 (NVIDIA H100 80GB HBM3): 16.3GB/79.2GB (20.6%) | Load: 0%
2025-11-13 01:16:21,155 - INFO - ================================================================================
2025-11-13 01:16:21,157 - INFO - ================================================================================

2025-11-13 01:16:21,157 - INFO -   ë¡œë”©: smol_koreantalk_data.jsonl
2025-11-13 01:16:45,502 - INFO -     ì¶”ê°€ë¨: 460,281ê°œ
2025-11-13 01:16:45,502 - INFO - [ COMPLETE ] ì´ 460,281ê°œ ë°ì´í„° ë¡œë“œ
2025-11-13 01:17:23,897 - INFO - [ COMPLETE ] ì´ 460,281ê°œ ë°ì´í„° ë¡œë“œ
2025-11-13 01:17:23,899 - INFO - [ INFO ] ë°ì´í„° ìƒ˜í”Œë§: 460,281 â†’ 200,000
2025-11-13 01:17:23,902 - INFO - [ COMPLETE ] ìƒ˜í”Œë§ ì™„ë£Œ: 200,000ê°œ
2025-11-13 01:17:23,902 - INFO - 
================================================================================
2025-11-13 01:17:23,902 - INFO - [ë°ì´í„° ë¡œë“œ í›„] ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§
2025-11-13 01:17:23,902 - INFO - ================================================================================
2025-11-13 01:17:24,154 - INFO - ================================================================================
2025-11-13 01:17:24,154 - INFO - [ë°ì´í„° ë¡œë“œ í›„] CPU/RAM ìƒì„¸ ëª¨ë‹ˆí„°ë§
2025-11-13 01:17:24,154 - INFO -   ì‹œê°„: 2025-11-13 01:17:24
2025-11-13 01:17:24,154 - INFO -   CPU ì‚¬ìš©ë¥ : 0.0% (12 cores @ 2000 MHz)
2025-11-13 01:17:24,155 - INFO -   RAM: 13.7GB / 187.0GB (7.3%)
2025-11-13 01:17:24,155 - INFO -   í”„ë¡œì„¸ìŠ¤ CPU: 0.0%
2025-11-13 01:17:24,155 - INFO -   í”„ë¡œì„¸ìŠ¤ RAM: 12.26GB
2025-11-13 01:17:24,155 - INFO -   í”„ë¡œì„¸ìŠ¤ ìŠ¤ë ˆë“œ: 102
2025-11-13 01:17:24,158 - INFO -   Top 3 í”„ë¡œì„¸ìŠ¤:
2025-11-13 01:17:24,158 - INFO -     1. python (PID: 1770): CPU 96.0% | RAM 6.6%
2025-11-13 01:17:24,158 - INFO -     2. backend.ai: ker (PID: 7): CPU 0.2% | RAM 0.0%
2025-11-13 01:17:24,158 - INFO -     3. node (PID: 416): CPU 0.2% | RAM 0.2%
2025-11-13 01:17:24,158 - INFO - ================================================================================
2025-11-13 01:17:24,158 - INFO - ================================================================================
2025-11-13 01:17:24,158 - INFO - [ë°ì´í„° ë¡œë“œ í›„] ì „ì²´ GPU ìƒíƒœ
2025-11-13 01:17:24,158 - INFO -   ì´ GPU ìˆ˜: 1
2025-11-13 01:17:24,570 - INFO -   GPU0 (NVIDIA H100 80GB HBM3): 16.3GB/79.2GB (20.6%) | Load: 0%
2025-11-13 01:17:24,570 - INFO - ================================================================================
2025-11-13 01:17:24,571 - INFO - ================================================================================

2025-11-13 01:17:24,571 - INFO - ================================================================================
2025-11-13 01:17:24,571 - INFO - ================================================================================
2025-11-13 01:17:24,571 - INFO -  Qwen2.5-14B ë©€í‹°í„´ ëŒ€í™” íŒŒì¸íŠœë‹
2025-11-13 01:17:24,571 - INFO -  Run: qwen25-14b-KR-multiturn-20251113_011438
2025-11-13 01:17:24,572 - INFO - ================================================================================
2025-11-13 01:17:24,573 - INFO - 
================================================================================
2025-11-13 01:17:24,573 - INFO - [í•™ìŠµ ì¤€ë¹„] ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§
2025-11-13 01:17:24,573 - INFO - ================================================================================
2025-11-13 01:17:24,805 - INFO - ================================================================================
2025-11-13 01:17:24,805 - INFO - [í•™ìŠµ ì¤€ë¹„] CPU/RAM ìƒì„¸ ëª¨ë‹ˆí„°ë§
2025-11-13 01:17:24,805 - INFO -   ì‹œê°„: 2025-11-13 01:17:24
2025-11-13 01:17:24,805 - INFO -   CPU ì‚¬ìš©ë¥ : 0.0% (12 cores @ 2000 MHz)
2025-11-13 01:17:24,805 - INFO -   RAM: 13.7GB / 187.0GB (7.3%)
2025-11-13 01:17:24,805 - INFO -   í”„ë¡œì„¸ìŠ¤ CPU: 0.0%
2025-11-13 01:17:24,806 - INFO -   í”„ë¡œì„¸ìŠ¤ RAM: 12.26GB
2025-11-13 01:17:24,806 - INFO -   í”„ë¡œì„¸ìŠ¤ ìŠ¤ë ˆë“œ: 102
2025-11-13 01:17:24,808 - INFO -   Top 3 í”„ë¡œì„¸ìŠ¤:
2025-11-13 01:17:24,808 - INFO -     1. python3 (PID: 87): CPU 1.5% | RAM 0.1%
2025-11-13 01:17:24,808 - INFO -     2. node (PID: 416): CPU 1.5% | RAM 0.2%
2025-11-13 01:17:24,808 - INFO -     3. docker-init (PID: 1): CPU 0.0% | RAM 0.0%
2025-11-13 01:17:24,809 - INFO - ================================================================================
2025-11-13 01:17:24,809 - INFO - ================================================================================
2025-11-13 01:17:24,809 - INFO - [í•™ìŠµ ì¤€ë¹„] ì „ì²´ GPU ìƒíƒœ
2025-11-13 01:17:24,809 - INFO -   ì´ GPU ìˆ˜: 1
2025-11-13 01:17:25,199 - INFO -   GPU0 (NVIDIA H100 80GB HBM3): 16.3GB/79.2GB (20.6%) | Load: 0%
2025-11-13 01:17:25,199 - INFO - ================================================================================
2025-11-13 01:17:25,200 - INFO - ================================================================================

2025-11-13 01:17:25,200 - INFO - [ COMPLETE ] GPU ìºì‹œ ì •ë¦¬ ì™„ë£Œ
2025-11-13 01:17:25,534 - INFO - [ COMPLETE ] Python GC ì‹¤í–‰ ì™„ë£Œ
2025-11-13 01:17:25,536 - INFO - í›ˆë ¨: 190,000ê°œ
2025-11-13 01:17:25,536 - INFO - ê²€ì¦: 10,000ê°œ (ìµœì†Œ 100ê°œ ë³´ì¥)
2025-11-13 01:17:25,536 - INFO - Epoch: 3
2025-11-13 01:17:25,536 - INFO - ë°°ì¹˜ í¬ê¸°: 28
2025-11-13 01:17:25,536 - INFO - íš¨ê³¼ì  ë°°ì¹˜: 56
2025-11-13 01:17:25,537 - INFO - ================================================================================
2025-11-13 01:17:25,537 - INFO - [ INFO ] ë°ì´í„° í¬ë§·íŒ… ì¤‘...
2025-11-13 01:17:25,537 - INFO - [ INFO ] ë°ì´í„°ì…‹ í¬ë§·íŒ… ì‹œì‘... (ì…ë ¥: 190,000ê°œ)
2025-11-13 01:17:25,537 - INFO - [ DEBUG ] ì²« 3ê°œ ìƒ˜í”Œ í‚¤ í™•ì¸:
2025-11-13 01:17:25,540 - INFO -   ìƒ˜í”Œ 0: ['messages', 'custom_id']
2025-11-13 01:17:25,540 - INFO -   ìƒ˜í”Œ 1: ['messages', 'custom_id']
2025-11-13 01:17:25,540 - INFO -   ìƒ˜í”Œ 2: ['messages', 'custom_id']
2025-11-13 01:17:31,536 - INFO -   ì§„í–‰: 50,000/190,000
2025-11-13 01:17:37,613 - INFO -   ì§„í–‰: 100,000/190,000
2025-11-13 01:17:43,735 - INFO -   ì§„í–‰: 150,000/190,000
2025-11-13 01:17:48,743 - INFO - [ COMPLETE ] í¬ë§·íŒ… ì™„ë£Œ:
2025-11-13 01:17:48,743 - INFO -   text í•„ë“œ ì‚¬ìš©: 0ê°œ
2025-11-13 01:17:48,743 - INFO -   messages ë³€í™˜: 190,000ê°œ
2025-11-13 01:17:48,743 - INFO -   ì‹¤íŒ¨: 0ê°œ
2025-11-13 01:17:48,743 - INFO -   ì´: 190,000ê°œ
2025-11-13 01:17:58,042 - INFO - [ INFO ] ë°ì´í„°ì…‹ í¬ë§·íŒ… ì‹œì‘... (ì…ë ¥: 10,000ê°œ)
2025-11-13 01:17:58,042 - INFO - [ DEBUG ] ì²« 3ê°œ ìƒ˜í”Œ í‚¤ í™•ì¸:
2025-11-13 01:17:58,043 - INFO -   ìƒ˜í”Œ 0: ['messages', 'custom_id']
2025-11-13 01:17:58,043 - INFO -   ìƒ˜í”Œ 1: ['messages', 'custom_id']
2025-11-13 01:17:58,043 - INFO -   ìƒ˜í”Œ 2: ['messages', 'custom_id']
2025-11-13 01:17:59,340 - INFO - [ COMPLETE ] í¬ë§·íŒ… ì™„ë£Œ:
2025-11-13 01:17:59,340 - INFO -   text í•„ë“œ ì‚¬ìš©: 0ê°œ
2025-11-13 01:17:59,340 - INFO -   messages ë³€í™˜: 10,000ê°œ
2025-11-13 01:17:59,340 - INFO -   ì‹¤íŒ¨: 0ê°œ
2025-11-13 01:17:59,341 - INFO -   ì´: 10,000ê°œ
2025-11-13 01:17:59,897 - INFO - [ COMPLETE ] í›ˆë ¨: 190,000ê°œ, ê²€ì¦: 10,000ê°œ
2025-11-13 01:17:59,898 - INFO - 
================================================================================
2025-11-13 01:17:59,898 - INFO - [ë°ì´í„° í¬ë§·íŒ… ì™„ë£Œ] ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§
2025-11-13 01:17:59,898 - INFO - ================================================================================
2025-11-13 01:18:00,145 - INFO - ================================================================================
2025-11-13 01:18:00,145 - INFO - [ë°ì´í„° í¬ë§·íŒ… ì™„ë£Œ] CPU/RAM ìƒì„¸ ëª¨ë‹ˆí„°ë§
2025-11-13 01:18:00,145 - INFO -   ì‹œê°„: 2025-11-13 01:18:00
2025-11-13 01:18:00,145 - INFO -   CPU ì‚¬ìš©ë¥ : 4.1% (12 cores @ 2080 MHz)
2025-11-13 01:18:00,335 - INFO -   RAM: 14.6GB / 187.0GB (7.8%)
2025-11-13 01:18:00,335 - INFO -   í”„ë¡œì„¸ìŠ¤ CPU: 0.0%
2025-11-13 01:18:00,335 - INFO -   í”„ë¡œì„¸ìŠ¤ RAM: 13.13GB
2025-11-13 01:18:00,335 - INFO -   í”„ë¡œì„¸ìŠ¤ ìŠ¤ë ˆë“œ: 102
2025-11-13 01:18:00,337 - INFO -   Top 3 í”„ë¡œì„¸ìŠ¤:
2025-11-13 01:18:00,337 - INFO -     1. python (PID: 1770): CPU 97.7% | RAM 7.0%
2025-11-13 01:18:00,338 - INFO -     2. backend.ai: ker (PID: 7): CPU 0.2% | RAM 0.0%
2025-11-13 01:18:00,338 - INFO -     3. node (PID: 339): CPU 0.2% | RAM 0.1%
2025-11-13 01:18:00,338 - INFO - ================================================================================
2025-11-13 01:18:00,338 - INFO - ================================================================================
2025-11-13 01:18:00,338 - INFO - [ë°ì´í„° í¬ë§·íŒ… ì™„ë£Œ] ì „ì²´ GPU ìƒíƒœ
2025-11-13 01:18:00,338 - INFO -   ì´ GPU ìˆ˜: 1
2025-11-13 01:18:00,940 - INFO -   GPU0 (NVIDIA H100 80GB HBM3): 16.3GB/79.2GB (20.6%) | Load: 0%
2025-11-13 01:18:00,940 - INFO - ================================================================================
2025-11-13 01:18:00,942 - INFO - ================================================================================

2025-11-13 01:18:00,942 - INFO - 
================================================================================
2025-11-13 01:18:00,942 - INFO - [ SAMPLE ] ì²« ë²ˆì§¸ í›ˆë ¨ ë°ì´í„°:
2025-11-13 01:18:00,942 - INFO - ================================================================================
2025-11-13 01:18:00,942 - INFO - <|im_start|>system
You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>
<|im_start|>user
ì œê°€ í¸ì§‘í•  ë‚´ìš©ì´ ìˆì–´ìš”. ì œê°€ ì‘ì„±í•œ í…ìŠ¤íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ì•„ìš”,

"ì €ì™€ ì œ ì¹œêµ¬ë“¤ì€ ì˜¤ë«ë™ì•ˆ ì˜í™”ë¥¼ ë‹¤ì‹œ ë³´ëŸ¬ ê°€ì„œ ìš°ë¦¬ê°€ ê¸°ë‹¤ë ¤ì˜¨ ì˜í™”ë¥¼ ë³¼ ë‚ ì„ ê¸°ë‹¤ë ¤ì™”ì–´ìš”. ì–´ì ¯ë°¤ì— ì €ëŠ” ë§ˆì¹¨ë‚´ í‘œë¥¼ ì‚¬ëŸ¬ ê°”ëŠ”ë°, ì˜í™”ê´€ì— ë„ì°©í–ˆì„ ë•Œ í‘œê°€ ë§¤ì§„ë˜ì—ˆê³ , ê·¸ë˜ì„œ ì €ëŠ” ë§¤ìš° í™”ê°€ ë‚¬ì–´ìš”. ìš°ë¦¬ ëª¨ë‘ í™”ê°€ ë‚¬ì–´ìš”. ê·¸ë˜ì„œ ì˜¤ëŠ˜ ìš°ë¦¬ëŠ” ëŒ€ì‹  ê²½ê¸°ë¥¼ ë³´ëŸ¬ ê°ˆì§€ ê²°ì •í–ˆì§€ë§Œ, ì§€ê¸ˆì€ ê²½ê¸°ë¥¼ ë³´ëŸ¬ ê°€ì§€ ì•Šì„ ê²ƒ ê°™ì•„ìš”."<|im_end|>
<|im_start|>assistant
 ë¬¸ë²•, ëª…í™•ì„± ë° ì „ì²´ì ì¸ íë¦„ì„ ê°œì„ í•˜ê¸° ìœ„í•´ ëª‡ ê°€ì§€ ìˆ˜ì • ì‚¬í•­ì„ ì œì•ˆí•œ ìˆ˜ì •ëœ ë²„ì „ì˜ í…ìŠ¤íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ì•„ìš”:

"ì œ ì¹œêµ¬ë“¤ê³¼ ì €ëŠ” ì˜í™”ë¥¼ ë³´ëŸ¬ ëŒì•„ê°€ì„œ ìš°ë¦¬ê°€ ê°„ì ˆíˆ ê¸°ëŒ€í•´ ì˜¨ ì˜í™”ë¥¼ ë³´ê¸°ë¥¼ ì˜¤ë«ë™ì•ˆ ê¸°ë‹¤ë ¤ ì™”ì–´ìš”. ì–´...
2025-11-13 01:18:00,942 - INFO - ================================================================================

2025-11-13 01:18:01,122 - INFO - [ INFO ] SFTTrainer ì´ˆê¸°í™” ì¤‘...
Unsloth: Tokenizing ["text"] (num_proc=16):   0%|          | 0/190000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):   1%|          | 1000/190000 [00:19<1:01:35, 51.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):   1%|          | 2000/190000 [00:22<30:09, 103.90 examples/s] Unsloth: Tokenizing ["text"] (num_proc=16):   2%|â–         | 3000/190000 [00:26<22:32, 138.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):   2%|â–         | 4000/190000 [00:28<15:26, 200.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):   3%|â–         | 6000/190000 [00:29<08:37, 355.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):   4%|â–         | 7000/190000 [00:30<06:47, 449.42 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):   4%|â–         | 8000/190000 [00:32<06:23, 474.26 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):   5%|â–         | 9000/190000 [00:33<05:07, 587.78 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):   5%|â–Œ         | 10000/190000 [00:37<07:14, 414.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):   6%|â–Œ         | 11000/190000 [00:38<06:25, 464.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):   6%|â–‹         | 12000/190000 [00:39<04:43, 627.76 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):   8%|â–Š         | 15000/190000 [00:40<02:37, 1111.60 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):   8%|â–Š         | 16000/190000 [00:40<02:17, 1261.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):   9%|â–‰         | 17000/190000 [00:40<01:53, 1525.88 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):   9%|â–‰         | 18000/190000 [00:42<02:42, 1061.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  10%|â–ˆ         | 19000/190000 [00:42<02:14, 1272.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  11%|â–ˆ         | 20000/190000 [00:43<01:52, 1510.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  11%|â–ˆ         | 21000/190000 [00:44<02:43, 1035.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  12%|â–ˆâ–        | 22000/190000 [00:45<02:16, 1230.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  12%|â–ˆâ–        | 23000/190000 [00:45<01:51, 1500.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  13%|â–ˆâ–        | 23875/190000 [00:49<04:48, 575.47 examples/s] Unsloth: Tokenizing ["text"] (num_proc=16):  13%|â–ˆâ–        | 24875/190000 [00:50<03:35, 767.68 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  14%|â–ˆâ–        | 25875/190000 [00:50<03:15, 840.61 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  15%|â–ˆâ–Œ        | 28875/190000 [00:51<01:27, 1848.41 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  16%|â–ˆâ–Œ        | 29875/190000 [00:53<02:14, 1191.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  16%|â–ˆâ–‹        | 30875/190000 [00:53<01:50, 1443.49 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  17%|â–ˆâ–‹        | 31875/190000 [00:53<01:38, 1605.86 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  17%|â–ˆâ–‹        | 32750/190000 [00:55<02:20, 1123.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  18%|â–ˆâ–Š        | 33750/190000 [00:55<02:06, 1231.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  18%|â–ˆâ–Š        | 34750/190000 [01:00<04:43, 546.70 examples/s] Unsloth: Tokenizing ["text"] (num_proc=16):  20%|â–ˆâ–ˆ        | 38750/190000 [01:00<02:00, 1257.88 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  21%|â–ˆâ–ˆ        | 39750/190000 [01:01<01:55, 1302.00 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  21%|â–ˆâ–ˆâ–       | 40750/190000 [01:02<02:07, 1166.49 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  22%|â–ˆâ–ˆâ–       | 41750/190000 [01:03<02:07, 1159.00 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  22%|â–ˆâ–ˆâ–       | 42750/190000 [01:04<01:53, 1302.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  23%|â–ˆâ–ˆâ–       | 43750/190000 [01:05<02:04, 1171.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  24%|â–ˆâ–ˆâ–       | 44750/190000 [01:08<03:49, 632.60 examples/s] Unsloth: Tokenizing ["text"] (num_proc=16):  24%|â–ˆâ–ˆâ–       | 45625/190000 [01:08<02:56, 815.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  25%|â–ˆâ–ˆâ–       | 46625/190000 [01:09<02:32, 940.79 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  26%|â–ˆâ–ˆâ–Œ       | 49625/190000 [01:10<01:26, 1631.11 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  27%|â–ˆâ–ˆâ–‹       | 50625/190000 [01:11<01:29, 1554.08 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  27%|â–ˆâ–ˆâ–‹       | 51625/190000 [01:11<01:37, 1423.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  28%|â–ˆâ–ˆâ–Š       | 52625/190000 [01:12<01:40, 1362.93 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  28%|â–ˆâ–ˆâ–Š       | 53625/190000 [01:13<01:52, 1209.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  29%|â–ˆâ–ˆâ–‰       | 54625/190000 [01:14<01:45, 1281.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  29%|â–ˆâ–ˆâ–‰       | 55625/190000 [01:17<03:01, 741.88 examples/s] Unsloth: Tokenizing ["text"] (num_proc=16):  30%|â–ˆâ–ˆâ–‰       | 56625/190000 [01:18<02:49, 784.79 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  30%|â–ˆâ–ˆâ–ˆ       | 57625/190000 [01:18<02:09, 1018.67 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  32%|â–ˆâ–ˆâ–ˆâ–      | 61500/190000 [01:19<01:09, 1859.92 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  33%|â–ˆâ–ˆâ–ˆâ–      | 62500/190000 [01:20<01:16, 1670.41 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  33%|â–ˆâ–ˆâ–ˆâ–      | 63500/190000 [01:21<01:28, 1424.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  34%|â–ˆâ–ˆâ–ˆâ–      | 64500/190000 [01:22<01:24, 1476.70 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  34%|â–ˆâ–ˆâ–ˆâ–      | 65500/190000 [01:23<01:33, 1330.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 66500/190000 [01:24<01:45, 1170.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 67500/190000 [01:26<02:31, 807.81 examples/s] Unsloth: Tokenizing ["text"] (num_proc=16):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 68500/190000 [01:27<02:25, 836.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 69500/190000 [01:27<01:52, 1070.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 72375/190000 [01:28<00:56, 2078.75 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  39%|â–ˆâ–ˆâ–ˆâ–Š      | 73375/190000 [01:29<01:15, 1538.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 74375/190000 [01:29<01:09, 1653.72 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  40%|â–ˆâ–ˆâ–ˆâ–‰      | 75375/190000 [01:30<01:09, 1637.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 76375/190000 [01:32<01:36, 1181.19 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 77375/190000 [01:32<01:18, 1432.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 78375/190000 [01:33<01:18, 1423.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 79375/190000 [01:36<02:57, 624.72 examples/s] Unsloth: Tokenizing ["text"] (num_proc=16):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 80375/190000 [01:37<02:14, 816.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 85250/190000 [01:38<00:58, 1789.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 86250/190000 [01:39<01:13, 1405.54 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 87250/190000 [01:40<01:01, 1669.69 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 88250/190000 [01:40<01:03, 1597.67 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 89250/190000 [01:42<01:27, 1146.84 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 91250/190000 [01:45<01:56, 844.39 examples/s] Unsloth: Tokenizing ["text"] (num_proc=16):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 92250/190000 [01:46<01:49, 892.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 96125/190000 [01:47<00:54, 1726.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 97125/190000 [01:47<00:47, 1975.98 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 98125/190000 [01:48<00:58, 1567.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 99125/190000 [01:49<00:55, 1638.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 100125/190000 [01:49<00:57, 1553.91 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 101125/190000 [01:50<01:10, 1260.49 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 102125/190000 [01:51<01:05, 1345.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103125/190000 [01:52<01:02, 1393.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 104125/190000 [01:55<02:04, 688.51 examples/s] Unsloth: Tokenizing ["text"] (num_proc=16):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 105125/190000 [01:55<01:37, 871.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 109000/190000 [01:56<00:46, 1728.39 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 110000/190000 [01:57<00:43, 1856.78 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 111000/190000 [01:58<00:54, 1438.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 112000/190000 [01:59<00:54, 1420.75 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 113000/190000 [01:59<00:49, 1562.36 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 114000/190000 [02:01<01:07, 1124.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 115000/190000 [02:03<01:42, 729.74 examples/s] Unsloth: Tokenizing ["text"] (num_proc=16):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 116000/190000 [02:05<01:49, 674.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 119875/190000 [02:06<00:45, 1542.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 120875/190000 [02:06<00:43, 1590.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 121875/190000 [02:07<00:41, 1650.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 122875/190000 [02:07<00:40, 1643.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 123875/190000 [02:09<00:54, 1213.66 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 124875/190000 [02:09<00:45, 1429.62 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 125875/190000 [02:10<00:45, 1404.13 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 126875/190000 [02:13<01:31, 693.10 examples/s] Unsloth: Tokenizing ["text"] (num_proc=16):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 127875/190000 [02:14<01:23, 740.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 128875/190000 [02:15<01:04, 954.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 131750/190000 [02:15<00:32, 1765.21 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 133750/190000 [02:16<00:33, 1690.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 134750/190000 [02:17<00:37, 1491.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 135750/190000 [02:18<00:31, 1698.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 136750/190000 [02:19<00:39, 1347.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 137750/190000 [02:20<00:41, 1270.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 138750/190000 [02:20<00:33, 1523.21 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 139750/190000 [02:24<01:14, 675.47 examples/s] Unsloth: Tokenizing ["text"] (num_proc=16):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 140750/190000 [02:24<00:56, 870.36 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 144625/190000 [02:25<00:26, 1711.76 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 146625/190000 [02:26<00:27, 1565.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 147625/190000 [02:27<00:29, 1422.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 149625/190000 [02:29<00:29, 1376.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 150625/190000 [02:32<00:46, 847.44 examples/s] Unsloth: Tokenizing ["text"] (num_proc=16):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 151625/190000 [02:33<00:47, 804.63 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 155500/190000 [02:34<00:21, 1577.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 156500/190000 [02:34<00:18, 1787.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 157500/190000 [02:35<00:21, 1490.08 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 158500/190000 [02:36<00:19, 1621.41 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 159500/190000 [02:37<00:21, 1448.61 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 160500/190000 [02:38<00:23, 1229.80 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 161500/190000 [02:38<00:20, 1402.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 162500/190000 [02:39<00:22, 1241.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 163500/190000 [02:40<00:22, 1167.42 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 164500/190000 [02:41<00:19, 1297.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 165500/190000 [02:42<00:20, 1222.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 166375/190000 [02:43<00:19, 1202.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 167375/190000 [02:43<00:14, 1568.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 168375/190000 [02:44<00:15, 1413.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 169375/190000 [02:44<00:15, 1331.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 170375/190000 [02:45<00:15, 1231.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 171375/190000 [02:46<00:14, 1309.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 172375/190000 [02:47<00:13, 1294.21 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 173375/190000 [02:48<00:14, 1148.63 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 174375/190000 [02:49<00:13, 1186.41 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 175375/190000 [02:49<00:11, 1257.88 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 176375/190000 [02:51<00:12, 1076.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 177375/190000 [02:52<00:11, 1069.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 178250/190000 [02:52<00:08, 1336.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 179250/190000 [02:53<00:10, 1022.66 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 180250/190000 [02:54<00:09, 1072.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 181250/190000 [02:56<00:10, 844.28 examples/s] Unsloth: Tokenizing ["text"] (num_proc=16):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 182250/190000 [02:57<00:08, 919.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 183250/190000 [02:59<00:08, 776.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 184250/190000 [02:59<00:06, 882.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 185250/190000 [03:01<00:06, 731.49 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 186125/190000 [03:01<00:04, 955.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 187125/190000 [03:04<00:04, 686.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 188125/190000 [03:06<00:03, 553.84 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 189125/190000 [03:09<00:01, 498.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 190000/190000 [03:11<00:00, 465.61 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 190000/190000 [03:14<00:00, 975.53 examples/s]
Unsloth: Tokenizing ["text"] (num_proc=16):   0%|          | 0/10000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):   6%|â–‹         | 625/10000 [00:09<02:28, 63.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  12%|â–ˆâ–        | 1250/10000 [00:10<01:04, 135.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  19%|â–ˆâ–‰        | 1875/10000 [00:11<00:38, 212.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  25%|â–ˆâ–ˆâ–Œ       | 2500/10000 [00:12<00:25, 288.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  31%|â–ˆâ–ˆâ–ˆâ–      | 3125/10000 [00:13<00:18, 364.64 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3750/10000 [00:14<00:14, 423.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4375/10000 [00:15<00:11, 477.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5000/10000 [00:16<00:09, 524.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 5625/10000 [00:17<00:07, 573.11 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 6250/10000 [00:18<00:06, 564.19 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 6875/10000 [00:19<00:05, 598.88 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 7500/10000 [00:20<00:04, 612.73 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 8125/10000 [00:21<00:03, 604.86 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 8750/10000 [00:22<00:01, 636.07 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 9375/10000 [00:23<00:00, 629.79 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:24<00:00, 642.54 examples/s]Unsloth: Tokenizing ["text"] (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:25<00:00, 395.07 examples/s]
2025-11-13 01:21:41,939 - INFO - [ COMPLETE ] Trainer ì´ˆê¸°í™” ì™„ë£Œ
2025-11-13 01:21:41,939 - INFO - ================================================================================
2025-11-13 01:21:41,939 - INFO -  í•™ìŠµ ì‹œì‘ 
2025-11-13 01:21:41,939 - INFO -  GPU: NVIDIA H100 80GB HBM3
2025-11-13 01:21:41,940 - INFO -  Flash Attention: True
2025-11-13 01:21:41,940 - INFO - ================================================================================
2025-11-13 01:21:41,940 - INFO - 
================================================================================
2025-11-13 01:21:41,940 - INFO - [í›ˆë ¨ ì‹œì‘] ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§
2025-11-13 01:21:41,940 - INFO - ================================================================================
2025-11-13 01:21:42,173 - INFO - ================================================================================
2025-11-13 01:21:42,173 - INFO - [í›ˆë ¨ ì‹œì‘] CPU/RAM ìƒì„¸ ëª¨ë‹ˆí„°ë§
2025-11-13 01:21:42,173 - INFO -   ì‹œê°„: 2025-11-13 01:21:42
2025-11-13 01:21:42,173 - INFO -   CPU ì‚¬ìš©ë¥ : 0.8% (12 cores @ 2000 MHz)
2025-11-13 01:21:42,173 - INFO -   RAM: 14.6GB / 187.0GB (7.8%)
2025-11-13 01:21:42,173 - INFO -   í”„ë¡œì„¸ìŠ¤ CPU: 0.0%
2025-11-13 01:21:42,173 - INFO -   í”„ë¡œì„¸ìŠ¤ RAM: 13.17GB
2025-11-13 01:21:42,174 - INFO -   í”„ë¡œì„¸ìŠ¤ ìŠ¤ë ˆë“œ: 56
2025-11-13 01:21:42,176 - INFO -   Top 3 í”„ë¡œì„¸ìŠ¤:
2025-11-13 01:21:42,177 - INFO -     1. python (PID: 1770): CPU 41.9% | RAM 7.0%
2025-11-13 01:21:42,177 - INFO -     2. node (PID: 339): CPU 0.3% | RAM 0.1%
2025-11-13 01:21:42,177 - INFO -     3. node (PID: 416): CPU 0.3% | RAM 0.2%
2025-11-13 01:21:42,177 - INFO - ================================================================================
2025-11-13 01:21:42,177 - INFO - ================================================================================
2025-11-13 01:21:42,177 - INFO - [í›ˆë ¨ ì‹œì‘] ì „ì²´ GPU ìƒíƒœ
2025-11-13 01:21:42,177 - INFO -   ì´ GPU ìˆ˜: 1
2025-11-13 01:21:42,299 - INFO -   GPU0 (NVIDIA H100 80GB HBM3): 16.3GB/79.2GB (20.6%) | Load: 0%
2025-11-13 01:21:42,299 - INFO - ================================================================================
2025-11-13 01:21:42,301 - INFO - ================================================================================

The model is already on multiple devices. Skipping the move to device specified in `args`.
2025-11-13 01:21:42,818 - INFO - ================================================================================
2025-11-13 01:21:42,818 - INFO - [ START ] í•™ìŠµ ì‹œì‘
2025-11-13 01:21:42,818 - INFO -   ì‹œì‘ ì‹œê°„: 2025-11-13 01:21:42
2025-11-13 01:21:42,818 - INFO - ================================================================================
  0%|          | 0/10179 [00:00<?, ?it/s]/home/work/.local/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/home/work/.local/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Unsloth: Will smartly offload gradients to save VRAM!
  0%|          | 1/10179 [00:36<104:25:05, 36.93s/it]  0%|          | 2/10179 [00:58<78:24:14, 27.73s/it]   0%|          | 3/10179 [01:19<69:27:13, 24.57s/it]  0%|          | 4/10179 [01:40<65:46:57, 23.27s/it]  0%|          | 5/10179 [02:01<63:47:39, 22.57s/it]  0%|          | 6/10179 [02:22<62:29:17, 22.11s/it]  0%|          | 7/10179 [02:43<60:45:09, 21.50s/it]  0%|          | 8/10179 [03:03<59:42:53, 21.14s/it]  0%|          | 9/10179 [03:23<59:03:54, 20.91s/it]