#!/usr/bin/env python3
"""
Phase 1 í•œêµ­ì–´ ê°•í™” ë°ì´í„° ì¤€ë¹„
- ì¤‘êµ­ì–´ í•„í„°ë§
- Messages í˜•ì‹ í†µì¼
- í•œêµ­ì–´ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
- 256K ìƒ˜í”Œ ìƒì„±
"""

import json
import random
import argparse
from pathlib import Path
from tqdm import tqdm
import re


def has_chinese(text: str) -> bool:
    """ì¤‘êµ­ì–´ í¬í•¨ ì—¬ë¶€ í™•ì¸"""
    return bool(re.search(r'[\u4e00-\u9fff]+', text))


def parse_chatml(text: str) -> list[dict]:
    """ChatML í˜•ì‹ íŒŒì‹±"""
    messages = []
    parts = text.split('<|im_start|>')
    
    for part in parts[1:]:
        if '<|im_end|>' in part:
            split_part = part.split('\n', 1)
            if len(split_part) < 2:
                continue
            role = split_part[0].strip()
            content = split_part[1].split('<|im_end|>')[0].strip()
            
            if role in ['system', 'user', 'assistant']:
                messages.append({"role": role, "content": content})
    
    return messages


def extract_messages(data_item: dict) -> list[dict] | None:
    """ë°ì´í„°ì—ì„œ messages ì¶”ì¶œ"""
    if 'messages' in data_item:
        return data_item['messages']
    elif 'text' in data_item:
        return parse_chatml(data_item['text'])
    else:
        return None


def prepare_phase1_data(
    input_dir: str = 'korean_large_data/cleaned_jsonl',
    output_dir: str = 'phase1_korean',
    target_samples: int = 256000,
    exclude_datasets: list[str] = ['identity_training_data.jsonl'],
    seed: int = 42
):
    """Phase 1ìš© í•œêµ­ì–´ ê°•í™” ë°ì´í„° ì¤€ë¹„"""
    
    input_dir = Path(input_dir)
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    all_data = []
    stats = {}
    
    print("ğŸ“– ë°ì´í„° ë¡œë”© ë° ì¤‘êµ­ì–´ í•„í„°ë§ ì‹œì‘...")
    print(f"   ì…ë ¥ ë””ë ‰í† ë¦¬: {input_dir}")
    print(f"   ì œì™¸ íŒŒì¼: {exclude_datasets}")
    print()
    
    # ëª¨ë“  JSONL íŒŒì¼ ë¡œë“œ
    jsonl_files = sorted(input_dir.glob('*.jsonl'))
    
    for jsonl_file in jsonl_files:
        if jsonl_file.name in exclude_datasets:
            print(f"â­ï¸  ê±´ë„ˆëœ€: {jsonl_file.name}")
            continue
        
        print(f"ğŸ“– ì²˜ë¦¬ ì¤‘: {jsonl_file.name}")
        
        with open(jsonl_file, 'r', encoding='utf-8') as f:
            data = []
            for line in f:
                if line.strip():
                    data.append(json.loads(line))
        
        # ì¤‘êµ­ì–´ í•„í„°ë§
        filtered = []
        chinese_count = 0
        empty_count = 0
        
        for item in tqdm(data, desc="  ì¤‘êµ­ì–´ í•„í„°ë§"):
            messages = extract_messages(item)
            
            if not messages:
                empty_count += 1
                continue
            
            # ëª¨ë“  ë©”ì‹œì§€ ë‚´ìš© ê²€ì‚¬
            has_chinese_content = False
            for msg in messages:
                if has_chinese(msg.get('content', '')):
                    has_chinese_content = True
                    break
            
            if not has_chinese_content:
                filtered.append({
                    'messages': messages,
                    'source': jsonl_file.stem
                })
            else:
                chinese_count += 1
        
        stats[jsonl_file.name] = {
            'original': len(data),
            'filtered': len(filtered),
            'removed_chinese': chinese_count,
            'removed_empty': empty_count
        }
        
        all_data.extend(filtered)
        print(f"   ê²°ê³¼: {len(data):,} â†’ {len(filtered):,} (ì¤‘êµ­ì–´: {chinese_count:,}, ë¹ˆ ë°ì´í„°: {empty_count:,})")
    
    print(f"\nğŸ“Š ì´ ìˆ˜ì§‘: {len(all_data):,}ê°œ")
    
    # íƒ€ê²Ÿ ìƒ˜í”Œ ìˆ˜ì— ë§ê²Œ ìƒ˜í”Œë§
    random.seed(seed)
    if len(all_data) > target_samples:
        print(f"ğŸ² ìƒ˜í”Œë§: {len(all_data):,} â†’ {target_samples:,}")
        all_data = random.sample(all_data, target_samples)
    else:
        print(f"âœ… ì „ì²´ ë°ì´í„° ì‚¬ìš©: {len(all_data):,}ê°œ")
    
    # Messages í˜•ì‹ í†µì¼ + í•œêµ­ì–´ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    korean_system = {
        "role": "system",
        "content": "ë‹¹ì‹ ì€ í•œêµ­ì–´ ì „ìš© AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ëª¨ë“  ì‘ë‹µì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ì‘ì„±í•˜ì„¸ìš”."
    }
    
    processed = []
    source_distribution = {}
    
    print("\nğŸ”„ í˜•ì‹ ë³€í™˜ ë° ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¶”ê°€ ì¤‘...")
    
    for item in tqdm(all_data, desc="í˜•ì‹ ë³€í™˜"):
        messages = item['messages']
        source = item.get('source', 'unknown')
        
        # ì†ŒìŠ¤ë³„ ë¶„í¬ ì§‘ê³„
        source_distribution[source] = source_distribution.get(source, 0) + 1
        
        # system ë©”ì‹œì§€ ì œê±° í›„ í•œêµ­ì–´ ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì¶”ê°€
        user_assistant_msgs = [m for m in messages if m['role'] in ['user', 'assistant']]
        
        if not user_assistant_msgs:
            continue
        
        new_messages = [korean_system] + user_assistant_msgs
        
        processed.append({
            "messages": new_messages,
            "source": source
        })
    
    # ì €ì¥
    output_file = output_dir / f'phase1_korean_{len(processed)}samples.jsonl'
    with open(output_file, 'w', encoding='utf-8') as f:
        for item in processed:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    print(f"\nâœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {output_file}")
    print(f"   ìµœì¢… ìƒ˜í”Œ: {len(processed):,}ê°œ")
    
    # ì†ŒìŠ¤ë³„ ë¶„í¬ ì¶œë ¥
    print(f"\nğŸ“Š ì†ŒìŠ¤ë³„ ë¶„í¬:")
    for source, count in sorted(source_distribution.items(), key=lambda x: x[1], reverse=True):
        percentage = (count / len(processed)) * 100
        print(f"   {source}: {count:,}ê°œ ({percentage:.1f}%)")
    
    # í†µê³„ ì €ì¥
    stats_file = output_dir / 'phase1_stats.json'
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump({
            'file_stats': stats,
            'source_distribution': source_distribution,
            'total_samples': len(processed),
            'target_samples': target_samples,
            'seed': seed
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“ˆ í†µê³„ ì €ì¥: {stats_file}")
    
    # ìƒ˜í”Œ ë¯¸ë¦¬ë³´ê¸°
    print(f"\nğŸ” ìƒ˜í”Œ ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 2ê°œ):")
    for i, sample in enumerate(processed[:2], 1):
        print(f"\n[ìƒ˜í”Œ {i}] - ì†ŒìŠ¤: {sample['source']}")
        for msg in sample['messages']:
            content_preview = msg['content'][:100] + "..." if len(msg['content']) > 100 else msg['content']
            print(f"  {msg['role']}: {content_preview}")
    
    return processed


def main():
    parser = argparse.ArgumentParser(description="Phase 1 ë°ì´í„° ì¤€ë¹„")
    parser.add_argument(
        '--input-dir',
        type=str,
        default='../../korean_large_data/cleaned_jsonl',
        help='ì…ë ¥ JSONL ë””ë ‰í† ë¦¬'
    )
    parser.add_argument(
        '--output-dir',
        type=str,
        default='phase1_korean',
        help='ì¶œë ¥ ë””ë ‰í† ë¦¬'
    )
    parser.add_argument(
        '--target-samples',
        type=int,
        default=256000,
        help='ëª©í‘œ ìƒ˜í”Œ ìˆ˜'
    )
    parser.add_argument(
        '--exclude',
        type=str,
        nargs='+',
        default=['identity_training_data.jsonl'],
        help='ì œì™¸í•  íŒŒì¼ ëª©ë¡'
    )
    parser.add_argument(
        '--seed',
        type=int,
        default=42,
        help='ëœë¤ ì‹œë“œ'
    )
    
    args = parser.parse_args()
    
    prepare_phase1_data(
        input_dir=args.input_dir,
        output_dir=args.output_dir,
        target_samples=args.target_samples,
        exclude_datasets=args.exclude,
        seed=args.seed
    )


if __name__ == "__main__":
    main()

