#!/usr/bin/env python3
"""
Phase 1: Qwen3-VL í•œêµ­ì–´ ê°•í™” íŒŒì¸íŠœë‹
- Qwen3-VL-8B-Instruct ê¸°ë°˜
- í•œêµ­ì–´ ëŠ¥ë ¥ í–¥ìƒ
- ì¤‘êµ­ì–´ ì°¨ë‹¨
- LoRA ì‚¬ìš©
"""

import os
import sys
import torch
import argparse
from pathlib import Path
from datetime import datetime

from unsloth import FastLanguageModel, is_bfloat16_supported
from datasets import load_dataset
from trl import SFTTrainer
from transformers import TrainingArguments


def setup_model(
    model_name: str = "Qwen/Qwen3-VL-8B-Instruct",
    max_seq_length: int = 4096,
    load_in_4bit: bool = True,
    lora_r: int = 16,
    lora_alpha: int = 32,
):
    """ëª¨ë¸ ë° LoRA ì„¤ì •"""
    
    print("ğŸš€ ëª¨ë¸ ë¡œë”© ì¤‘...")
    print(f"   ëª¨ë¸: {model_name}")
    print(f"   ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´: {max_seq_length}")
    print(f"   4bit ì–‘ìí™”: {load_in_4bit}")
    
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=model_name,
        max_seq_length=max_seq_length,
        load_in_4bit=load_in_4bit,
        dtype=None,  # Auto detection
        trust_remote_code=True,
    )
    
    print("\nğŸ”§ LoRA ì ìš© ì¤‘...")
    print(f"   Rank: {lora_r}")
    print(f"   Alpha: {lora_alpha}")
    
    model = FastLanguageModel.get_peft_model(
        model,
        r=lora_r,
        target_modules=[
            "q_proj", "k_proj", "v_proj", "o_proj",
            "gate_proj", "up_proj", "down_proj"
        ],
        lora_alpha=lora_alpha,
        lora_dropout=0.05,
        bias="none",
        use_gradient_checkpointing="unsloth",
        random_state=42,
        use_rslora=False,
        loftq_config=None,
    )
    
    print("âœ… ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ!\n")
    
    return model, tokenizer


def load_training_data(data_dir: str = "phase1_korean"):
    """í•™ìŠµ ë°ì´í„° ë¡œë“œ"""
    
    data_dir = Path(data_dir)
    
    print(f"ğŸ“– í•™ìŠµ ë°ì´í„° ë¡œë”©...")
    print(f"   ë””ë ‰í† ë¦¬: {data_dir}")
    
    # JSONL íŒŒì¼ ì°¾ê¸°
    jsonl_files = list(data_dir.glob("*.jsonl"))
    
    if not jsonl_files:
        raise FileNotFoundError(f"JSONL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_dir}")
    
    print(f"   ë°œê²¬ëœ íŒŒì¼: {len(jsonl_files)}ê°œ")
    for f in jsonl_files:
        print(f"      - {f.name}")
    
    # ë°ì´í„°ì…‹ ë¡œë“œ
    dataset = load_dataset(
        'json',
        data_files=[str(f) for f in jsonl_files],
        split='train'
    )
    
    print(f"\nâœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ!")
    print(f"   ì´ ìƒ˜í”Œ: {len(dataset):,}ê°œ")
    
    # ìƒ˜í”Œ í™•ì¸
    if len(dataset) > 0:
        sample = dataset[0]
        print(f"\nğŸ” ë°ì´í„° êµ¬ì¡°:")
        print(f"   í‚¤: {list(sample.keys())}")
        if 'messages' in sample:
            print(f"   ë©”ì‹œì§€ ìˆ˜: {len(sample['messages'])}")
            print(f"   ì²« ë©”ì‹œì§€ ì—­í• : {sample['messages'][0]['role']}")
    
    return dataset


def format_chat_template(example, tokenizer):
    """ì±„íŒ… í…œí”Œë¦¿ í¬ë§·íŒ…"""
    messages = example['messages']
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=False
    )
    return {"text": text}


def train_phase1(
    model_name: str = "Qwen/Qwen3-VL-8B-Instruct",
    data_dir: str = "phase1_korean",
    output_dir: str = "qwen3-vl-8b-korean-instruct",
    max_seq_length: int = 4096,
    num_train_epochs: int = 2,
    per_device_train_batch_size: int = 4,
    gradient_accumulation_steps: int = 4,
    learning_rate: float = 2e-5,
    warmup_steps: int = 100,
    logging_steps: int = 10,
    save_steps: int = 500,
    lora_r: int = 16,
    lora_alpha: int = 32,
):
    """Phase 1 í•™ìŠµ ì‹¤í–‰"""
    
    print("="*80)
    print("Phase 1: Qwen3-VL í•œêµ­ì–´ ê°•í™” íŒŒì¸íŠœë‹")
    print("="*80)
    print(f"ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    # ëª¨ë¸ ì„¤ì •
    model, tokenizer = setup_model(
        model_name=model_name,
        max_seq_length=max_seq_length,
        load_in_4bit=True,
        lora_r=lora_r,
        lora_alpha=lora_alpha,
    )
    
    # ë°ì´í„° ë¡œë“œ
    dataset = load_training_data(data_dir)
    
    # ë°ì´í„° í¬ë§·íŒ…
    print("\nğŸ”„ ë°ì´í„° í¬ë§·íŒ… ì¤‘...")
    dataset = dataset.map(
        lambda x: format_chat_template(x, tokenizer),
        remove_columns=dataset.column_names
    )
    print("âœ… í¬ë§·íŒ… ì™„ë£Œ!\n")
    
    # í•™ìŠµ ì„¤ì •
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("âš™ï¸  í•™ìŠµ ì„¤ì •:")
    print(f"   ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
    print(f"   Epochs: {num_train_epochs}")
    print(f"   ë°°ì¹˜ í¬ê¸°: {per_device_train_batch_size}")
    print(f"   Gradient Accumulation: {gradient_accumulation_steps}")
    print(f"   íš¨ê³¼ì  ë°°ì¹˜ í¬ê¸°: {per_device_train_batch_size * gradient_accumulation_steps}")
    print(f"   í•™ìŠµë¥ : {learning_rate}")
    print(f"   Warmup Steps: {warmup_steps}")
    print()
    
    training_args = TrainingArguments(
        output_dir=str(output_dir),
        per_device_train_batch_size=per_device_train_batch_size,
        gradient_accumulation_steps=gradient_accumulation_steps,
        warmup_steps=warmup_steps,
        num_train_epochs=num_train_epochs,
        learning_rate=learning_rate,
        fp16=not is_bfloat16_supported(),
        bf16=is_bfloat16_supported(),
        logging_steps=logging_steps,
        logging_dir=str(output_dir / "logs"),
        optim="adamw_8bit",
        weight_decay=0.01,
        lr_scheduler_type="cosine",
        save_strategy="steps",
        save_steps=save_steps,
        save_total_limit=3,
        report_to=["tensorboard"],
        seed=42,
    )
    
    # Trainer ìƒì„±
    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=dataset,
        dataset_text_field="text",
        max_seq_length=max_seq_length,
        args=training_args,
        packing=False,
    )
    
    # í•™ìŠµ ì‹œì‘
    print("ğŸš€ í•™ìŠµ ì‹œì‘!")
    print("="*80)
    
    trainer.train()
    
    print("\n" + "="*80)
    print("âœ… í•™ìŠµ ì™„ë£Œ!")
    print("="*80)
    
    # ëª¨ë¸ ì €ì¥
    print(f"\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘: {output_dir}")
    model.save_pretrained(str(output_dir))
    tokenizer.save_pretrained(str(output_dir))
    
    # LoRA ì–´ëŒ‘í„° ë³‘í•© (ì„ íƒì )
    print("\nğŸ’¾ LoRA ì–´ëŒ‘í„° ë³‘í•© ì¤‘...")
    model.save_pretrained_merged(
        str(output_dir / "merged"),
        tokenizer,
        save_method="merged_16bit",
    )
    
    print(f"\nâœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    print(f"   ëª¨ë¸ ìœ„ì¹˜: {output_dir}")
    print(f"   ë³‘í•© ëª¨ë¸: {output_dir / 'merged'}")
    print(f"   ì¢…ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")


def main():
    parser = argparse.ArgumentParser(description="Phase 1 í•™ìŠµ")
    
    parser.add_argument(
        '--model-name',
        type=str,
        default="Qwen/Qwen3-VL-8B-Instruct",
        help='ê¸°ë³¸ ëª¨ë¸ ì´ë¦„'
    )
    parser.add_argument(
        '--data-dir',
        type=str,
        default="phase1_korean",
        help='í•™ìŠµ ë°ì´í„° ë””ë ‰í† ë¦¬'
    )
    parser.add_argument(
        '--output-dir',
        type=str,
        default="qwen3-vl-8b-korean-instruct",
        help='ì¶œë ¥ ë””ë ‰í† ë¦¬'
    )
    parser.add_argument(
        '--max-seq-length',
        type=int,
        default=4096,
        help='ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´'
    )
    parser.add_argument(
        '--epochs',
        type=int,
        default=2,
        help='í•™ìŠµ ì—í­ ìˆ˜'
    )
    parser.add_argument(
        '--batch-size',
        type=int,
        default=4,
        help='ë°°ì¹˜ í¬ê¸°'
    )
    parser.add_argument(
        '--gradient-accumulation',
        type=int,
        default=4,
        help='Gradient Accumulation Steps'
    )
    parser.add_argument(
        '--learning-rate',
        type=float,
        default=2e-5,
        help='í•™ìŠµë¥ '
    )
    parser.add_argument(
        '--lora-r',
        type=int,
        default=16,
        help='LoRA rank'
    )
    parser.add_argument(
        '--lora-alpha',
        type=int,
        default=32,
        help='LoRA alpha'
    )
    
    args = parser.parse_args()
    
    train_phase1(
        model_name=args.model_name,
        data_dir=args.data_dir,
        output_dir=args.output_dir,
        max_seq_length=args.max_seq_length,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.batch_size,
        gradient_accumulation_steps=args.gradient_accumulation,
        learning_rate=args.learning_rate,
        lora_r=args.lora_r,
        lora_alpha=args.lora_alpha,
    )


if __name__ == "__main__":
    main()

