#!/usr/bin/env python3
"""
EXAONE 4.0 1.2Bë¥¼ ì‚¬ìš©í•œ CoT ë°ì´í„° ì¦ê°•
- í•œêµ­ì–´ íŠ¹í™” Chain-of-Thought ìƒì„±
- ì‹±ê¸€í„´/ë©€í‹°í„´ ì§€ì›
- H100 80GB ìµœì í™”
"""

import json
import random
import argparse
from pathlib import Path
from typing import Optional
from vllm import LLM, SamplingParams
from tqdm import tqdm
import re


class CoTDataAugmenter:
    def __init__(self, model_name: str = "LGAI-EXAONE/EXAONE-4.0-1.2B-Instruct"):
        print("EXAONE 4.0 1.2B ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.llm = LLM(
            model=model_name,
            dtype="bfloat16",
            gpu_memory_utilization=0.9,
            max_model_len=8192,
            tensor_parallel_size=1,
            trust_remote_code=True,
        )
        
        self.sampling_params = SamplingParams(
            temperature=0.7,
            top_p=0.9,
            max_tokens=2048,
            stop=["</s>", "<|im_end|>"],
        )
        
        print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    
    def sample_dataset(self, input_jsonl: str, sample_ratio: float = 0.05, seed: int = 42) -> list:
        """ë°ì´í„°ì…‹ì—ì„œ ì¼ì • ë¹„ìœ¨ë§Œ ìƒ˜í”Œë§"""
        print(f"ğŸ“– ë°ì´í„° ìƒ˜í”Œë§: {input_jsonl} (ë¹„ìœ¨: {sample_ratio*100}%)")
        
        all_data = []
        with open(input_jsonl, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    all_data.append(json.loads(line))
        
        if sample_ratio >= 1.0:
            print(f"âœ… ì „ì²´ ë°ì´í„° ì‚¬ìš©: {len(all_data):,}ê°œ")
            return all_data
        
        random.seed(seed)
        num_samples = int(len(all_data) * sample_ratio)
        sampled = random.sample(all_data, num_samples)
        
        print(f"âœ… ì „ì²´ {len(all_data):,}ê°œ ì¤‘ {len(sampled):,}ê°œ ìƒ˜í”Œë§ë¨")
        return sampled
    
    def create_singleturn_prompt(self, question: str, answer: str) -> str:
        """ì‹±ê¸€í„´ CoT ìƒì„± í”„ë¡¬í”„íŠ¸"""
        return f"""ë‹¹ì‹ ì€ ë‹¨ê³„ë³„ ì¶”ë¡ ì„ í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ë‹¤ìŒ ë¬¸ì œì— ëŒ€í•œ ë‹µë³€ì„ <think> íƒœê·¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¨ê³„ë³„ë¡œ ì‘ì„±í•˜ì„¸ìš”.

[ê·œì¹™]
1. <think> íƒœê·¸ ì•ˆì— ì¶”ë¡  ê³¼ì •ì„ í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê²Œ ì‘ì„±
2. ê° ë‹¨ê³„ëŠ” "1ë‹¨ê³„:", "2ë‹¨ê³„:" í˜•ì‹ìœ¼ë¡œ êµ¬ë¶„
3. </think> íƒœê·¸ ë°–ì— ìµœì¢… ë‹µë³€ë§Œ ê°„ê²°í•˜ê²Œ ì‘ì„±
4. ì ˆëŒ€ ì¤‘êµ­ì–´ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”

[ì§ˆë¬¸]
{question}

[ì›ë³¸ ë‹µë³€ ì°¸ê³ ]
{answer}

[ì¶œë ¥ í˜•ì‹]
<think>
1ë‹¨ê³„: [ë¬¸ì œ ë¶„ì„ ë° ì´í•´]
2ë‹¨ê³„: [í•´ê²° ë°©ë²• ê²°ì •]
3ë‹¨ê³„: [ë‹¨ê³„ë³„ ê³„ì‚°/ì¶”ë¡ ]
4ë‹¨ê³„: [ë‹µë³€ ê²€ì¦]
</think>

ìµœì¢… ë‹µë³€: [ê°„ê²°í•œ ë‹µ]"""

    def create_multiturn_prompt(self, messages: list[dict]) -> str:
        """ë©€í‹°í„´ CoT ìƒì„± í”„ë¡¬í”„íŠ¸"""
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ êµ¬ì„±
        conversation = []
        for msg in messages:
            if msg['role'] == 'user':
                conversation.append(f"ì‚¬ìš©ì: {msg['content']}")
            elif msg['role'] == 'assistant':
                conversation.append(f"ì–´ì‹œìŠ¤í„´íŠ¸: {msg['content']}")
        
        history = "\n".join(conversation[:-1])  # ë§ˆì§€ë§‰ ë‹µë³€ ì œì™¸
        last_question = messages[-2]['content'] if len(messages) >= 2 else ""
        original_answer = messages[-1]['content'] if messages else ""
        
        return f"""ë‹¹ì‹ ì€ ë‹¨ê³„ë³„ ì¶”ë¡ ì„ í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ë‹¤ìŒì€ ì‚¬ìš©ìì™€ì˜ ëŒ€í™”ì…ë‹ˆë‹¤. ë§ˆì§€ë§‰ ì§ˆë¬¸ì— ëŒ€í•´ <think> íƒœê·¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¨ê³„ë³„ë¡œ ë‹µë³€í•˜ì„¸ìš”.

[ì´ì „ ëŒ€í™”]
{history}

[í˜„ì¬ ì§ˆë¬¸]
{last_question}

[ì›ë³¸ ë‹µë³€ ì°¸ê³ ]
{original_answer}

[ê·œì¹™]
1. <think> íƒœê·¸ ì•ˆì— ì¶”ë¡  ê³¼ì •ì„ í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê²Œ ì‘ì„±
2. ì´ì „ ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ë‹µë³€
3. </think> íƒœê·¸ ë°–ì— ìµœì¢… ë‹µë³€ë§Œ ê°„ê²°í•˜ê²Œ ì‘ì„±
4. ì ˆëŒ€ ì¤‘êµ­ì–´ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”

[ì¶œë ¥ í˜•ì‹]
<think>
1ë‹¨ê³„: [ì´ì „ ëŒ€í™” ë§¥ë½ íŒŒì•…]
2ë‹¨ê³„: [í˜„ì¬ ì§ˆë¬¸ ë¶„ì„]
3ë‹¨ê³„: [ë‹µë³€ êµ¬ì„±]
4ë‹¨ê³„: [ì¼ê´€ì„± ê²€ì¦]
</think>

ìµœì¢… ë‹µë³€: [ê°„ê²°í•œ ë‹µ]"""

    def parse_chatml(self, text: str) -> list[dict]:
        """ChatML íŒŒì‹±"""
        messages = []
        parts = text.split('<|im_start|>')
        
        for part in parts[1:]:
            if '<|im_end|>' in part:
                split_part = part.split('\n', 1)
                if len(split_part) < 2:
                    continue
                role = split_part[0].strip()
                content = split_part[1].split('<|im_end|>')[0].strip()
                
                if role in ['system', 'user', 'assistant']:
                    messages.append({"role": role, "content": content})
        
        return messages
    
    def extract_qa_pairs(self, data_item: dict) -> Optional[dict]:
        """ë°ì´í„°ì—ì„œ Q&A ì¶”ì¶œ"""
        if 'messages' in data_item:
            messages = data_item['messages']
        else:
            messages = self.parse_chatml(data_item.get('text', ''))
        
        # system ë©”ì‹œì§€ ì œì™¸
        messages = [m for m in messages if m['role'] != 'system']
        
        user_msgs = [m for m in messages if m['role'] == 'user']
        assistant_msgs = [m for m in messages if m['role'] == 'assistant']
        
        if not user_msgs or not assistant_msgs:
            return None
        
        is_multiturn = len(user_msgs) > 1 and len(assistant_msgs) > 1
        
        return {
            'messages': messages,
            'is_multiturn': is_multiturn,
            'last_question': user_msgs[-1]['content'],
            'last_answer': assistant_msgs[-1]['content']
        }
    
    def generate_cot_batch(self, prompts: list[str], batch_size: int = 128) -> list[str]:
        """ë°°ì¹˜ ë‹¨ìœ„ CoT ìƒì„±"""
        all_results = []
        
        for i in tqdm(range(0, len(prompts), batch_size), desc="ğŸ¤– CoT ìƒì„±"):
            batch = prompts[i:i+batch_size]
            outputs = self.llm.generate(batch, self.sampling_params)
            
            for output in outputs:
                generated = output.outputs[0].text.strip()
                all_results.append(generated)
        
        return all_results
    
    def augment_singleturn(self, data_items: list[dict]) -> list[dict]:
        """ì‹±ê¸€í„´ ë°ì´í„° ì¦ê°•"""
        print("\nğŸ“ ì‹±ê¸€í„´ CoT ìƒì„± ì¤‘...")
        
        prompts = []
        valid_items = []
        
        for item in data_items:
            qa = self.extract_qa_pairs(item)
            if qa and not qa['is_multiturn']:
                prompt = self.create_singleturn_prompt(
                    qa['last_question'],
                    qa['last_answer']
                )
                prompts.append(prompt)
                valid_items.append(qa)
        
        print(f"âœ… {len(prompts):,}ê°œ ì‹±ê¸€í„´ í”„ë¡¬í”„íŠ¸ ìƒì„±ë¨")
        
        if not prompts:
            print("âš ï¸  ì‹±ê¸€í„´ ë°ì´í„° ì—†ìŒ")
            return []
        
        cot_results = self.generate_cot_batch(prompts, batch_size=128)
        
        augmented = []
        for qa, cot_text in zip(valid_items, cot_results):
            augmented.append({
                "messages": [
                    {
                        "role": "system",
                        "content": "ë‹¹ì‹ ì€ ë‹¨ê³„ë³„ë¡œ ì‚¬ê³ í•˜ëŠ” í•œêµ­ì–´ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."
                    },
                    {
                        "role": "user",
                        "content": qa['last_question']
                    },
                    {
                        "role": "assistant",
                        "content": cot_text
                    }
                ],
                "type": "singleturn",
                "augmented_by": "EXAONE-4.0-1.2B"
            })
        
        return augmented
    
    def augment_multiturn(self, data_items: list[dict]) -> list[dict]:
        """ë©€í‹°í„´ ë°ì´í„° ì¦ê°•"""
        print("\nğŸ’¬ ë©€í‹°í„´ CoT ìƒì„± ì¤‘...")
        
        prompts = []
        valid_items = []
        
        for item in data_items:
            qa = self.extract_qa_pairs(item)
            if qa and qa['is_multiturn']:
                prompt = self.create_multiturn_prompt(qa['messages'])
                prompts.append(prompt)
                valid_items.append(qa)
        
        print(f"âœ… {len(prompts):,}ê°œ ë©€í‹°í„´ í”„ë¡¬í”„íŠ¸ ìƒì„±ë¨")
        
        if not prompts:
            print("âš ï¸  ë©€í‹°í„´ ë°ì´í„° ì—†ìŒ")
            return []
        
        cot_results = self.generate_cot_batch(prompts, batch_size=64)
        
        augmented = []
        for qa, cot_text in zip(valid_items, cot_results):
            # ì›ë³¸ ëŒ€í™”ì—ì„œ ë§ˆì§€ë§‰ ë‹µë³€ë§Œ CoTë¡œ êµì²´
            new_messages = [
                {
                    "role": "system",
                    "content": "ë‹¹ì‹ ì€ ë‹¨ê³„ë³„ë¡œ ì‚¬ê³ í•˜ëŠ” í•œêµ­ì–´ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."
                }
            ] + qa['messages'][:-1] + [
                {
                    "role": "assistant",
                    "content": cot_text
                }
            ]
            
            augmented.append({
                "messages": new_messages,
                "type": "multiturn",
                "augmented_by": "EXAONE-4.0-1.2B"
            })
        
        return augmented
    
    def process_dataset(
        self, 
        input_jsonl: str, 
        output_dir: str, 
        sample_ratio: float = 0.05, 
        singleturn_ratio: float = 0.7
    ) -> list[dict]:
        """ì „ì²´ ë°ì´í„°ì…‹ ì²˜ë¦¬"""
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        dataset_name = Path(input_jsonl).stem
        
        # ìƒ˜í”Œë§
        sampled_data = self.sample_dataset(input_jsonl, sample_ratio)
        
        # ì‹±ê¸€í„´/ë©€í‹°í„´ ë¶„í• 
        random.shuffle(sampled_data)
        split_idx = int(len(sampled_data) * singleturn_ratio)
        
        singleturn_data = sampled_data[:split_idx]
        multiturn_data = sampled_data[split_idx:]
        
        print(f"\nğŸ“Š ë°ì´í„° ë¶„í¬:")
        print(f"   ì‹±ê¸€í„´: {len(singleturn_data):,}ê°œ ({singleturn_ratio*100}%)")
        print(f"   ë©€í‹°í„´: {len(multiturn_data):,}ê°œ ({(1-singleturn_ratio)*100}%)")
        
        results = []
        
        # ì‹±ê¸€í„´ ì²˜ë¦¬
        if singleturn_data:
            singleturn_results = self.augment_singleturn(singleturn_data)
            results.extend(singleturn_results)
        
        # ë©€í‹°í„´ ì²˜ë¦¬
        if multiturn_data:
            multiturn_results = self.augment_multiturn(multiturn_data)
            results.extend(multiturn_results)
        
        # ì €ì¥
        output_file = output_dir / f"{dataset_name}_cot_augmented.jsonl"
        with open(output_file, 'w', encoding='utf-8') as f:
            for item in results:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
        
        print(f"\nâœ… ì™„ë£Œ! {len(results):,}ê°œ ìƒ˜í”Œ â†’ {output_file}")
        
        return results


def main():
    parser = argparse.ArgumentParser(description="EXAONE CoT ë°ì´í„° ì¦ê°•")
    parser.add_argument(
        '--input-dir', 
        type=str, 
        default='../../korean_large_data/cleaned_jsonl',
        help='ì…ë ¥ JSONL ë””ë ‰í† ë¦¬'
    )
    parser.add_argument(
        '--output-dir', 
        type=str, 
        default='phase2_thinking_exaone',
        help='ì¶œë ¥ ë””ë ‰í† ë¦¬'
    )
    parser.add_argument(
        '--sample-ratio', 
        type=float, 
        default=0.05,
        help='ìƒ˜í”Œë§ ë¹„ìœ¨ (0.05 = 5%)'
    )
    parser.add_argument(
        '--singleturn-ratio', 
        type=float, 
        default=0.7,
        help='ì‹±ê¸€í„´ ë¹„ìœ¨ (0.7 = 70%)'
    )
    parser.add_argument(
        '--datasets',
        type=str,
        nargs='+',
        default=None,
        help='ì²˜ë¦¬í•  ë°ì´í„°ì…‹ ëª©ë¡ (ê¸°ë³¸: ì „ì²´)'
    )
    
    args = parser.parse_args()
    
    # ê¸°ë³¸ ë°ì´í„°ì…‹ ëª©ë¡
    if args.datasets is None:
        datasets = [
            'orca_math_ko_data.jsonl',
            'kullm_v2_full_data.jsonl',
            'smol_koreantalk_data.jsonl',
            'won_instruct_data.jsonl',
            'koalpaca_data.jsonl',
            'kovicuna_data.jsonl',
            'kowiki_qa_data.jsonl',
            'kullm_v2_data.jsonl',
            'ko_evol_writing_data.jsonl',
        ]
    else:
        datasets = args.datasets
    
    # EXAONE ëª¨ë¸ ë¡œë“œ
    augmenter = CoTDataAugmenter()
    
    all_results = []
    
    # ê° ë°ì´í„°ì…‹ ì²˜ë¦¬
    for dataset_name in datasets:
        dataset_path = Path(args.input_dir) / dataset_name
        
        if not dataset_path.exists():
            print(f"âš ï¸  íŒŒì¼ ì—†ìŒ: {dataset_path}")
            continue
        
        print(f"\n{'='*80}")
        print(f"ğŸ“¦ ì²˜ë¦¬ ì¤‘: {dataset_name}")
        print(f"{'='*80}")
        
        try:
            results = augmenter.process_dataset(
                input_jsonl=str(dataset_path),
                output_dir=args.output_dir,
                sample_ratio=args.sample_ratio,
                singleturn_ratio=args.singleturn_ratio
            )
            all_results.extend(results)
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue
    
    # ì „ì²´ ê²°ê³¼ ì €ì¥
    if all_results:
        combined_file = Path(args.output_dir) / 'all_cot_augmented.jsonl'
        with open(combined_file, 'w', encoding='utf-8') as f:
            for item in all_results:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
        
        print(f"\n{'='*80}")
        print(f"ğŸ‰ ì „ì²´ ì™„ë£Œ!")
        print(f"   ì´ ìƒ˜í”Œ: {len(all_results):,}ê°œ")
        print(f"   í†µí•© íŒŒì¼: {combined_file}")
        print(f"{'='*80}")


if __name__ == "__main__":
    main()

