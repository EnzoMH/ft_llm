#!/usr/bin/env python3
"""
Phase 2: Qwen3-VL Thinking ëŠ¥ë ¥ íŒŒì¸íŠœë‹
- CoT ì¶”ë¡  ëŠ¥ë ¥ ì¶”ê°€
- <think> íƒœê·¸ í™œìš©
- Phase 1 ëª¨ë¸ ë˜ëŠ” Qwen3-VL-8B-Thinking ê¸°ë°˜
"""

import os
import sys
import torch
import argparse
from pathlib import Path
from datetime import datetime

from unsloth import FastLanguageModel, is_bfloat16_supported
from datasets import load_dataset
from trl import SFTTrainer
from transformers import TrainingArguments


def setup_model(
    model_name: str,
    max_seq_length: int = 8192,
    load_in_4bit: bool = True,
    lora_r: int = 32,
    lora_alpha: int = 64,
):
    """ëª¨ë¸ ë° LoRA ì„¤ì • (Thinkingìš© ë” í° rank)"""
    
    print("ğŸš€ ëª¨ë¸ ë¡œë”© ì¤‘...")
    print(f"   ëª¨ë¸: {model_name}")
    print(f"   ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´: {max_seq_length} (CoTìš© í™•ì¥)")
    print(f"   4bit ì–‘ìí™”: {load_in_4bit}")
    
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=model_name,
        max_seq_length=max_seq_length,
        load_in_4bit=load_in_4bit,
        dtype=None,
        trust_remote_code=True,
    )
    
    print("\nğŸ”§ LoRA ì ìš© ì¤‘ (Thinkingìš© í™•ì¥)...")
    print(f"   Rank: {lora_r} (Thinkingì€ ë” í° rank í•„ìš”)")
    print(f"   Alpha: {lora_alpha}")
    
    model = FastLanguageModel.get_peft_model(
        model,
        r=lora_r,
        target_modules=[
            "q_proj", "k_proj", "v_proj", "o_proj",
            "gate_proj", "up_proj", "down_proj"
        ],
        lora_alpha=lora_alpha,
        lora_dropout=0.05,
        bias="none",
        use_gradient_checkpointing="unsloth",
        random_state=42,
        use_rslora=False,
        loftq_config=None,
    )
    
    print("âœ… ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ!\n")
    
    return model, tokenizer


def load_training_data(data_dir: str = "phase2_thinking_exaone"):
    """CoT í•™ìŠµ ë°ì´í„° ë¡œë“œ"""
    
    data_dir = Path(data_dir)
    
    print(f"ğŸ“– CoT í•™ìŠµ ë°ì´í„° ë¡œë”©...")
    print(f"   ë””ë ‰í† ë¦¬: {data_dir}")
    
    # JSONL íŒŒì¼ ì°¾ê¸°
    jsonl_files = list(data_dir.glob("*_cot_augmented.jsonl"))
    
    if not jsonl_files:
        # all_cot_augmented.jsonl ì°¾ê¸°
        jsonl_files = list(data_dir.glob("all_cot_augmented.jsonl"))
    
    if not jsonl_files:
        raise FileNotFoundError(f"CoT JSONL íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_dir}")
    
    print(f"   ë°œê²¬ëœ íŒŒì¼: {len(jsonl_files)}ê°œ")
    for f in jsonl_files:
        print(f"      - {f.name}")
    
    # ë°ì´í„°ì…‹ ë¡œë“œ
    dataset = load_dataset(
        'json',
        data_files=[str(f) for f in jsonl_files],
        split='train'
    )
    
    print(f"\nâœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ!")
    print(f"   ì´ ìƒ˜í”Œ: {len(dataset):,}ê°œ")
    
    # íƒ€ì…ë³„ ë¶„í¬ í™•ì¸
    if 'type' in dataset.column_names:
        types = dataset['type']
        from collections import Counter
        type_counts = Counter(types)
        
        print(f"\nğŸ“Š ë°ì´í„° íƒ€ì… ë¶„í¬:")
        for dtype, count in type_counts.items():
            percentage = (count / len(dataset)) * 100
            print(f"   {dtype}: {count:,}ê°œ ({percentage:.1f}%)")
    
    # ìƒ˜í”Œ í™•ì¸
    if len(dataset) > 0:
        sample = dataset[0]
        print(f"\nğŸ” ë°ì´í„° êµ¬ì¡°:")
        print(f"   í‚¤: {list(sample.keys())}")
        if 'messages' in sample:
            print(f"   ë©”ì‹œì§€ ìˆ˜: {len(sample['messages'])}")
            
            # <think> íƒœê·¸ í™•ì¸
            for msg in sample['messages']:
                if msg['role'] == 'assistant':
                    has_think = '<think>' in msg['content']
                    print(f"   <think> íƒœê·¸: {'âœ… í¬í•¨' if has_think else 'âŒ ì—†ìŒ'}")
                    break
    
    return dataset


def format_chat_template(example, tokenizer):
    """ì±„íŒ… í…œí”Œë¦¿ í¬ë§·íŒ…"""
    messages = example['messages']
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=False
    )
    return {"text": text}


def train_phase2(
    model_name: str,
    data_dir: str = "phase2_thinking_exaone",
    output_dir: str = "qwen3-vl-8b-korean-thinking",
    max_seq_length: int = 8192,
    num_train_epochs: int = 3,
    per_device_train_batch_size: int = 2,
    gradient_accumulation_steps: int = 8,
    learning_rate: float = 1e-5,
    warmup_steps: int = 200,
    logging_steps: int = 10,
    save_steps: int = 500,
    lora_r: int = 32,
    lora_alpha: int = 64,
):
    """Phase 2 í•™ìŠµ ì‹¤í–‰"""
    
    print("="*80)
    print("Phase 2: Qwen3-VL Thinking ëŠ¥ë ¥ íŒŒì¸íŠœë‹")
    print("="*80)
    print(f"ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    # ëª¨ë¸ ì„¤ì •
    model, tokenizer = setup_model(
        model_name=model_name,
        max_seq_length=max_seq_length,
        load_in_4bit=True,
        lora_r=lora_r,
        lora_alpha=lora_alpha,
    )
    
    # ë°ì´í„° ë¡œë“œ
    dataset = load_training_data(data_dir)
    
    # ë°ì´í„° í¬ë§·íŒ…
    print("\nğŸ”„ ë°ì´í„° í¬ë§·íŒ… ì¤‘...")
    dataset = dataset.map(
        lambda x: format_chat_template(x, tokenizer),
        remove_columns=dataset.column_names
    )
    print("âœ… í¬ë§·íŒ… ì™„ë£Œ!\n")
    
    # í•™ìŠµ ì„¤ì •
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("âš™ï¸  í•™ìŠµ ì„¤ì • (CoT íŠ¹í™”):")
    print(f"   ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
    print(f"   Epochs: {num_train_epochs} (CoTëŠ” ë” ë§ì€ epoch í•„ìš”)")
    print(f"   ë°°ì¹˜ í¬ê¸°: {per_device_train_batch_size} (ê¸´ ì‹œí€€ìŠ¤ë¡œ ì‘ê²Œ)")
    print(f"   Gradient Accumulation: {gradient_accumulation_steps}")
    print(f"   íš¨ê³¼ì  ë°°ì¹˜ í¬ê¸°: {per_device_train_batch_size * gradient_accumulation_steps}")
    print(f"   í•™ìŠµë¥ : {learning_rate} (ë” ì‘ì€ LR)")
    print(f"   Warmup Steps: {warmup_steps}")
    print()
    
    training_args = TrainingArguments(
        output_dir=str(output_dir),
        per_device_train_batch_size=per_device_train_batch_size,
        gradient_accumulation_steps=gradient_accumulation_steps,
        warmup_steps=warmup_steps,
        num_train_epochs=num_train_epochs,
        learning_rate=learning_rate,
        fp16=not is_bfloat16_supported(),
        bf16=is_bfloat16_supported(),
        logging_steps=logging_steps,
        logging_dir=str(output_dir / "logs"),
        optim="adamw_8bit",
        weight_decay=0.01,
        lr_scheduler_type="cosine",
        save_strategy="steps",
        save_steps=save_steps,
        save_total_limit=3,
        report_to=["tensorboard"],
        seed=42,
        max_grad_norm=1.0,  # CoT í•™ìŠµ ì•ˆì •í™”
    )
    
    # Trainer ìƒì„±
    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=dataset,
        dataset_text_field="text",
        max_seq_length=max_seq_length,
        args=training_args,
        packing=False,
    )
    
    # í•™ìŠµ ì‹œì‘
    print("ğŸš€ í•™ìŠµ ì‹œì‘! (CoT íŒ¨í„´ í•™ìŠµ)")
    print("="*80)
    
    trainer.train()
    
    print("\n" + "="*80)
    print("âœ… í•™ìŠµ ì™„ë£Œ!")
    print("="*80)
    
    # ëª¨ë¸ ì €ì¥
    print(f"\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘: {output_dir}")
    model.save_pretrained(str(output_dir))
    tokenizer.save_pretrained(str(output_dir))
    
    # LoRA ì–´ëŒ‘í„° ë³‘í•© (ì„ íƒì )
    print("\nğŸ’¾ LoRA ì–´ëŒ‘í„° ë³‘í•© ì¤‘...")
    model.save_pretrained_merged(
        str(output_dir / "merged"),
        tokenizer,
        save_method="merged_16bit",
    )
    
    print(f"\nâœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    print(f"   ëª¨ë¸ ìœ„ì¹˜: {output_dir}")
    print(f"   ë³‘í•© ëª¨ë¸: {output_dir / 'merged'}")
    print(f"   ì¢…ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    print(f"\nğŸ’¡ í…ŒìŠ¤íŠ¸ ë°©ë²•:")
    print(f"   ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— '/think' ì¶”ê°€")
    print(f"   ë˜ëŠ” enable_thinking=True ì‚¬ìš©")


def main():
    parser = argparse.ArgumentParser(description="Phase 2 í•™ìŠµ")
    
    parser.add_argument(
        '--model-name',
        type=str,
        required=True,
        help='ê¸°ë³¸ ëª¨ë¸ (Phase 1 ê²°ê³¼ ë˜ëŠ” Qwen3-VL-8B-Thinking)'
    )
    parser.add_argument(
        '--data-dir',
        type=str,
        default="phase2_thinking_exaone",
        help='CoT í•™ìŠµ ë°ì´í„° ë””ë ‰í† ë¦¬'
    )
    parser.add_argument(
        '--output-dir',
        type=str,
        default="qwen3-vl-8b-korean-thinking",
        help='ì¶œë ¥ ë””ë ‰í† ë¦¬'
    )
    parser.add_argument(
        '--max-seq-length',
        type=int,
        default=8192,
        help='ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ (CoTìš© í™•ì¥)'
    )
    parser.add_argument(
        '--epochs',
        type=int,
        default=3,
        help='í•™ìŠµ ì—í­ ìˆ˜'
    )
    parser.add_argument(
        '--batch-size',
        type=int,
        default=2,
        help='ë°°ì¹˜ í¬ê¸°'
    )
    parser.add_argument(
        '--gradient-accumulation',
        type=int,
        default=8,
        help='Gradient Accumulation Steps'
    )
    parser.add_argument(
        '--learning-rate',
        type=float,
        default=1e-5,
        help='í•™ìŠµë¥ '
    )
    parser.add_argument(
        '--lora-r',
        type=int,
        default=32,
        help='LoRA rank (Thinkingì€ ë” í° rank ê¶Œì¥)'
    )
    parser.add_argument(
        '--lora-alpha',
        type=int,
        default=64,
        help='LoRA alpha'
    )
    
    args = parser.parse_args()
    
    train_phase2(
        model_name=args.model_name,
        data_dir=args.data_dir,
        output_dir=args.output_dir,
        max_seq_length=args.max_seq_length,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.batch_size,
        gradient_accumulation_steps=args.gradient_accumulation,
        learning_rate=args.learning_rate,
        lora_r=args.lora_r,
        lora_alpha=args.lora_alpha,
    )


if __name__ == "__main__":
    main()

