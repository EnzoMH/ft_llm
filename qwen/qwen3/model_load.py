#!/usr/bin/env python3
"""
Qwen3-VL ëª¨ë¸ ë¡œë“œ ë° ì¶”ë¡  í…ŒìŠ¤íŠ¸
- Phase 1/2 ëª¨ë¸ í…ŒìŠ¤íŠ¸
- Thinking ëª¨ë“œ í™œì„±í™”
- ë°°ì¹˜ ì¶”ë¡  ì§€ì›
"""

import argparse
import torch
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer


def load_model(model_path: str, device: str = "auto"):
    """ëª¨ë¸ ë¡œë“œ"""
    
    print(f"ğŸš€ ëª¨ë¸ ë¡œë”©: {model_path}")
    
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        device_map=device,
        torch_dtype=torch.bfloat16,
        trust_remote_code=True,
    )
    
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        trust_remote_code=True
    )
    
    print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!\n")
    
    return model, tokenizer


def generate_response(
    model,
    tokenizer,
    prompt: str,
    enable_thinking: bool = False,
    max_new_tokens: int = 1024,
    temperature: float = 0.7,
    top_p: float = 0.9,
):
    """ì‘ë‹µ ìƒì„±"""
    
    # ë©”ì‹œì§€ êµ¬ì„±
    messages = [
        {
            "role": "system",
            "content": "/think" if enable_thinking else "ë‹¹ì‹ ì€ í•œêµ­ì–´ ì „ìš© AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."
        },
        {
            "role": "user",
            "content": prompt
        }
    ]
    
    # í† í¬ë‚˜ì´ì§•
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    inputs = tokenizer(text, return_tensors="pt").to(model.device)
    
    # ìƒì„±
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            top_p=top_p,
            do_sample=True,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    
    # ë””ì½”ë”©
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # ì…ë ¥ ì œê±°í•˜ê³  ì‘ë‹µë§Œ ì¶”ì¶œ
    response = response.split("assistant\n")[-1] if "assistant\n" in response else response
    
    return response


def interactive_mode(model, tokenizer, enable_thinking: bool = False):
    """ëŒ€í™”í˜• ëª¨ë“œ"""
    
    print("\n" + "="*80)
    print("ëŒ€í™”í˜• ëª¨ë“œ ì‹œì‘")
    print("="*80)
    print(f"Thinking ëª¨ë“œ: {'âœ… í™œì„±í™”' if enable_thinking else 'âŒ ë¹„í™œì„±í™”'}")
    print("ì¢…ë£Œ: 'quit' ë˜ëŠ” 'exit' ì…ë ¥")
    print("Thinking ì „í™˜: '/think' ë˜ëŠ” '/no_think' ì…ë ¥")
    print("="*80 + "\n")
    
    thinking_enabled = enable_thinking
    
    while True:
        try:
            user_input = input("ì‚¬ìš©ì: ").strip()
            
            if not user_input:
                continue
            
            if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
                print("\nğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            if user_input == '/think':
                thinking_enabled = True
                print("ğŸ’­ Thinking ëª¨ë“œ í™œì„±í™”\n")
                continue
            
            if user_input == '/no_think':
                thinking_enabled = False
                print("ğŸ’¬ ì¼ë°˜ ëª¨ë“œ í™œì„±í™”\n")
                continue
            
            print("\nì–´ì‹œìŠ¤í„´íŠ¸: ", end="", flush=True)
            
            response = generate_response(
                model=model,
                tokenizer=tokenizer,
                prompt=user_input,
                enable_thinking=thinking_enabled,
            )
            
            print(response)
            print()
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}\n")


def test_model(model_path: str, interactive: bool = False, enable_thinking: bool = False):
    """ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    
    # ëª¨ë¸ ë¡œë“œ
    model, tokenizer = load_model(model_path)
    
    if interactive:
        # ëŒ€í™”í˜• ëª¨ë“œ
        interactive_mode(model, tokenizer, enable_thinking)
    else:
        # í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
        test_prompts = [
            "AGVê°€ ì•ì— ì¥ì• ë¬¼ì´ ìˆì„ ë•Œ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?",
            "3x + 5 = 14 ë¥¼ í’€ì–´ì£¼ì„¸ìš”.",
            "í•œêµ­ì–´ë¡œ íŒŒì¸íŠœë‹ì´ ì˜ ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤. ìê¸°ì†Œê°œë¥¼ í•´ì£¼ì„¸ìš”.",
        ]
        
        print(f"ğŸ§ª í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ({len(test_prompts)}ê°œ)")
        print(f"Thinking ëª¨ë“œ: {'âœ… í™œì„±í™”' if enable_thinking else 'âŒ ë¹„í™œì„±í™”'}")
        print("="*80 + "\n")
        
        for i, prompt in enumerate(test_prompts, 1):
            print(f"[í…ŒìŠ¤íŠ¸ {i}/{len(test_prompts)}]")
            print(f"ì§ˆë¬¸: {prompt}\n")
            
            response = generate_response(
                model=model,
                tokenizer=tokenizer,
                prompt=prompt,
                enable_thinking=enable_thinking,
            )
            
            print(f"ì‘ë‹µ:\n{response}\n")
            print("="*80 + "\n")


def main():
    parser = argparse.ArgumentParser(description="Qwen3-VL ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    
    parser.add_argument(
        'model_path',
        type=str,
        help='ëª¨ë¸ ê²½ë¡œ (Phase 1/2 ê²°ê³¼ ë˜ëŠ” HF ëª¨ë¸)'
    )
    parser.add_argument(
        '--interactive',
        action='store_true',
        help='ëŒ€í™”í˜• ëª¨ë“œ'
    )
    parser.add_argument(
        '--thinking',
        action='store_true',
        help='Thinking ëª¨ë“œ í™œì„±í™”'
    )
    parser.add_argument(
        '--device',
        type=str,
        default='auto',
        help='ë””ë°”ì´ìŠ¤ (auto, cuda, cpu)'
    )
    
    args = parser.parse_args()
    
    test_model(
        model_path=args.model_path,
        interactive=args.interactive,
        enable_thinking=args.thinking,
    )


if __name__ == "__main__":
    main()

