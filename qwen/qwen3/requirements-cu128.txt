# Qwen3-VL 한국어 VLA 파인튜닝 - CUDA 12.8 최적화
# H100 GPU + Flash-Attention 3 환경

# ============================================================================
# 코어 딥러닝 프레임워크 (CUDA 12.8 최적화)
# ============================================================================

# PyTorch (CUDA 12.8)
--extra-index-url https://download.pytorch.org/whl/cu128
torch>=2.8.0
torchvision>=0.19.0
torchaudio>=2.5.0

# Transformers 생태계 (최신 버전)
transformers>=4.56.0,<4.58.0
tokenizers>=0.22.0
accelerate>=1.5.0
datasets>=3.2.0

# ============================================================================
# 파인튜닝 최적화
# ============================================================================

# Unsloth (H100 최적화)
unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git

# PEFT & LoRA
peft>=0.14.0
trl>=0.24.0

# 양자화
bitsandbytes>=0.45.0

# ============================================================================
# Flash-Attention 3 (H100 Hopper 최적화) ⭐
# ============================================================================
# Flash-Attention 3는 H100에서 1.5~2배 성능 향상
# Beta 버전이므로 안정성 확인 필요

# Option 1: Flash-Attention 3 (권장 - H100 최적)
# flash-attn-3 @ git+https://github.com/Dao-AILab/flash-attention-3.git

# Option 2: Flash-Attention 2 (안정적 대안)
flash-attn>=2.7.0

# FlashInfer (vLLM용 - H100 최적화)
flashinfer>=0.2.0

# ============================================================================
# vLLM (CoT 데이터 증강용)
# ============================================================================

# vLLM (CUDA 12.8 지원)
vllm>=0.8.0

# ============================================================================
# 추가 최적화 라이브러리
# ============================================================================

# 텐서 최적화
triton>=3.2.0

# 분산 학습
deepspeed>=0.15.0

# Sentence Transformers
sentence-transformers>=2.8.0

# ============================================================================
# 유틸리티
# ============================================================================

# 데이터 처리
numpy>=1.26.0
pandas>=2.2.0
scikit-learn>=1.5.0

# Progress bar
tqdm>=4.67.0

# 환경 관리
python-dotenv>=1.0.0

# ============================================================================
# 모니터링 & 로깅
# ============================================================================

# TensorBoard
tensorboard>=2.18.0

# Weights & Biases (선택적)
# wandb>=0.18.0

# ============================================================================
# 개발 도구
# ============================================================================

# 코드 품질
# black>=24.0.0
# flake8>=7.0.0
# mypy>=1.10.0

# Jupyter (선택적)
# jupyter>=1.1.0
# ipywidgets>=8.1.0

# ============================================================================
# 버전 호환성 참고
# ============================================================================
# PyTorch 2.8.0+cu128:
#   - CUDA 12.8 완벽 지원
#   - Flash-Attention 3 호환
#   - H100 Tensor Core 최적화
#   - TF32, BF16 완전 지원
#
# Flash-Attention 3 vs 2:
#   - FA3: H100 특화, 1.5~2배 빠름, Beta 단계
#   - FA2: 안정적, 모든 GPU 지원
#
# vLLM 0.8.0+:
#   - FlashInfer 백엔드 지원
#   - CUDA 12.8 최적화
#   - H100 최대 성능

