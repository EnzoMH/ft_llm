# Qwen3-VL í•œêµ­ì–´ VLA íŒŒì¸íŠœë‹

> AGV/AMRìš© í•œêµ­ì–´ Vision-Language-Action ëª¨ë¸ ê°œë°œ
> í™˜ê²½: H100 80GB GPU
> ì¦ê°• ëª¨ë¸: EXAONE 4.0 1.2B

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
qwen3/
â”œâ”€â”€ augment_cot_exaone.py           # CoT ë°ì´í„° ì¦ê°• (EXAONE)
â”œâ”€â”€ prepare_phase1_data.py          # Phase 1 ë°ì´í„° ì¤€ë¹„
â”œâ”€â”€ validate_cot_quality.py         # CoT í’ˆì§ˆ ê²€ì¦
â”œâ”€â”€ train_phase1_korean_instruct.py # Phase 1 í•™ìŠµ
â”œâ”€â”€ train_phase2_thinking.py        # Phase 2 í•™ìŠµ
â”œâ”€â”€ model_load.py                   # ëª¨ë¸ í…ŒìŠ¤íŠ¸/ì¶”ë¡ 
â”œâ”€â”€ run_all.sh                      # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
â”œâ”€â”€ guied.md                        # ìƒì„¸ ê°€ì´ë“œ ë¬¸ì„œ
â””â”€â”€ README.md                       # ì´ íŒŒì¼
```

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. í™˜ê²½ ì„¤ì •

```bash
# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install torch transformers datasets trl accelerate
pip install unsloth  # íŒŒì¸íŠœë‹ ìµœì í™”
pip install vllm     # CoT ì¦ê°•ìš©

# GPU í™•ì¸
nvidia-smi
python --version  # Python 3.10+ í•„ìš”
```

### 2. ë°ì´í„° ì¤€ë¹„

```bash
# ë°ì´í„° ìœ„ì¹˜ í™•ì¸
ls ../../korean_large_data/cleaned_jsonl/

# ì´ 681K ìƒ˜í”Œ:
# - orca_math_ko_data.jsonl (192K)
# - kullm_v2_full_data.jsonl (147K)
# - smol_koreantalk_data.jsonl (89K)
# ë“±...
```

### 3. ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

```bash
# ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬
chmod +x run_all.sh

# ì „ì²´ ì‹¤í–‰ (ëŒ€í™”í˜•)
./run_all.sh

# ë˜ëŠ” ë‹¨ê³„ë³„ ì‹¤í–‰ (ì•„ë˜ ì°¸ì¡°)
```

## ğŸ“‹ ë‹¨ê³„ë³„ ì‹¤í–‰

### Step 1: Phase 1 ë°ì´í„° ì¤€ë¹„ (30ë¶„)

```bash
python prepare_phase1_data.py \
    --input-dir ../../korean_large_data/cleaned_jsonl \
    --output-dir phase1_korean \
    --target-samples 256000
```

**ì¶œë ¥:**
- `phase1_korean/phase1_korean_256000samples.jsonl`
- ì¤‘êµ­ì–´ í•„í„°ë§ ì™„ë£Œ
- í•œêµ­ì–´ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¶”ê°€

### Step 2: CoT ë°ì´í„° ì¦ê°• (1-2ì‹œê°„)

```bash
# 5% ìƒ˜í”Œë§ (í…ŒìŠ¤íŠ¸)
python augment_cot_exaone.py \
    --input-dir ../../korean_large_data/cleaned_jsonl \
    --output-dir phase2_thinking_exaone \
    --sample-ratio 0.05 \
    --singleturn-ratio 0.7

# ì „ì²´ ë°ì´í„° (ìŠ¹ì¸ í›„)
python augment_cot_exaone.py \
    --input-dir ../../korean_large_data/cleaned_jsonl \
    --output-dir phase2_thinking_full \
    --sample-ratio 1.0 \
    --singleturn-ratio 0.7
```

**ì²˜ë¦¬ ì‹œê°„ (H100 80GB):**
- 5% (28K ìƒ˜í”Œ): 1-2ì‹œê°„
- 100% (565K ìƒ˜í”Œ): 20-40ì‹œê°„

### Step 3: í’ˆì§ˆ ê²€ì¦ (10ë¶„)

```bash
# ë‹¨ì¼ íŒŒì¼ ê²€ì¦
python validate_cot_quality.py \
    phase2_thinking_exaone/orca_math_ko_data_cot_augmented.jsonl \
    --num-samples 20 \
    --show-full

# ë””ë ‰í† ë¦¬ ì „ì²´ ê²€ì¦
python validate_cot_quality.py \
    phase2_thinking_exaone \
    --directory \
    --num-samples 20
```

**í™•ì¸ í•­ëª©:**
- âœ… `<think>` íƒœê·¸ í¬í•¨ ì—¬ë¶€
- âœ… ì¤‘êµ­ì–´ í¬í•¨ ì—¬ë¶€
- âœ… ë‹¨ê³„ë³„ êµ¬ë¶„ (1ë‹¨ê³„:, 2ë‹¨ê³„:)
- âœ… ë‹µë³€ ê¸¸ì´

### Step 4: Phase 1 í•™ìŠµ (4-6ì‹œê°„)

```bash
python train_phase1_korean_instruct.py \
    --model-name Qwen/Qwen3-VL-8B-Instruct \
    --data-dir phase1_korean \
    --output-dir qwen3-vl-8b-korean-instruct \
    --epochs 2 \
    --batch-size 4 \
    --gradient-accumulation 4 \
    --learning-rate 2e-5 \
    --lora-r 16 \
    --lora-alpha 32
```

**ì¶œë ¥:**
- `qwen3-vl-8b-korean-instruct/`: LoRA ì–´ëŒ‘í„°
- `qwen3-vl-8b-korean-instruct/merged/`: ë³‘í•© ëª¨ë¸

### Step 5: Phase 2 í•™ìŠµ (6-8ì‹œê°„)

```bash
# ì˜µì…˜ 1: Phase 1 ê²°ê³¼ ì‚¬ìš© (ê¶Œì¥)
python train_phase2_thinking.py \
    --model-name qwen3-vl-8b-korean-instruct/merged \
    --data-dir phase2_thinking_exaone \
    --output-dir qwen3-vl-8b-korean-thinking \
    --epochs 3 \
    --batch-size 2 \
    --gradient-accumulation 8 \
    --learning-rate 1e-5 \
    --lora-r 32 \
    --lora-alpha 64

# ì˜µì…˜ 2: Thinking ë² ì´ìŠ¤ ì‚¬ìš©
python train_phase2_thinking.py \
    --model-name Qwen/Qwen3-VL-8B-Thinking \
    --data-dir phase2_thinking_exaone \
    --output-dir qwen3-vl-8b-korean-thinking \
    --epochs 3 \
    --batch-size 2 \
    --gradient-accumulation 8
```

**ì¶œë ¥:**
- `qwen3-vl-8b-korean-thinking/`: LoRA ì–´ëŒ‘í„°
- `qwen3-vl-8b-korean-thinking/merged/`: ë³‘í•© ëª¨ë¸

### Step 6: ëª¨ë¸ í…ŒìŠ¤íŠ¸

```bash
# ìë™ í…ŒìŠ¤íŠ¸
python model_load.py qwen3-vl-8b-korean-thinking/merged

# Thinking ëª¨ë“œ í…ŒìŠ¤íŠ¸
python model_load.py qwen3-vl-8b-korean-thinking/merged --thinking

# ëŒ€í™”í˜• ëª¨ë“œ
python model_load.py qwen3-vl-8b-korean-thinking/merged --interactive --thinking
```

**ëŒ€í™”í˜• ëª…ë ¹ì–´:**
- `/think`: Thinking ëª¨ë“œ í™œì„±í™”
- `/no_think`: ì¼ë°˜ ëª¨ë“œ
- `quit` ë˜ëŠ” `exit`: ì¢…ë£Œ

## ğŸ“Š ë°ì´í„° êµ¬ì„±

### Phase 1: í•œêµ­ì–´ ê°•í™” (256K)

| ë°ì´í„°ì…‹ | ìƒ˜í”Œ ìˆ˜ | ìš©ë„ |
|---------|---------|------|
| kullm_v2_full | 147K | ì „ë¬¸ ì§€ì‹ |
| smol_koreantalk | 89K | ì¼ë°˜ ëŒ€í™” |
| won_instruct | 86K | ì „ë¬¸ ì§€ì‹ |
| ê¸°íƒ€ | ~34K | ë‹¤ì–‘í•œ ë„ë©”ì¸ |

### Phase 2: Thinking CoT (28K @ 5%)

| ë°ì´í„°ì…‹ | 5% ìƒ˜í”Œ | ìš©ë„ |
|---------|---------|------|
| orca_math_ko | 9,640 | ìˆ˜í•™ ì¶”ë¡  |
| kullm_v2_full | 7,348 | ì¼ë°˜ ì¶”ë¡  |
| smol_koreantalk | 4,438 | ëŒ€í™” ì¶”ë¡  |
| won_instruct | 4,300 | ì „ë¬¸ ì¶”ë¡  |
| ê¸°íƒ€ | ~2,531 | ë‹¤ì–‘í•œ ì¶”ë¡  |

## âš™ï¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°

### Phase 1 (í•œêµ­ì–´ ê°•í™”)

```python
max_seq_length = 4096        # ì¼ë°˜ ëŒ€í™” ê¸¸ì´
num_train_epochs = 2         # ì ì€ epochìœ¼ë¡œ ë¹ ë¥´ê²Œ
batch_size = 4               # GPU ë©”ëª¨ë¦¬ì— ë§ê²Œ
gradient_accumulation = 4    # íš¨ê³¼ì  ë°°ì¹˜ = 16
learning_rate = 2e-5         # í‘œì¤€ íŒŒì¸íŠœë‹ LR
lora_r = 16                  # ì ë‹¹í•œ rank
lora_alpha = 32              # rì˜ 2ë°°
```

### Phase 2 (Thinking)

```python
max_seq_length = 8192        # CoTëŠ” ê¸´ ì‹œí€€ìŠ¤ í•„ìš”
num_train_epochs = 3         # CoT íŒ¨í„´ í•™ìŠµì— ë” ë§ì€ epoch
batch_size = 2               # ê¸´ ì‹œí€€ìŠ¤ë¡œ ë°°ì¹˜ ì¤„ì„
gradient_accumulation = 8    # íš¨ê³¼ì  ë°°ì¹˜ = 16
learning_rate = 1e-5         # ë” ì‘ì€ LR (ì•ˆì •ì„±)
lora_r = 32                  # ë” í° rank (ë³µì¡í•œ íŒ¨í„´)
lora_alpha = 64              # rì˜ 2ë°°
```

## ğŸ”§ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### CUDA Out of Memory

```bash
# ë°°ì¹˜ ì‚¬ì´ì¦ˆ ê°ì†Œ
--batch-size 2
--gradient-accumulation 8

# ì‹œí€€ìŠ¤ ê¸¸ì´ ê°ì†Œ
--max-seq-length 4096  # (Phase 2)
```

### CoT í’ˆì§ˆ ë‚®ìŒ

1. **Temperature ì¡°ì •**
```bash
# augment_cot_exaone.py ìˆ˜ì •
temperature=0.7 â†’ 0.5  # ë” ê²°ì •ì 
```

2. **í”„ë¡¬í”„íŠ¸ ê°œì„ **
```python
# create_singleturn_prompt() ìˆ˜ì •
# ë” ëª…í™•í•œ ì§€ì‹œì‚¬í•­ ì¶”ê°€
```

3. **ëª¨ë¸ ì—…ê·¸ë ˆì´ë“œ**
```bash
# EXAONE 3B ì‹œë„
model_name="LGAI-EXAONE/EXAONE-4.0-3B-Instruct"
```

### í•™ìŠµ ë¶ˆì•ˆì •

```bash
# Learning rate ê°ì†Œ
--learning-rate 1e-5

# Warmup ì¦ê°€
--warmup-steps 500

# Gradient clipping
# training_argsì— max_grad_norm=1.0 ì¶”ê°€ë¨
```

### vLLM ì„¤ì¹˜ ì‹¤íŒ¨

```bash
# CUDA ë²„ì „ í™•ì¸
nvcc --version

# í˜¸í™˜ë˜ëŠ” ë²„ì „ ì„¤ì¹˜
pip install vllm==0.5.0  # CUDA 11.8
pip install vllm==0.6.0  # CUDA 12.1
```

## ğŸ“ˆ ì˜ˆìƒ ì†Œìš” ì‹œê°„ (H100 80GB)

| ë‹¨ê³„ | 5% ìƒ˜í”Œë§ | 100% (ì „ì²´) |
|-----|-----------|-------------|
| **Phase 1 ë°ì´í„° ì¤€ë¹„** | 30ë¶„ | 30ë¶„ |
| **CoT ë°ì´í„° ì¦ê°•** | 1-2ì‹œê°„ | 20-40ì‹œê°„ |
| **í’ˆì§ˆ ê²€ì¦** | 10ë¶„ | 30ë¶„ |
| **Phase 1 í•™ìŠµ** | 4-6ì‹œê°„ | 4-6ì‹œê°„ |
| **Phase 2 í•™ìŠµ** | 2-3ì‹œê°„ | 20-30ì‹œê°„ |
| **ì´ ì†Œìš” ì‹œê°„** | **8-12ì‹œê°„** | **45-107ì‹œê°„** |

## ğŸ’¾ ë””ìŠ¤í¬ ê³µê°„

- ì›ë³¸ ë°ì´í„°: ~1.4GB
- Phase 1 ë°ì´í„°: ~1GB
- Phase 2 ë°ì´í„° (5%): ~100MB
- Phase 2 ë°ì´í„° (100%): ~2GB
- ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸: ~20GB per phase
- **ì´ í•„ìš” ê³µê°„**: ~50GB (5% ê¸°ì¤€), ~100GB (100% ê¸°ì¤€)

## ğŸ¯ ì„±ëŠ¥ í‰ê°€

### í‰ê°€ í•­ëª©

1. **í•œêµ­ì–´ ëŠ¥ë ¥**
   - ì¤‘êµ­ì–´ ì¶œë ¥ ì—¬ë¶€
   - ë¬¸ë²• ì •í™•ì„±
   - ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„

2. **Thinking ëŠ¥ë ¥**
   - `<think>` íƒœê·¸ ì‚¬ìš©
   - ë‹¨ê³„ë³„ ì¶”ë¡ 
   - ìµœì¢… ë‹µë³€ ì •í™•ì„±

3. **ë„ë©”ì¸ íŠ¹í™”**
   - AGV/AMR ê´€ë ¨ ì§ˆì˜ ì‘ë‹µ
   - í•œì†ë¡œë´‡ ì œì–´ ëª…ë ¹

### í…ŒìŠ¤íŠ¸ ì˜ˆì‹œ

```bash
# ìˆ˜í•™ ì¶”ë¡ 
python model_load.py qwen3-vl-8b-korean-thinking/merged --thinking
> 3x + 5 = 14ë¥¼ í’€ì–´ì£¼ì„¸ìš”.

# AGV ì œì–´
python model_load.py qwen3-vl-8b-korean-thinking/merged --thinking
> AGVê°€ ì•ì— ì¥ì• ë¬¼ì´ ìˆì„ ë•Œ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?

# í•œêµ­ì–´ ê²€ì¦
python model_load.py qwen3-vl-8b-korean-thinking/merged
> ìê¸°ì†Œê°œë¥¼ í•´ì£¼ì„¸ìš”. (ì¤‘êµ­ì–´ ì¶œë ¥ ì—¬ë¶€ í™•ì¸)
```

## ğŸ“š ì°¸ê³  ìë£Œ

- [Qwen3 Technical Report](https://arxiv.org/abs/2505.09388)
- [Qwen3-VL GitHub](https://github.com/QwenLM/Qwen3-VL)
- [EXAONE 4.0](https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B-Instruct)
- [Unsloth Documentation](https://docs.unsloth.ai/)
- [ìƒì„¸ ê°€ì´ë“œ ë¬¸ì„œ](guied.md)

## ğŸ¤ ê¸°ì—¬

ì´ìŠˆ ë° ê°œì„  ì œì•ˆì€ GitHub Issuesì— ë“±ë¡í•´ì£¼ì„¸ìš”.

## ğŸ“ ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” ê° ëª¨ë¸ì˜ ë¼ì´ì„ ìŠ¤ë¥¼ ë”°ë¦…ë‹ˆë‹¤:
- Qwen3-VL: Apache 2.0
- EXAONE 4.0: Apache 2.0

---

**ì‘ì„±ì¼**: 2025-11-03  
**í™˜ê²½**: H100 80GB GPU  
**í”„ë ˆì„ì›Œí¬**: vLLM, Unsloth, Transformers

