# Korean LLM Fine-tuning & Evaluation Suite

**í•œêµ­ì–´ íŠ¹í™” ëŒ€ê·œëª¨ ì–¸ì–´ëª¨ë¸(LLM) íŒŒì¸íŠœë‹ ë° í‰ê°€ í†µí•© í”„ë¡œì íŠ¸**

ì´ í”„ë¡œì íŠ¸ëŠ” ë‹¤ì–‘í•œ ì˜¤í”ˆì†ŒìŠ¤ í•œêµ­ì–´ LLMì˜ íŒŒì¸íŠœë‹, í‰ê°€, ë²¤ì¹˜ë§ˆí‚¹ì„ ìœ„í•œ í†µí•© íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤.

## Supported Models

| Model | Status | Description |
|-------|--------|-------------|
| **EEVE-Korean-10.8B** | âœ… Complete | Instruction tuning, HuggingFace ë°°í¬ ì™„ë£Œ |
| **Qwen2.5** | ğŸ”„ In Progress | Unsupervised & Checkpoint training |
| **VCLM-Korean-7B** | âœ… Complete | Benchmarking & Evaluation |
| **SOLAR-10.7B** | âœ… Complete | Legacy project (archived) |

## Key Features

- ğŸš€ **ë‹¤ì¤‘ ëª¨ë¸ ì§€ì›**: EEVE, Qwen, VCLM, SOLAR
- ğŸ”§ **ìµœì í™”**: Unsloth, LoRA, 4-bit quantization
- ğŸ“Š **ë²¤ì¹˜ë§ˆí‚¹**: KoCoder, HumanEval í‰ê°€
- ğŸ—ƒï¸ **ë°ì´í„° ìƒì„±**: RAG ê¸°ë°˜ instruction ë°ì´í„° ìë™ ìƒì„±
- ğŸ” **ê²€ì¦ ë„êµ¬**: ë°ì´í„°ì…‹ í’ˆì§ˆ ê²€ì¦, ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§
- ğŸ’¾ **íš¨ìœ¨ì  í•™ìŠµ**: Gradient checkpointing, Mixed precision 

## Deployed Model

**HuggingFace**: [MyeongHo0621/eeve-vss-smh](https://huggingface.co/MyeongHo0621/eeve-vss-smh)

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(
    "MyeongHo0621/eeve-vss-smh",
    device_map="auto",
    torch_dtype="auto"
)
tokenizer = AutoTokenizer.from_pretrained("MyeongHo0621/eeve-vss-smh")
```

## Model Information

- **Base Model**: [yanolja/EEVE-Korean-Instruct-10.8B-v1.0](https://huggingface.co/yanolja/EEVE-Korean-Instruct-10.8B-v1.0)
- **How to fine-tune**: LoRA (r=128, alpha=256) + Unsloth
- **Data**: ê³ í’ˆì§ˆ í•œêµ­ì–´ instruction ë°ì´í„° (~100K ìƒ˜í”Œ)

## Train envrionment & configuration

### H/W info
- **GPU**: NVIDIA H100 80GB HBM3
- **CPU**: 24 cores
- **RAM**: 192GB
- **Framework**: Unsloth + PyTorch 2.8, Transformers 4.56.2

### LoRA configuration 
- **r**: 128 
- **alpha**: 256 (alpha = 2 * r)
- **dropout**: 0.0 (Only 0.0)
- **target_modules**: q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj
- **use_rslora**: false

### Training Hyper Parameter 
- **Framework**: Unsloth 
- **Epochs**: 3 
- **Batch Size**: 8 
- **Gradient Accumulation**: 2 
- **Learning Rate**: 1e-4
- **Max Sequence Length**: 4096 tokens
- **Warmup Ratio**: 0.05
- **Weight Decay**: 0.01

### Memory Optimization
- **Full Precision Training**
- **Unsloth Gradient Checkpointing**
- **BF16 Training**
- **Peak VRAM**

## Directory Structure

```
tesseract/
â”œâ”€â”€ eeve/                           # EEVE-Korean-10.8B Fine-tuning
â”‚   â”œâ”€â”€ 0_unsl_ft.py               # Main training script
â”‚   â”œâ”€â”€ 1_cp_ft.py                 # Checkpoint resume training
â”‚   â”œâ”€â”€ 2_merg_uplod.py            # Model merge & HuggingFace upload
â”‚   â”œâ”€â”€ 3_test_checkpoint.py       # Checkpoint testing
â”‚   â”œâ”€â”€ UNSLOTH_GUIDE.md           # Unsloth optimization guide
â”‚   â””â”€â”€ quant/                     # Quantization scripts
â”‚
â”œâ”€â”€ qwen/                           # Qwen2.5 Fine-tuning
â”‚   â”œâ”€â”€ 0_qwen_ft_us_cp.py         # Qwen training with checkpoint
â”‚   â”œâ”€â”€ util/                      # Training utilities
â”‚   â”‚   â”œâ”€â”€ cpu_mntrg.py           # CPU monitoring
â”‚   â”‚   â”œâ”€â”€ gpu_mnrtg.py           # GPU monitoring
â”‚   â”‚   â”œâ”€â”€ local_dataset_loader.py # Dataset loader
â”‚   â”‚   â””â”€â”€ monitoring_callback.py  # Training callback
â”‚   â””â”€â”€ 4_credential/              # Credentials (empty)
â”‚
â”œâ”€â”€ vclm/                           # VCLM-Korean-7B Evaluation
â”‚   â”œâ”€â”€ benchmark_vclm_kocoder.py  # KoCoder benchmarking
â”‚   â”œâ”€â”€ benchmark_kocoder_final.py # Final evaluation
â”‚   â””â”€â”€ .gitattributes             # LFS configuration
â”‚
â”œâ”€â”€ solar/                          # SOLAR-10.7B (Legacy)
â”‚   â””â”€â”€ ...                        # Archived fine-tuning scripts
â”‚
â”œâ”€â”€ datageneration/                 # Data Generation Pipeline
â”‚   â”œâ”€â”€ inst_eeve/                 # EEVE instruction data
â”‚   â”‚   â”œâ”€â”€ train_eeve_wms.py      # WMS training data
â”‚   â”‚   â”œâ”€â”€ train_eeve_wms_fp8.py  # FP8 training
â”‚   â”‚   â””â”€â”€ test_eeve_wms.py       # Testing
â”‚   â”œâ”€â”€ instruction/               # Instruction generation
â”‚   â”‚   â”œâ”€â”€ compare_all_models.py  # Model comparison
â”‚   â”‚   â””â”€â”€ convert_to_eeve.py     # Format conversion
â”‚   â””â”€â”€ valid/                     # Validation tools
â”‚       â”œâ”€â”€ validtest.py           # General validation
â”‚       â””â”€â”€ validate_qa_dataset.py # QA validation
â”‚
â”œâ”€â”€ eval_computing_resource/        # Resource Evaluation
â”‚   â””â”€â”€ eval.py                    # Computing resource profiling
â”‚
â”œâ”€â”€ faiss_storage/                  # Vector Database (gitignored)
â”‚   â””â”€â”€ ...                        # FAISS index for RAG
â”‚
â””â”€â”€ korean_large_data/              # Large Datasets (gitignored)
    â””â”€â”€ ...                        # Training datasets
```

---

## 1. EEVE-Korean-10.8B Fine-tuning

### Model Information

- **Base Model**: [yanolja/EEVE-Korean-Instruct-10.8B-v1.0](https://huggingface.co/yanolja/EEVE-Korean-Instruct-10.8B-v1.0)
- **Deployed Model**: [MyeongHo0621/eeve-vss-smh](https://huggingface.co/MyeongHo0621/eeve-vss-smh)
- **Status**: âœ… Training Complete & HuggingFace Deployment Complete
- **Vocab Size**: 40,960 (í•œì˜ balanced)
- **Context Length**: 8K tokens
- **Method**: LoRA (r=128, alpha=256) + Unsloth

### Training Configuration

#### Hardware
- **GPU**: NVIDIA H100 80GB HBM3
- **CPU**: 24 cores
- **RAM**: 192GB
- **Framework**: Unsloth + PyTorch 2.8, Transformers 4.56.2

#### LoRA Settings
- **r**: 128 
- **alpha**: 256 (alpha = 2 * r)
- **dropout**: 0.0
- **target_modules**: q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj
- **use_rslora**: false

#### Training Hyperparameters
- **Epochs**: 3 
- **Batch Size**: 8 
- **Gradient Accumulation**: 2 
- **Learning Rate**: 1e-4
- **Max Sequence Length**: 4096 tokens
- **Warmup Ratio**: 0.05
- **Weight Decay**: 0.01

#### Results
- **Training time**: ~3 hours (6,250 steps)
- **Peak VRAM**: ~26GB
- **Checkpoint Interval**: 250 steps

### How to Use

#### 1. HuggingFace (Recommended)

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# model load
model = AutoModelForCausalLM.from_pretrained(
    "MyeongHo0621/eeve-vss-smh",
    device_map="auto",
    torch_dtype="auto"
)
tokenizer = AutoTokenizer.from_pretrained("MyeongHo0621/eeve-vss-smh")

# prompt Template
def create_prompt(user_input):
    return f"""A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.
Human: {user_input}
Assistant: """

# generating response
prompt = create_prompt("í•œêµ­ì˜ ìˆ˜ë„ê°€ ì–´ë””ì•¼?")
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
outputs = model.generate(
    **inputs,
    max_new_tokens=512,
    temperature=0.3,
    top_p=0.85,
    do_sample=True
)
response = tokenizer.decode(outputs[0][len(inputs['input_ids'][0]):], skip_special_tokens=True)
print(response)
```

#### 2. Re-Training from Checkpoint

```bash
cd eeve

# Train from scratch
python 0_unsl_ft.py

# Resume from checkpoint
python 1_cp_ft.py

# Merge LoRA and upload to HuggingFace
python 2_merg_uplod.py

# Test checkpoints
python 3_test_checkpoint.py --compare \
  /path/to/checkpoint-1 \
  /path/to/checkpoint-2
```

#### 3. Model Load (Python API)

#### ê¸°ë³¸ ë¡œë“œ (4-bit ì–‘ìí™”)
```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel

# 4bit Quantization Configuration 
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True
)

# Base Model Load
base_model = AutoModelForCausalLM.from_pretrained(
    "yanolja/EEVE-Korean-Instruct-10.8B-v1.0",
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
    torch_dtype=torch.bfloat16
)

# LoRA Adaptor load
model = PeftModel.from_pretrained(
    base_model, 
    "/home/work/eeve-korean-output/final",
    is_trainable=False
)

# Tokenizer
tokenizer = AutoTokenizer.from_pretrained(
    "/home/work/eeve-korean-output/final",
    trust_remote_code=True
)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
```

#### Text Generation (EEVE Prompt Template)
```python
def generate_response(user_input, max_tokens=512):
    # EEVE Official Prompt Template
    prompt = f"""A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.
Human: {user_input}
Assistant: """
    
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=4096
    )
    
    input_length = inputs.input_ids.shape[1]
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            temperature=0.7,           # ìì—°ìŠ¤ëŸ¬ìš´ ë‹¤ì–‘ì„±
            top_p=0.9,                # Nucleus sampling
            top_k=50,
            repetition_penalty=1.1,    # ë°˜ë³µ ë°©ì§€
            do_sample=True,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(
        outputs[0][input_length:], 
        skip_special_tokens=True
    ).strip()
    
    return response

# example
print(generate_response("í•œêµ­ì˜ ìˆ˜ë„ê°€ ì–´ë””ì•¼?"))
print(generate_response("í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ ì„¤ëª…í•´ë´"))
```

### Training Strategy

#### Label Masking
```python
# í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ì€ -100ìœ¼ë¡œ ë§ˆìŠ¤í‚¹ (loss ê³„ì‚° ì œì™¸)
labels = input_ids.clone()
labels[:prompt_length] = -100  # í”„ë¡¬í”„íŠ¸ ë§ˆìŠ¤í‚¹
labels[labels == pad_token_id] = -100  # íŒ¨ë”© ë§ˆìŠ¤í‚¹
```

**Why Label Masking?**
- ì‚¬ìš©ì ì§ˆë¬¸ì€ í•™ìŠµí•˜ì§€ ì•ŠìŒ
- ì–´ì‹œìŠ¤í„´íŠ¸ ë‹µë³€ë§Œ í•™ìŠµ
- ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™” ìŠ¤íƒ€ì¼ í˜•ì„±

#### Memory Optimization
1. **4-bit Quantization (NF4)**: ëª¨ë¸ í¬ê¸° 1/4ë¡œ ì¶•ì†Œ
2. **Gradient Checkpointing**: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ
3. **LoRA**: ì „ì²´ íŒŒë¼ë¯¸í„°ì˜ ~0.5%ë§Œ í•™ìŠµ
4. **BF16 Training**: H100 í•˜ë“œì›¨ì–´ ìµœì í™”

**ê²°ê³¼**: 80GB GPUì—ì„œ 26GBë§Œ ì‚¬ìš©

---

## 2. Qwen2.5 Fine-tuning

### Model Information

- **Base Model**: Qwen2.5 series
- **Status**: ğŸ”„ In Progress
- **Method**: Unsupervised learning with checkpoint support
- **Features**: CPU/GPU monitoring, custom dataset loader

### Training Scripts

```bash
cd qwen

# Main training script with checkpoint support
python 0_qwen_ft_us_cp.py
```

### Utilities

#### Monitoring Tools
- `util/cpu_mntrg.py`: CPU usage monitoring
- `util/gpu_mnrtg.py`: GPU usage monitoring (NVIDIA-SMI)
- `util/monitoring_callback.py`: Training callback with resource tracking

#### Dataset Loader
- `util/local_dataset_loader.py`: Custom dataset loading utilities

### Key Features
- âœ… Checkpoint save/resume
- âœ… Real-time resource monitoring
- âœ… Custom dataset pipeline
- âœ… Distributed training support

---

## 3. VCLM-Korean-7B Benchmarking

### Model Information

- **Model**: VCLM-Korean-7B (Quantized GGUF format)
- **Status**: âœ… Benchmarking Complete
- **Benchmark**: KoCoder evaluation

### Evaluation Scripts

```bash
cd vclm

# Run KoCoder benchmark
python benchmark_vclm_kocoder.py

# Final evaluation
python benchmark_kocoder_final.py
```

### Benchmark Results

VCLM-Korean-7Bì˜ ì½”ë“œ ìƒì„± ëŠ¥ë ¥ì„ KoCoder ë²¤ì¹˜ë§ˆí¬ë¡œ í‰ê°€í•©ë‹ˆë‹¤.

- **í‰ê°€ í•­ëª©**: í•œêµ­ì–´ ì½”ë“œ ìƒì„±, ì£¼ì„ ì‘ì„±, ë””ë²„ê¹…
- **ëª¨ë¸ í˜•ì‹**: GGUF (Q4_K_M quantization)
- **ì‹¤í–‰ í™˜ê²½**: llama.cpp ê¸°ë°˜

---

## 4. Data Generation Pipeline

### Instruction Data Generation

#### EEVE Instruction Data (`datageneration/inst_eeve/`)

```bash
cd datageneration/inst_eeve

# Generate training data for WMS domain
python train_eeve_wms.py

# FP8 precision training data
python train_eeve_wms_fp8.py

# Test data generation
python test_eeve_wms.py
```

#### General Instruction Tools (`datageneration/instruction/`)

```bash
cd datageneration/instruction

# Compare outputs from multiple models
python compare_all_models.py

# Convert datasets to EEVE format
python convert_to_eeve.py
```

### Data Validation (`datageneration/valid/`)

```bash
cd datageneration/valid

# General dataset validation
python validtest.py

# QA dataset validation
python validate_qa_dataset.py
```

**Features**:
- RAG ê¸°ë°˜ ìë™ ë°ì´í„° ìƒì„±
- WMS(ì°½ê³ ê´€ë¦¬) ë„ë©”ì¸ íŠ¹í™”
- ë°ì´í„° í’ˆì§ˆ ê²€ì¦
- ë‹¤ì¤‘ ëª¨ë¸ ì¶œë ¥ ë¹„êµ

---

## 5. Resource Evaluation

### Computing Resource Profiling

```bash
cd eval_computing_resource

# Profile system resources during training
python eval.py
```

**Metrics**:
- GPU utilization & memory
- CPU usage & load
- Memory consumption
- Training throughput

---

## 6. Training Environment

### Hardware Requirements

| Component | Recommended | Minimum |
|-----------|-------------|---------|
| **GPU** | H100 80GB | RTX 3090 24GB |
| **CPU** | 24+ cores | 8+ cores |
| **RAM** | 192GB | 64GB |
| **Storage** | 1TB NVMe | 500GB SSD |

### Software Dependencies

```bash
# PyTorch & Transformers
pip install torch transformers accelerate

# Optimization libraries
pip install unsloth bitsandbytes peft

# Monitoring & utilities
pip install psutil nvidia-ml-py3 tqdm

# Data processing
pip install datasets faiss-cpu pandas
```

--- 


## Quick Start

### 1. Clone Repository

```bash
git clone https://github.com/EnzoMH/ft_llm.git
cd ft_llm
```

### 2. Install Dependencies

```bash
pip install torch transformers accelerate unsloth bitsandbytes peft
pip install psutil nvidia-ml-py3 datasets faiss-cpu
```

### 3. Run Training

```bash
# EEVE Fine-tuning
cd eeve
python 0_unsl_ft.py

# Qwen Fine-tuning
cd qwen
python 0_qwen_ft_us_cp.py

# VCLM Benchmarking
cd vclm
python benchmark_vclm_kocoder.py
```

---

## Best Practices

### Memory Optimization Tips
1. **4-bit Quantization**: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 75% ê°ì†Œ
2. **Gradient Checkpointing**: ì¶”ê°€ 30% ë©”ëª¨ë¦¬ ì ˆì•½
3. **LoRA**: Full fine-tuning ëŒ€ë¹„ 99.5% íŒŒë¼ë¯¸í„° ê°ì†Œ
4. **Mixed Precision (BF16)**: í•™ìŠµ ì†ë„ 2ë°° í–¥ìƒ

### Training Tips
- Checkpoint ìì£¼ ì €ì¥ (250-500 steps)
- Learning rate warmup ì‚¬ìš© (5-10%)
- Gradient accumulationìœ¼ë¡œ effective batch size ì¦ê°€
- Label maskingìœ¼ë¡œ instruction tuning í’ˆì§ˆ í–¥ìƒ

### Data Quality
- ë°ì´í„° ê²€ì¦ ë„êµ¬ë¡œ í’ˆì§ˆ í™•ì¸ (`datageneration/valid/`)
- ì¤‘ë³µ ë°ì´í„° ì œê±°
- Instruction-response í˜•ì‹ ì¼ê´€ì„± ìœ ì§€
- ë„ë©”ì¸ íŠ¹í™” ë°ì´í„°ë¡œ ì„±ëŠ¥ í–¥ìƒ

---

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

### Contribution Guidelines
1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## License

ì´ í”„ë¡œì íŠ¸ëŠ” ê° ë² ì´ìŠ¤ ëª¨ë¸ì˜ ë¼ì´ì„ ìŠ¤ë¥¼ ë”°ë¦…ë‹ˆë‹¤:

- **EEVE-Korean-10.8B**: [Apache 2.0](https://huggingface.co/yanolja/EEVE-Korean-Instruct-10.8B-v1.0)
- **Qwen2.5**: [Apache 2.0](https://huggingface.co/Qwen)
- **VCLM-Korean-7B**: í•´ë‹¹ ëª¨ë¸ ë¼ì´ì„ ìŠ¤ ì°¸ì¡°
- **SOLAR-10.7B**: [Apache 2.0](https://huggingface.co/upstage/SOLAR-10.7B-v1.0)

---

## Acknowledgments

### Model Providers
- **[Yanolja (EEVE Team)](https://huggingface.co/yanolja)**: EEVE-Korean-Instruct-10.8B
- **[Alibaba Cloud (Qwen Team)](https://huggingface.co/Qwen)**: Qwen2.5 series
- **[VCLM Team](https://huggingface.co/VCLM)**: VCLM-Korean-7B
- **[Upstage](https://huggingface.co/upstage)**: SOLAR-10.7B

### Infrastructure
- **KT Cloud**: H100 GPU ì¸í”„ë¼ ì œê³µ

### Libraries & Tools
- **[Unsloth](https://github.com/unslothai/unsloth)**: 2ë°° ë¹ ë¥¸ í•™ìŠµ ì†ë„, ë©”ëª¨ë¦¬ ìµœì í™”
- **[Hugging Face](https://huggingface.co)**: Transformers, PEFT, Datasets, TRL
- **[llama.cpp](https://github.com/ggerganov/llama.cpp)**: GGUF ëª¨ë¸ ì¶”ë¡ 

### Datasets
- **í•œêµ­ì–´ ë°ì´í„°ì…‹ ê¸°ì—¬ìë“¤**: KoAlpaca, Kullm-v2, Smol Korean Talk, KoWiki QA ë“±

---

## Citation

If you use this project in your research, please cite:

```bibtex
@misc{korean-llm-finetuning-suite,
  author = {MyeongHo},
  title = {Korean LLM Fine-tuning & Evaluation Suite},
  year = {2024},
  publisher = {GitHub},
  url = {https://github.com/EnzoMH/ft_llm}
}
```

---

## Contact

- **GitHub**: [@EnzoMH](https://github.com/EnzoMH)
- **HuggingFace**: [MyeongHo0621](https://huggingface.co/MyeongHo0621)

---

**Made with ğŸ”¥ for Korean NLP Community**
