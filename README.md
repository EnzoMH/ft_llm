# Korean LLM Fine-tuning & Evaluation Suite

**í•œêµ­ì–´ íŠ¹í™” ëŒ€ê·œëª¨ ì–¸ì–´ëª¨ë¸(LLM) íŒŒì¸íŠœë‹ ë° í‰ê°€ í†µí•© í”„ë¡œì íŠ¸**

ì´ í”„ë¡œì íŠ¸ëŠ” ë‹¤ì–‘í•œ ì˜¤í”ˆì†ŒìŠ¤ í•œêµ­ì–´ LLMì˜ íŒŒì¸íŠœë‹, í‰ê°€, ë²¤ì¹˜ë§ˆí‚¹ì„ ìœ„í•œ í†µí•© íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤.

## Supported Models

| Model | Status | Description |
|-------|--------|-------------|
| **EEVE-Korean-10.8B** | âœ… Complete | Instruction tuning, HuggingFace ë°°í¬ ì™„ë£Œ |
| **Qwen2.5-14B-Instruct** | ğŸ”„ In Progress | í•œêµ­ì–´ ë©€í‹°í„´ ëŒ€í™” íŒŒì¸íŠœë‹, Flash Attention 2 |
| **VCLM-Korean-7B** | âœ… Complete | Benchmarking & Evaluation |
| **SOLAR-10.7B** | âœ… Complete | Legacy project (archived) |

## Key Features

- ğŸš€ **ë‹¤ì¤‘ ëª¨ë¸ ì§€ì›**: EEVE, Qwen, VCLM, SOLAR
- ğŸ”§ **ìµœì í™”**: Unsloth, LoRA, 4-bit quantization
- ğŸ“Š **ë²¤ì¹˜ë§ˆí‚¹**: KoCoder, HumanEval í‰ê°€
- ğŸ—ƒï¸ **ë°ì´í„° ìƒì„±**: RAG ê¸°ë°˜ instruction ë°ì´í„° ìë™ ìƒì„±
- ğŸ” **ê²€ì¦ ë„êµ¬**: ë°ì´í„°ì…‹ í’ˆì§ˆ ê²€ì¦, ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§
- ğŸ’¾ **íš¨ìœ¨ì  í•™ìŠµ**: Gradient checkpointing, Mixed precision 

## Deployed Model

**HuggingFace**: [MyeongHo0621/eeve-vss-smh](https://huggingface.co/MyeongHo0621/eeve-vss-smh)

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(
    "MyeongHo0621/eeve-vss-smh",
    device_map="auto",
    torch_dtype="auto"
)
tokenizer = AutoTokenizer.from_pretrained("MyeongHo0621/eeve-vss-smh")
```

## Model Information

- **Base Model**: [yanolja/EEVE-Korean-Instruct-10.8B-v1.0](https://huggingface.co/yanolja/EEVE-Korean-Instruct-10.8B-v1.0)
- **How to fine-tune**: LoRA (r=128, alpha=256) + Unsloth
- **Data**: ê³ í’ˆì§ˆ í•œêµ­ì–´ instruction ë°ì´í„° (~100K ìƒ˜í”Œ)

## Train envrionment & configuration

### H/W info
- **GPU**: NVIDIA H100 80GB HBM3
- **CPU**: 24 cores
- **RAM**: 192GB
- **Framework**: Unsloth + PyTorch 2.8, Transformers 4.56.2

### LoRA configuration 
- **r**: 128 
- **alpha**: 256 (alpha = 2 * r)
- **dropout**: 0.0 (Only 0.0)
- **target_modules**: q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj
- **use_rslora**: false

### Training Hyper Parameter 
- **Framework**: Unsloth 
- **Epochs**: 3 
- **Batch Size**: 8 
- **Gradient Accumulation**: 2 
- **Learning Rate**: 1e-4
- **Max Sequence Length**: 4096 tokens
- **Warmup Ratio**: 0.05
- **Weight Decay**: 0.01

### Memory Optimization
- **Full Precision Training**
- **Unsloth Gradient Checkpointing**
- **BF16 Training**
- **Peak VRAM**

## Directory Structure

```
tesseract/
â”œâ”€â”€ eeve/                           # EEVE-Korean-10.8B Fine-tuning
â”‚   â”œâ”€â”€ 0_unsl_ft.py               # Main training script
â”‚   â”œâ”€â”€ 1_cp_ft.py                 # Checkpoint resume training
â”‚   â”œâ”€â”€ 2_merg_uplod.py            # Model merge & HuggingFace upload
â”‚   â”œâ”€â”€ 3_test_checkpoint.py       # Checkpoint testing
â”‚   â”œâ”€â”€ UNSLOTH_GUIDE.md           # Unsloth optimization guide
â”‚   â””â”€â”€ quant/                     # Quantization scripts
â”‚
â”œâ”€â”€ qwen/                           # Qwen2.5 Fine-tuning
â”‚   â”œâ”€â”€ 2.5_14B_Inst/              # Qwen2.5-14B-Instruct ë©€í‹°í„´ ëŒ€í™” íŒŒì¸íŠœë‹
â”‚   â”‚   â”œâ”€â”€ 0_qwen14b_multiturn_ft.py  # Main training script
â”‚   â”‚   â”œâ”€â”€ 0_qwen14b/             # Training modules
â”‚   â”‚   â”‚   â”œâ”€â”€ config.py          # Configuration
â”‚   â”‚   â”‚   â”œâ”€â”€ trainer.py         # Trainer with checkpoint resume
â”‚   â”‚   â”‚   â”œâ”€â”€ dataset_loader.py  # Multi-turn dataset loader
â”‚   â”‚   â”‚   â”œâ”€â”€ callbacks.py       # Hub upload & monitoring callbacks
â”‚   â”‚   â”‚   â””â”€â”€ utils.py           # System resource monitoring
â”‚   â”‚   â”œâ”€â”€ requirements.txt       # Dependencies
â”‚   â”‚   â””â”€â”€ output/                # Training outputs & checkpoints
â”‚   â”œâ”€â”€ 2.5_7B/                    # Qwen2.5-7B (legacy)
â”‚   â”œâ”€â”€ 0_qwen_ft_us_cp.py         # Legacy Qwen training script
â”‚   â”œâ”€â”€ util/                      # Training utilities
â”‚   â”‚   â”œâ”€â”€ cpu_mntrg.py           # CPU monitoring
â”‚   â”‚   â”œâ”€â”€ gpu_mnrtg.py           # GPU monitoring
â”‚   â”‚   â”œâ”€â”€ local_dataset_loader.py # Dataset loader
â”‚   â”‚   â””â”€â”€ monitoring_callback.py  # Training callback
â”‚   â””â”€â”€ 4_credential/              # Credentials (empty)
â”‚
â”œâ”€â”€ vclm/                           # VCLM-Korean-7B Evaluation
â”‚   â”œâ”€â”€ benchmark_vclm_kocoder.py  # KoCoder benchmarking
â”‚   â”œâ”€â”€ benchmark_kocoder_final.py # Final evaluation
â”‚   â””â”€â”€ .gitattributes             # LFS configuration
â”‚
â”œâ”€â”€ solar/                          # SOLAR-10.7B (Legacy)
â”‚   â””â”€â”€ ...                        # Archived fine-tuning scripts
â”‚
â”œâ”€â”€ datageneration/                 # Data Generation Pipeline
â”‚   â”œâ”€â”€ inst_eeve/                 # EEVE instruction data
â”‚   â”‚   â”œâ”€â”€ train_eeve_wms.py      # WMS training data
â”‚   â”‚   â”œâ”€â”€ train_eeve_wms_fp8.py  # FP8 training
â”‚   â”‚   â””â”€â”€ test_eeve_wms.py       # Testing
â”‚   â”œâ”€â”€ instruction/               # Instruction generation
â”‚   â”‚   â”œâ”€â”€ compare_all_models.py  # Model comparison
â”‚   â”‚   â””â”€â”€ convert_to_eeve.py     # Format conversion
â”‚   â””â”€â”€ valid/                     # Validation tools
â”‚       â”œâ”€â”€ validtest.py           # General validation
â”‚       â””â”€â”€ validate_qa_dataset.py # QA validation
â”‚
â”œâ”€â”€ eval_computing_resource/        # Resource Evaluation
â”‚   â””â”€â”€ eval.py                    # Computing resource profiling
â”‚
â”œâ”€â”€ faiss_storage/                  # Vector Database (gitignored)
â”‚   â””â”€â”€ ...                        # FAISS index for RAG
â”‚
â””â”€â”€ korean_large_data/              # Large Datasets (gitignored)
    â””â”€â”€ ...                        # Training datasets
```

---

## 1. EEVE-Korean-10.8B Fine-tuning

### Model Information

- **Base Model**: [yanolja/EEVE-Korean-Instruct-10.8B-v1.0](https://huggingface.co/yanolja/EEVE-Korean-Instruct-10.8B-v1.0)
- **Deployed Model**: [MyeongHo0621/eeve-vss-smh](https://huggingface.co/MyeongHo0621/eeve-vss-smh)
- **Status**: âœ… Training Complete & HuggingFace Deployment Complete
- **Vocab Size**: 40,960 (í•œì˜ balanced)
- **Context Length**: 8K tokens
- **Method**: LoRA (r=128, alpha=256) + Unsloth

### Training Configuration

#### Hardware
- **GPU**: NVIDIA H100 80GB HBM3
- **CPU**: 24 cores
- **RAM**: 192GB
- **Framework**: Unsloth + PyTorch 2.8, Transformers 4.56.2

#### LoRA Settings
- **r**: 128 
- **alpha**: 256 (alpha = 2 * r)
- **dropout**: 0.0
- **target_modules**: q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj
- **use_rslora**: false

#### Training Hyperparameters
- **Epochs**: 3 
- **Batch Size**: 8 
- **Gradient Accumulation**: 2 
- **Learning Rate**: 1e-4
- **Max Sequence Length**: 4096 tokens
- **Warmup Ratio**: 0.05
- **Weight Decay**: 0.01

#### Results
- **Training time**: ~3 hours (6,250 steps)
- **Peak VRAM**: ~26GB
- **Checkpoint Interval**: 250 steps

### How to Use

#### 1. HuggingFace (Recommended)

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# model load
model = AutoModelForCausalLM.from_pretrained(
    "MyeongHo0621/eeve-vss-smh",
    device_map="auto",
    torch_dtype="auto"
)
tokenizer = AutoTokenizer.from_pretrained("MyeongHo0621/eeve-vss-smh")

# prompt Template
def create_prompt(user_input):
    return f"""A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.
Human: {user_input}
Assistant: """

# generating response
prompt = create_prompt("í•œêµ­ì˜ ìˆ˜ë„ê°€ ì–´ë””ì•¼?")
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
outputs = model.generate(
    **inputs,
    max_new_tokens=512,
    temperature=0.3,
    top_p=0.85,
    do_sample=True
)
response = tokenizer.decode(outputs[0][len(inputs['input_ids'][0]):], skip_special_tokens=True)
print(response)
```

#### 2. Re-Training from Checkpoint

```bash
cd eeve

# Train from scratch
python 0_unsl_ft.py

# Resume from checkpoint
python 1_cp_ft.py

# Merge LoRA and upload to HuggingFace
python 2_merg_uplod.py

# Test checkpoints
python 3_test_checkpoint.py --compare \
  /path/to/checkpoint-1 \
  /path/to/checkpoint-2
```

#### 3. Model Load (Python API)

#### ê¸°ë³¸ ë¡œë“œ (4-bit ì–‘ìí™”)
```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel

# 4bit Quantization Configuration 
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True
)

# Base Model Load
base_model = AutoModelForCausalLM.from_pretrained(
    "yanolja/EEVE-Korean-Instruct-10.8B-v1.0",
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
    torch_dtype=torch.bfloat16
)

# LoRA Adaptor load
model = PeftModel.from_pretrained(
    base_model, 
    "/home/work/eeve-korean-output/final",
    is_trainable=False
)

# Tokenizer
tokenizer = AutoTokenizer.from_pretrained(
    "/home/work/eeve-korean-output/final",
    trust_remote_code=True
)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
```

#### Text Generation (EEVE Prompt Template)
```python
def generate_response(user_input, max_tokens=512):
    # EEVE Official Prompt Template
    prompt = f"""A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.
Human: {user_input}
Assistant: """
    
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=4096
    )
    
    input_length = inputs.input_ids.shape[1]
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            temperature=0.7,           # ìì—°ìŠ¤ëŸ¬ìš´ ë‹¤ì–‘ì„±
            top_p=0.9,                # Nucleus sampling
            top_k=50,
            repetition_penalty=1.1,    # ë°˜ë³µ ë°©ì§€
            do_sample=True,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(
        outputs[0][input_length:], 
        skip_special_tokens=True
    ).strip()
    
    return response

# example
print(generate_response("í•œêµ­ì˜ ìˆ˜ë„ê°€ ì–´ë””ì•¼?"))
print(generate_response("í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ ì„¤ëª…í•´ë´"))
```

### Training Strategy

#### Label Masking
```python
# í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ì€ -100ìœ¼ë¡œ ë§ˆìŠ¤í‚¹ (loss ê³„ì‚° ì œì™¸)
labels = input_ids.clone()
labels[:prompt_length] = -100  # í”„ë¡¬í”„íŠ¸ ë§ˆìŠ¤í‚¹
labels[labels == pad_token_id] = -100  # íŒ¨ë”© ë§ˆìŠ¤í‚¹
```

**Why Label Masking?**
- ì‚¬ìš©ì ì§ˆë¬¸ì€ í•™ìŠµí•˜ì§€ ì•ŠìŒ
- ì–´ì‹œìŠ¤í„´íŠ¸ ë‹µë³€ë§Œ í•™ìŠµ
- ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™” ìŠ¤íƒ€ì¼ í˜•ì„±

#### Memory Optimization
1. **4-bit Quantization (NF4)**: ëª¨ë¸ í¬ê¸° 1/4ë¡œ ì¶•ì†Œ
2. **Gradient Checkpointing**: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ
3. **LoRA**: ì „ì²´ íŒŒë¼ë¯¸í„°ì˜ ~0.5%ë§Œ í•™ìŠµ
4. **BF16 Training**: H100 í•˜ë“œì›¨ì–´ ìµœì í™”

**ê²°ê³¼**: 80GB GPUì—ì„œ 26GBë§Œ ì‚¬ìš©

---

## 2. Qwen2.5-14B-Instruct Fine-tuning

### Model Information

- **Base Model**: [Qwen/Qwen2.5-14B-Instruct](https://huggingface.co/Qwen/Qwen2.5-14B-Instruct)
- **Status**: ğŸ”„ In Progress
- **Deployed Model**: [MyeongHo0621/Qwen2.5-14B-Korean](https://huggingface.co/MyeongHo0621/Qwen2.5-14B-Korean)
- **Method**: LoRA (r=64, alpha=128) + Unsloth + Flash Attention 2
- **Task**: í•œêµ­ì–´ ë©€í‹°í„´ ëŒ€í™” íŒŒì¸íŠœë‹
- **Vocab Size**: 151,665
- **Context Length**: 4,096 tokens

### Training Configuration

#### Hardware
- **GPU**: NVIDIA H100 80GB HBM3
- **CPU**: 12 cores (limited for RAM optimization)
- **RAM**: 127GB
- **Framework**: Unsloth 2025.11.2 + PyTorch 2.8.0 + Transformers 4.57.1

#### LoRA Settings
- **r**: 64
- **alpha**: 128
- **dropout**: 0.0 (Unsloth ìµœì í™” í™œì„±í™”)
- **target_modules**: q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj
- **use_rslora**: false

#### Training Hyperparameters
- **Epochs**: 3
- **Batch Size**: 12 per device
- **Gradient Accumulation**: 4
- **Effective Batch Size**: 48 (12 Ã— 4)
- **Learning Rate**: 2e-4
- **Max Sequence Length**: 4096 tokens
- **Warmup Ratio**: 0.03
- **Weight Decay**: 0.01
- **Max Grad Norm**: 1.0

#### Memory Optimization
- **8-bit Quantization**: BitsAndBytes 8-bit
- **Flash Attention 2**: 2.8.3 (ì •ìƒ ì‘ë™)
- **Unsloth Gradient Checkpointing**: í™œì„±í™”
- **BF16 Training**: H100 í•˜ë“œì›¨ì–´ ìµœì í™”
- **Peak VRAM**: ~16.2GB / 39.8GB (40.8%)

#### Data
- **Dataset**: smol_koreantalk_full.jsonl (460K ìƒ˜í”Œ)
- **Max Samples**: 200,000 (RAM ìµœì í™”)
- **Train/Val Split**: 190,000 / 10,000
- **Format**: ChatML (ë©€í‹°í„´ ëŒ€í™”)

#### Checkpoint & Hub Upload
- **Save Steps**: 100 steps (ì•½ 30ë¶„ë§ˆë‹¤)
- **Save Total Limit**: 5 checkpoints
- **Hub Upload**: ë¹„ë™ê¸° ë°±ê·¸ë¼ìš´ë“œ ì—…ë¡œë“œ (í•™ìŠµ ë¸”ë¡œí‚¹ ë°©ì§€)
- **Hub Strategy**: checkpoint (ë§¤ checkpointë§ˆë‹¤ ìë™ ì—…ë¡œë“œ)
- **Hub Tracking**: `.hub_uploaded_checkpoints.json`ìœ¼ë¡œ ì—…ë¡œë“œ ì™„ë£Œ ì¶”ì 

### Training Scripts

```bash
cd qwen/2.5_14B_Inst

# Main training script (ë©€í‹°í„´ ëŒ€í™” íŒŒì¸íŠœë‹)
python 0_qwen14b_multiturn_ft.py

# Training with nohup (ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰)
nohup python 0_qwen14b_multiturn_ft.py > train.log 2>&1 &
```

### Key Features

- âœ… **Flash Attention 2**: ì •ìƒ ì‘ë™ (2.8.3)
- âœ… **Checkpoint Resume**: Hub ì—…ë¡œë“œ ì™„ë£Œëœ checkpoint ìš°ì„  ì‚¬ìš©
- âœ… **ë¹„ë™ê¸° Hub ì—…ë¡œë“œ**: í•™ìŠµ ë¸”ë¡œí‚¹ ë°©ì§€
- âœ… **ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§**: CPU/RAM/GPU ì‹¤ì‹œê°„ ì¶”ì 
- âœ… **ë©€í‹°í„´ ëŒ€í™”**: ChatML í¬ë§· ì§€ì›
- âœ… **ìë™ ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬**: Hub ì—…ë¡œë“œ ì™„ë£Œ ì—¬ë¶€ ì¶”ì 

### Training Progress

- **Current Status**: Step 200+ ì§„í–‰ ì¤‘
- **Total Steps**: 11,877 steps (3 epochs)
- **Step Time**: ~19.6ì´ˆ/step
- **Estimated Time**: ~64ì‹œê°„ (2.7ì¼)

### Training Metrics (Step 200 ê¸°ì¤€)

- **Loss**: 0.96~1.01 (ì •ìƒ ë²”ìœ„)
- **Eval Loss**: 0.9719
- **Grad Norm**: 0.20~0.31 (ì •ìƒ ë²”ìœ„)
- **Learning Rate**: Warmup ì§„í–‰ ì¤‘ (í˜„ì¬ 0.000111)

### Legacy Qwen Scripts

```bash
cd qwen

# Legacy training script with checkpoint support
python 0_qwen_ft_us_cp.py
```

### Utilities

#### Monitoring Tools
- `util/cpu_mntrg.py`: CPU usage monitoring
- `util/gpu_mnrtg.py`: GPU usage monitoring (NVIDIA-SMI)
- `util/monitoring_callback.py`: Training callback with resource tracking
- `0_qwen14b/utils.py`: ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ (CPU/RAM/GPU)

#### Dataset Loader
- `util/local_dataset_loader.py`: Custom dataset loading utilities
- `0_qwen14b/dataset_loader.py`: ë©€í‹°í„´ ëŒ€í™” ë°ì´í„°ì…‹ ë¡œë”

---

## 3. VCLM-Korean-7B Benchmarking

### Model Information

- **Model**: VCLM-Korean-7B (Quantized GGUF format)
- **Status**: âœ… Benchmarking Complete
- **Benchmark**: KoCoder evaluation

### Evaluation Scripts

```bash
cd vclm

# Run KoCoder benchmark
python benchmark_vclm_kocoder.py

# Final evaluation
python benchmark_kocoder_final.py
```

### Benchmark Results

VCLM-Korean-7Bì˜ ì½”ë“œ ìƒì„± ëŠ¥ë ¥ì„ KoCoder ë²¤ì¹˜ë§ˆí¬ë¡œ í‰ê°€í•©ë‹ˆë‹¤.

- **í‰ê°€ í•­ëª©**: í•œêµ­ì–´ ì½”ë“œ ìƒì„±, ì£¼ì„ ì‘ì„±, ë””ë²„ê¹…
- **ëª¨ë¸ í˜•ì‹**: GGUF (Q4_K_M quantization)
- **ì‹¤í–‰ í™˜ê²½**: llama.cpp ê¸°ë°˜

---

## 4. Data Generation Pipeline

### Instruction Data Generation

#### EEVE Instruction Data (`datageneration/inst_eeve/`)

```bash
cd datageneration/inst_eeve

# Generate training data for WMS domain
python train_eeve_wms.py

# FP8 precision training data
python train_eeve_wms_fp8.py

# Test data generation
python test_eeve_wms.py
```

#### General Instruction Tools (`datageneration/instruction/`)

```bash
cd datageneration/instruction

# Compare outputs from multiple models
python compare_all_models.py

# Convert datasets to EEVE format
python convert_to_eeve.py
```

### Data Validation (`datageneration/valid/`)

```bash
cd datageneration/valid

# General dataset validation
python validtest.py

# QA dataset validation
python validate_qa_dataset.py
```

**Features**:
- RAG ê¸°ë°˜ ìë™ ë°ì´í„° ìƒì„±
- WMS(ì°½ê³ ê´€ë¦¬) ë„ë©”ì¸ íŠ¹í™”
- ë°ì´í„° í’ˆì§ˆ ê²€ì¦
- ë‹¤ì¤‘ ëª¨ë¸ ì¶œë ¥ ë¹„êµ

---

## 5. Resource Evaluation

### Computing Resource Profiling

```bash
cd eval_computing_resource

# Profile system resources during training
python eval.py
```

**Metrics**:
- GPU utilization & memory
- CPU usage & load
- Memory consumption
- Training throughput

---

## 6. Training Environment

### Hardware Requirements

| Component | Recommended | Minimum |
|-----------|-------------|---------|
| **GPU** | H100 80GB | RTX 3090 24GB |
| **CPU** | 24+ cores | 8+ cores |
| **RAM** | 192GB | 64GB |
| **Storage** | 1TB NVMe | 500GB SSD |

### Software Dependencies

#### Base Dependencies
```bash
# PyTorch & Transformers
pip install torch>=2.8.0 transformers>=4.57.0 accelerate>=1.11.0

# Optimization libraries
pip install unsloth>=2025.11.2 bitsandbytes>=0.48.0 peft>=0.14.0

# Flash Attention 2 (H100 ìµœì í™”)
pip install flash-attn==2.8.3 --no-build-isolation

# Monitoring & utilities
pip install psutil>=7.1.0 nvidia-ml-py3 tqdm

# Data processing
pip install datasets>=3.4.0 faiss-cpu pandas
```

#### Qwen2.5-14B Specific
```bash
cd qwen/2.5_14B_Inst
pip install -r requirements.txt
```

--- 


## Quick Start

### 1. Clone Repository

```bash
git clone https://github.com/EnzoMH/ft_llm.git
cd ft_llm
```

### 2. Install Dependencies

```bash
pip install torch transformers accelerate unsloth bitsandbytes peft
pip install psutil nvidia-ml-py3 datasets faiss-cpu
```

### 3. Run Training

```bash
# EEVE Fine-tuning
cd eeve
python 0_unsl_ft.py

# Qwen2.5-14B-Instruct Fine-tuning
cd qwen/2.5_14B_Inst
python 0_qwen14b_multiturn_ft.py

# Legacy Qwen Fine-tuning
cd qwen
python 0_qwen_ft_us_cp.py

# VCLM Benchmarking
cd vclm
python benchmark_vclm_kocoder.py
```

---

## Best Practices

### Memory Optimization Tips
1. **8-bit Quantization**: BitsAndBytes 8-bitë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ
2. **Flash Attention 2**: ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì–´í…ì…˜ ì—°ì‚° (H100 ìµœì í™”)
3. **Gradient Checkpointing**: Unsloth ìµœì í™”ë¡œ ì¶”ê°€ ë©”ëª¨ë¦¬ ì ˆì•½
4. **LoRA**: Full fine-tuning ëŒ€ë¹„ 98%+ íŒŒë¼ë¯¸í„° ê°ì†Œ (r=64 ê¸°ì¤€)
5. **Mixed Precision (BF16)**: H100 í•˜ë“œì›¨ì–´ ìµœì í™”ë¡œ í•™ìŠµ ì†ë„ í–¥ìƒ

### Training Tips
- **Checkpoint ê´€ë¦¬**: ìì£¼ ì €ì¥ (100-250 steps), Hub ì—…ë¡œë“œ ì™„ë£Œ ì¶”ì 
- **Learning rate warmup**: 3-5% warmup ratio ê¶Œì¥
- **Gradient accumulation**: Effective batch size ì¦ê°€ (ì˜ˆ: 12Ã—4=48)
- **Label masking**: Instruction tuning í’ˆì§ˆ í–¥ìƒ (í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ë§ˆìŠ¤í‚¹)
- **ë¹„ë™ê¸° Hub ì—…ë¡œë“œ**: í•™ìŠµ ë¸”ë¡œí‚¹ ë°©ì§€ë¥¼ ìœ„í•´ ë°±ê·¸ë¼ìš´ë“œ ì—…ë¡œë“œ ì‚¬ìš©
- **Flash Attention 2**: H100 GPUì—ì„œ í•„ìˆ˜ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ë° ì†ë„ í–¥ìƒ)

### Data Quality
- ë°ì´í„° ê²€ì¦ ë„êµ¬ë¡œ í’ˆì§ˆ í™•ì¸ (`datageneration/valid/`)
- ì¤‘ë³µ ë°ì´í„° ì œê±°
- Instruction-response í˜•ì‹ ì¼ê´€ì„± ìœ ì§€
- ë„ë©”ì¸ íŠ¹í™” ë°ì´í„°ë¡œ ì„±ëŠ¥ í–¥ìƒ

---

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

### Contribution Guidelines
1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## License

ì´ í”„ë¡œì íŠ¸ëŠ” ê° ë² ì´ìŠ¤ ëª¨ë¸ì˜ ë¼ì´ì„ ìŠ¤ë¥¼ ë”°ë¦…ë‹ˆë‹¤:

- **EEVE-Korean-10.8B**: [Apache 2.0](https://huggingface.co/yanolja/EEVE-Korean-Instruct-10.8B-v1.0)
- **Qwen2.5**: [Apache 2.0](https://huggingface.co/Qwen)
- **VCLM-Korean-7B**: í•´ë‹¹ ëª¨ë¸ ë¼ì´ì„ ìŠ¤ ì°¸ì¡°
- **SOLAR-10.7B**: [Apache 2.0](https://huggingface.co/upstage/SOLAR-10.7B-v1.0)

---

## Acknowledgments

### Model Providers
- **[Yanolja (EEVE Team)](https://huggingface.co/yanolja)**: EEVE-Korean-Instruct-10.8B
- **[Alibaba Cloud (Qwen Team)](https://huggingface.co/Qwen)**: Qwen2.5 series
- **[VCLM Team](https://huggingface.co/VCLM)**: VCLM-Korean-7B
- **[Upstage](https://huggingface.co/upstage)**: SOLAR-10.7B

### Infrastructure
- **KT Cloud**: H100 GPU ì¸í”„ë¼ ì œê³µ

### Libraries & Tools
- **[Unsloth](https://github.com/unslothai/unsloth)**: 2ë°° ë¹ ë¥¸ í•™ìŠµ ì†ë„, ë©”ëª¨ë¦¬ ìµœì í™”
- **[Hugging Face](https://huggingface.co)**: Transformers, PEFT, Datasets, TRL
- **[llama.cpp](https://github.com/ggerganov/llama.cpp)**: GGUF ëª¨ë¸ ì¶”ë¡ 

### Datasets
- **í•œêµ­ì–´ ë°ì´í„°ì…‹ ê¸°ì—¬ìë“¤**: KoAlpaca, Kullm-v2, Smol Korean Talk, KoWiki QA ë“±

---

## Citation

If you use this project in your research, please cite:

```bibtex
@misc{korean-llm-finetuning-suite,
  author = {MyeongHo},
  title = {Korean LLM Fine-tuning & Evaluation Suite},
  year = {2024},
  publisher = {GitHub},
  url = {https://github.com/EnzoMH/ft_llm}
}
```

---

## Contact

- **GitHub**: [@EnzoMH](https://github.com/EnzoMH)
- **HuggingFace**: [MyeongHo0621](https://huggingface.co/MyeongHo0621)

---

**Made with ğŸ”¥ for Korean NLP Community**
